[{"authors":["admin"],"categories":null,"content":"Antes de ensinar computadores trabalhei ensinando matem√°tica e f√≠sica para alunos do ensino m√©dio em escolas e cursinhos comunit√°rios. Hoje pesquiso na √°rea de grafos, matrizes aleat√≥rias e redes complexas. Contribuo para desenvolvimento de software livre e sou um dos fundadores da comunidade python tri√¢ngulo.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"pt-br","lastmod":-62135596800,"objectID":"390e5dfac3efbc79c4b523e2b287aa98","permalink":"/pt-br/author/bruno-messias/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/pt-br/author/bruno-messias/","section":"authors","summary":"Antes de ensinar computadores trabalhei ensinando matem√°tica e f√≠sica para alunos do ensino m√©dio em escolas e cursinhos comunit√°rios. Hoje pesquiso na √°rea de grafos, matrizes aleat√≥rias e redes complexas. Contribuo para desenvolvimento de software livre e sou um dos fundadores da comunidade python tri√¢ngulo.","tags":null,"title":"Bruno Messias","type":"authors"},{"authors":null,"categories":["Python"],"content":"Lista de Conte√∫dos    Intro: our previous problem ASTs: What they are? Python: interpreted or compiled? Extracting ASTs and interpreting them How can I be efficient in metaprogramming?  The 6 simple steps   Creating our metaprogramming function  First six-steps interaction The NodeTransformer class The second six-steps interaction Creating a new function at runtime   Integrating the AST manipulation with a decorator      Intro: our previous problem Don\u0026rsquo;t be afraid by the names on the title. Although they can seem scary or strange probably you already have been in touch with tools that work with this kind of stuff. For example, pytest and numba.\nIn the previous post, I talked about python frames and inspection module. I\u0026rsquo;ve started showing how we can use the inspect.signature to construct a decorator that validates arguments:\n@math_validator() def simple_method(x: \u0026quot;\\in R\u0026quot;, y: \u0026quot;\\in R_+\u0026quot;, z: float = 2) -\u0026gt; float: ... simple_method(1, 0)  simple_method((1, 2)) -\u0026gt; 1.5 ---\u0026gt; 19 simple_method(1, 0) ... \u0026lt;locals\u0026gt;.decorate.\u0026lt;locals\u0026gt;.decorated(*_args) 11 continue 13 if not MATH_SPACES[annotation][\u0026quot;validator\u0026quot;](_args[i]): ---\u0026gt; 14 raise ValueError(f\u0026quot;{k} doesn't belong to the {MATH_SPACES[annotation]['name']}\u0026quot;) 15 result = func(*_args) 16 print(f\u0026quot;{func.__name__}({_args}) -\u0026gt; {result}\u0026quot;) ValueError: y doesn't belong to the space of real numbers greater than zero  And after that, I\u0026rsquo;ve combined the inspect.singature+sys.trace+locals to construct a decorator that exposes the local variables of a decorated function. All this stuff allows us to do cool things like creating a generic report decorator that have access to the local variables of the decorated method\n@report('{arg.n_bananas} Monkey {gluttonous_monkey} ate too much bananas. Num monkeys {num_monkeys}') def feed_monkeys(n_bananas): num_monkeys = 3 monkeys = { f\u0026quot;monkey_{i}\u0026quot;: {\u0026quot;bananas\u0026quot;: 0} for i in range(num_monkeys) } while n_bananas \u0026gt; 0: if np.random.uniform() \u0026lt; 0.4: continue monkey = monkeys[np.random.choice(list(monkeys.keys()))] if n_bananas \u0026gt; 0: monkey[\u0026quot;bananas\u0026quot;] += 1 n_bananas -= 1 gluttonous_monkey = max(monkeys, key=lambda k: monkeys[k][\u0026quot;bananas\u0026quot;])  These two examples can be found in real application scenarios. But at the end of my previous post I\u0026rsquo;ve told you some issues regarding the use of sys.trace. I\u0026rsquo;ll put the code here of the previous solution:  Click here to see the solution  import sys import inspect from types import SimpleNamespace def call_and_extract_frame(func, *args, **kwargs): frame_var = None trace = sys.gettrace() def update_frame_var(stack_frame, event_name, arg_frame): \u0026quot;\u0026quot;\u0026quot; Args: stack_frame: (frame) The current stack frame. event_name: (str) The name of the event that triggered the call. Can be 'call', 'line', 'return' and 'exception'. arg_frame: Depends on the event. Can be a None type \u0026quot;\u0026quot;\u0026quot; nonlocal frame_var # nonlocal is a keyword which allows us to modify the outisde scope variable if event_name != 'call': return trace frame_var = stack_frame sys.settrace(trace) return trace sys.settrace(update_frame_var) try: func_result = func(*args, **kwargs) finally: sys.settrace(trace) return frame_var, func_result def report(formater): def decorate(func): def decorated(*_args): sig = inspect.signature(func) named_args = {} num_args = len(_args) for i, (k, v) in enumerate(sig.parameters.items()): if i \u0026lt; num_args: named_args[k] = repr(_args[i]) else: named_args[k] = repr(v.default) frame_func, _result = call_and_extract_frame(func, *_args) name = func.__name__ result = repr(_result) args_dict = { \u0026quot;args\u0026quot;: SimpleNamespace(**named_args), \u0026quot;args_repr\u0026quot;: repr(SimpleNamespace(**named_args)), **locals(), **frame_func.f_locals, } print(formater.format(**args_dict)) # do other stuff here return _result return decorated return decorate    \nWhat are the problems with this solution?\n A tracing always creates a cost. Thus, is expected that we will reduce the performance of our system. If you use this just for debugging purposes, it\u0026rsquo;s ok. This can create conflicts with other tools and libs that also trying to use the trace tool it seems dirty!  Ok, maybe you\u0026rsquo;re asking yourself \u0026ldquo;This guy is overthinking. Why he didn\u0026rsquo;t just do this?\u0026quot;\n@report('stuff goes here') def func(x, y): random_var = np.random.uniform() ... #more local vars result = (x+y)**random_var return result, locals  \u0026rdquo;\u0026hellip;and then, inside of decorator change to this:\u0026quot;\n_result, local_vars = func(x, y)  The reason is:\n The main point of using this decorator is to avoid any change in other parts of the codebase. For example, if in any part of the codebase func has been called you will have to change to  result = func(x, y) # to result = func(x, y)[0]  If after you choose to remove the decorator from a function, you will need to be rollback all the above changes.\n You will increase the cognitive load in all members of the team who doesn\u0026rsquo;t care about what your decorator needs to do. If you propose this a solution is better just to create another function and face the consequences of this increase in complexity in the original codebase.  Ok, maybe you\u0026rsquo;re now thinking: \u0026ldquo;Right, this makes sense, but you\u0026rsquo;re avoiding theses issues creating issues in performance and debugging. Don\u0026rsquo;t sound good besides for just some special cases\u0026rdquo;. And I need to agree with you, it\u0026rsquo;s not a good solution for most of the cases!\nWell, what can I do? The problem we\u0026rsquo;re facing is that python doesn\u0026rsquo;t have context managers that can deal with namespaces. Although there is an active discussion about this https://mail.python.org/archives/list/python-ideas@python.org/. But don\u0026rsquo;t worry about this big name. The important point here is that:\n If a language doesn\u0026rsquo;t have a feature that I need what can I do?   In python we are fine with this because it\u0026rsquo;s a language that turns to be easy to manipulate what is called Abstract Syntax Tree and recompile a function with the manipulated syntax tree. Doing that way we\u0026rsquo;re in the realm of metaprogramming. Writing code which writes code. If t\u0026rsquo;s not clear I\u0026rsquo;ll try to be more clear now.\nASTs: What they are? A programming language obviously is at least a language. OK, but what is a language? Do all the human languages share the same building blocks? How can we compare different sentences? These questions seem more proper to be answered by philosophers. Well, maybe this is true, but these questions can also be answered by mathematicians and computer scientists. Although, mathematicians and CS people usually prefer to talk using mathematical formalism rather than long debates about the meaning of the stuff. In essence, an AST is a mathematical formalism that allows us to represent a sentence using a well-defined set of rules and structures represented by a tree.\nHow do you know that a sentence is grammatically correct? Intuitively, probably you remember a set of rules that you learned during your life about how to organize and compose verbs, nouns, adjectives, adverbs, etc. This set of rules and guidelines is the Syntax of a language. A Syntax Tree is a structure that helps us to understand a sentence.\n After constructing the syntax tree we can look in the guidelines book of our language and check if this tree has a valid structure.   Take for example the sentence: \u0026ldquo;I drive a car to my college\u0026rdquo;, the syntax tree is the following:\n  A Syntax Tree for the sentence: I drive a car to my college. Source:Geeks for Geeks:Syntax Tree ‚Äì Natural Language Processing.   What is the advantage of using ASTs? Notice that we don\u0026rsquo;t need to talk about how many spaces you\u0026rsquo;re using, we didn\u0026rsquo;t talk about your calligraphy and besides that, we have a hierarchy structure that allows us to analyze the validity of the sentence per level! If we want to change any element of the sentence we can directly manipulate the node which represents that element for a safe guarantee that the manipulated sentence is still grammatically correct!\nIt\u0026rsquo;s not a surprise that ASTs are also a common tool used in computer science to analyze the correctness of a piece of code and as a common part of the process of compiling/interpreting a code. Here we will extend the behavior of a python decorator manipulating the AST. But before that, I would like to ask you a question:\nIs Python an interpreted language? Python: interpreted or compiled? Usually, when I meet a python hater (or even an enthusiast) they say phrases like that\n \u0026ldquo;Python is slow because it\u0026rsquo;s an interpreted language!\u0026quot; \u0026ldquo;Python sucks because doesn\u0026rsquo;t have a compiler!\u0026quot;  Well, these assertions are not true. The important point is that: when people refer to python commonly they are actually talking about the language python and the CPython virtual machine. Let\u0026rsquo;s talk more about these misconceptions.\nFirst, the distinction between interpreted and compiled languages is very blurry today. Second, let\u0026rsquo;s see a nasty thing\nhello_world = \u0026quot;print('Hello, world!')\u0026quot; hello_world_obj = compile(hello_world, '\u0026lt;string\u0026gt;', 'single')  Yeah, if you\u0026rsquo;re trying to defend that python is interpreted the things start to get more hard for you. Why is there a compile available?\nexec(hello_world_obj)  Hello, world!  I\u0026rsquo;m executing a thing that has been compiled??? What is this hello_world_obj?\nprint(f\u0026quot;Bad news for you:\\n\\tContent: {hello_world_obj.co_code}\\n\\tType: {type(hello_world_obj.co_code)}\u0026quot;)  Bad news for you: Content: b'e\\x00d\\x00\\x83\\x01F\\x00d\\x01S\\x00' Type: \u0026lt;class 'bytes'\u0026gt;  But what is this stuff?\nIs important to understand what happens behind the scenes.\nAfter you write a python code and call the python command, python starts a compiling phase creating the ASTs; generating the bytecotes that will be attached to code objects, and then, these code objects will be interpreted by the CPython virtual machine. The diagram below is a simple representation of this process with some details hidden\ngraph LR; A[Source Code]--|parsing|B[Parse Tree]; B--C[AST]; C--E[Bytecode]; E--F[Code Object]; F--|execution by|G[CPython Virtual Machine];  The compilation phase are the firts steps of the above diagram\ngraph LR; A[Source Code]--|parsing|B[Parse Tree]; B--C[AST]; C--E[Bytecode]; E--F[Code Object];  But don\u0026rsquo;t worry about most of the big names above. The only concepts that will matter to us are the AST, bytecodes, and Code object. Bytecodes are just a compact way to tell the interpreter what we want to do. The code object is just a way to encapsulate the bytecodes extracted from the AST.\nBut how does this help us?\n Our solution will involve the manipulation of the AST and after that generating a new code object with the related manipulated AST!    A funny history from Luciano Ramalho: In 2018 I told a CBP officer I was entering the US to speak at PyCon. He asked: \u0026quot;Is Python interpreted or compiled?\u0026quot; After a 2 second pause I said \u0026quot;Interpreted\u0026quot;. I didn\u0026#39;t give the correct answer because I didn\u0026#39;t want to extend the \u0026quot;pleasant\u0026quot; conversation. He let me in.\n\u0026mdash; Luciano Ramalho ‚òî üêç ‚öó ‚ñ∂Ô∏èüò∑üíâüíâüíâ (@ramalhoorg) December 23, 2021   Extracting ASTs and interpreting them Let\u0026rsquo;s see a simple example of a function and the extracted AST.\nimport inspect import ast import astor # install this for pretty printing def example(a: float, b:float = 2) -\u0026gt; float: s = a+b return s tree = ast.parse(inspect.getsource(example)) print(astor.dump(tree)) astor.to_source(tree)  Module( body=[ FunctionDef(name='example', args=arguments(posonlyargs=[], args=[arg(arg='a', annotation=Name(id='float'), type_comment=None), arg(arg='b', annotation=Name(id='float'), type_comment=None)], vararg=None, kwonlyargs=[], kw_defaults=[], kwarg=None, defaults=[Constant(value=2, kind=None)]), body=[ Assign(targets=[Name(id='s')], value=BinOp(left=Name(id='a'), op=Add, right=Name(id='b')), type_comment=None), Return(value=Name(id='s'))], decorator_list=[], returns=Name(id='float'), type_comment=None)], type_ignores=[])  The above output is our AST, take some time looking into it to see how all our code stuff is organized. The image below shows the graph representation of the above output\n  Each element in the above output with an upper case letter is a node (Name, BinOp, FunctionDef, etc) from the base class ast.Node. One of the most important node types are the ast.Name. For example,\nvalue=BinOp(left=Name(id='a'), op=Add, right=Name(id='b')),  the ast.Name is used to refer a variable by the name, id.\nNow let\u0026rsquo;s come back to our problem. Remember that one bad solution it was rewriting every function\ndef func(x, y): random_var = np.random.uniform() ... #more local vars result = (x+y)**random_var return result  as\ndef func_transformed(x, y): random_var = np.random.uniform() ... #more local vars result = (x+y)**random_var return result, locals  The big stuff that we will do is to write a function that codes new functions for us! This is metaprogramming! And at same time we will write a decorator that will avoid any change in our codebase!\nHow can I be efficient in metaprogramming? We must create a function that generates a new one similar to func_transformed. How to get an idea of what we need to do?\nThe 6 simple steps  Create an example function Code the transformed function from the example function Code a simple test to check if the transformed function is correct Extract the AST from the example and the transformed function Compare the ASTs. What is the difference? Annotate this difference somewhere  You can use the difflib module that comes with python to diff strings   Creates a new and more complex example function and repeats the process until you get a good idea of what you need to do.  After you have a good idea of what you need to do, you can start writing your metaprogramming function.\nCreating our metaprogramming function First six-steps interaction Let\u0026rsquo;s start our first interaction writing one function, the expected transformed function and the test to check if it is correct.\ndef example_1(x, y): internal_var = 222 result = (x+y)**internal_var return result def example_1_expected(x, y): internal_var = 222 result = (x+y)**internal_var return result, locals() def test_meta_example_1(meta_func, x, y): expected_result, expected_locals = example_1_expected(x, y) result, locals_dict = meta_func(x, y) assert result == expected_result assert expected_locals == locals_dict  Everything looks fine. Now we will use the difflib to see the differences between the two ASTs.\nimport difflib from pprint import pprint example_1_ast_str = astor.dump_tree(ast.parse(inspect.getsource(example_1))) example_1_expected_str = astor.dump_tree(ast.parse(inspect.getsource(example_1_expected))) pprint( list( difflib.unified_diff(example_1_ast_str.splitlines(), example_1_expected_str.splitlines(), n=0) ) )  ['--- \\n', '+++ \\n', '@@ -3 +3 @@\\n', \u0026quot;- FunctionDef(name='example_1',\u0026quot;, \u0026quot;+ FunctionDef(name='example_1_expected',\u0026quot;, '@@ -19 +19 @@\\n', \u0026quot;- Return(value=Name(id='result'))],\u0026quot;, \u0026quot;+ Return(value=Tuple(elts=[Name(id='result'), \u0026quot; \u0026quot;Call(func=Name(id='locals'), args=[], keywords=[])]))],\u0026quot;]  Now we know that we must change this Node in the AST\nReturn(value=Name(id='result'))],  To this\nReturn(value=Tuple(elts=[Name(id='result'), Call(func=Name(id='locals'), args=[], keywords=[])]))],  How we can do this? With the help of NodeTransformer class\nThe NodeTransformer class The ast.NodeTransformer allows us to create objects with a walker-like interface. The walker will visit each node in the AST and during each visit, the walker can remove, replace, modify or add nodes, and after that, he can continue to walk to the childreens of the node or stop there.\nHow can we use this? First, we start by creating a new class derived from ast.NodeTransformer\nclass ASTTransformer(ast.NodeTransformer): def visit_Return(self, node):  If you want to interact/change/delete a node of type Something you must override the visit_Something method. Thus, because we need to change the Return node we override the visit_Return. If we do just the following, our walker will not change our AST,\nclass ASTTransformer(ast.NodeTransformer): ...  Let\u0026rsquo;s start the modifications. We need to create a new node responsible to call the locals\nclass ASTTransformer(ast.NodeTransformer): def visit_Return(self, node): node_locals = ast.Call( func=ast.Name(id='locals', ctx=ast.Load()), args=[], keywords=[] ) self.generic_visit(node) return node  We used a Name node to identify the locals function. Now, according to the diff result our Return node must be transformed into a Return of a Tuple node\nclass ASTTransformer(ast.NodeTransformer): def visit_Return(self, node): node_locals = ast.Call( func=ast.Name(id='locals', ctx=ast.Load()), args=[], keywords=[] ) new_node.value = ast.Tuple( elts=[ node.value, node_locals ], ctx=ast.Load() ) self.generic_visit(new_node) return new_node  A new thing appeared. The elts argument. But don\u0026rsquo;t worry, this is just an argument which tells what is the list of other nodes Tuple has. Whenever you have some doubt about AST stuff, you can check the ast documentation here. The documentation is simple to understand because python is simple!\nEverything is almost done. The last thing is to fix our AST. Because when we change the Node we need to fill missing information like the line_number and column_offset. Thanks to python we just need to call fix_missing_locations to fill this for us.\nclass ASTTransformer(ast.NodeTransformer): def visit_Return(self, node): new_node = node node_locals = ast.Call( func=ast.Name(id='locals', ctx=ast.Load()), args=[], keywords=[] ) new_node.value = ast.Tuple( elts=[ node.value, node_locals ], ctx=ast.Load() ) ast.copy_location(new_node, node) ast.fix_missing_locations(new_node) self.generic_visit(new_node) return new_node  Ok, let\u0026rsquo;s see if is working. We must instantiate our transformer and call the visit method that tells the walker to walk in the AST and do all the modification we\u0026rsquo;re asking\ntree_meta = ast.parse(inspect.getsource(example_1)) transformer = ASTTransformer() transformer.visit(tree_meta) example_1_meta_ast_str = astor.dump_tree(tree_meta) example_1_expected_str = astor.dump_tree(ast.parse(inspect.getsource(example_1_expected))) pprint( list( difflib.unified_diff(example_1_meta_ast_str.splitlines(), example_1_expected_str.splitlines(), n=0) ) )  ['--- \\n', '+++ \\n', '@@ -3 +3 @@\\n', \u0026quot;- FunctionDef(name='example_1',\u0026quot;, \u0026quot;+ FunctionDef(name='example_1_expected',\u0026quot;]  Our first iteration was successful! Let\u0026rsquo;s try a more complex example.\nThe second six-steps interaction We\u0026rsquo;ll just add more complexity without any particular meaning, we can be creative!\ndef example_2(x, y): internal_var = 222 def sub(x, y): ommit_this_var = 1 return x - y result = sub(x,y)**internal_var return (result, False) def example_2_expected(x, y): internal_var = 222 def sub(x, y): ommit_this_var = 1 return x - y result = sub(x,y)**internal_var return ((result, False), locals()) def test_meta_example_2(meta_func, x, y): expected_result, expected_locals = example_2_expected(x, y) result, locals_dict = meta_func(x, y) del locals_dict[\u0026quot;sub\u0026quot;] del expected_locals[\u0026quot;sub\u0026quot;] assert result == expected_result assert expected_locals == locals_dict  example_2_ast_str = astor.dump_tree(ast.parse(inspect.getsource(example_2))) example_2_expected_str = astor.dump_tree(ast.parse(inspect.getsource(example_2_expected))) pprint( list( difflib.unified_diff(example_2_ast_str.splitlines(), example_2_expected_str.splitlines(), n=0) ) )  ['--- \\n', '+++ \\n', '@@ -3 +3 @@\\n', \u0026quot;- FunctionDef(name='example_2',\u0026quot;, \u0026quot;+ FunctionDef(name='example_2_expected',\u0026quot;, '@@ -37 +37,4 @@\\n', \u0026quot;- Return(value=Tuple(elts=[Name(id='result'), \u0026quot; 'Constant(value=False, kind=None)]))],', '+ Return(', '+ value=Tuple(', \u0026quot;+ elts=[Tuple(elts=[Name(id='result'), \u0026quot; 'Constant(value=False, kind=None)]),', \u0026quot;+ Call(func=Name(id='locals'), args=[], \u0026quot; 'keywords=[])]))],']  Now, it\u0026rsquo;s time to cross the fingers and see if we need working more\ntree_meta = ast.parse(inspect.getsource(example_2)) transformer = ASTTransformer() transformer.visit(tree_meta) example_2_meta_ast_str = astor.dump_tree(tree_meta) example_2_expected_str = astor.dump_tree(ast.parse(inspect.getsource(example_2_expected))) pprint( list( difflib.unified_diff(example_2_meta_ast_str.splitlines(), example_2_expected_str.splitlines(), n=0) ) )  ['--- \\n', '+++ \\n', '@@ -3 +3 @@\\n', \u0026quot;- FunctionDef(name='example_2',\u0026quot;, \u0026quot;+ FunctionDef(name='example_2_expected',\u0026quot;, '@@ -27,4 +27 @@\\n', '- Return(', '- value=Tuple(', \u0026quot;- elts=[BinOp(left=Name(id='x'), op=Sub, \u0026quot; \u0026quot;right=Name(id='y')),\u0026quot;, \u0026quot;- Call(func=Name(id='locals'), args=[], \u0026quot; 'keywords=[])]))],', \u0026quot;+ Return(value=BinOp(left=Name(id='x'), op=Sub, \u0026quot; \u0026quot;right=Name(id='y')))],\u0026quot;]  Unfortunately, our ASTTransformer was not able to deal with this crazy guy. What is the problem? If you check carefully you will notice that the inner function def sub is the problem. We don\u0026rsquo;t want to change any \u0026ldquo;sub\u0026rdquo; function, so we need to tell our walker to avoid changing this kind of stuff. To do so, we will create a flag to tell if the walker is in a sub-function, and we will just override the visit_FunctionDef method to check this flag\nclass ASTTransformer(ast.NodeTransformer): def visit_FunctionDef(self, node): if self._sub: return node self._sub = True self.generic_visit(node) return node def visit_Module(self, node): self._sub = 0 self.generic_visit(node) def visit_Return(self, node): new_node = node node_locals = ast.Call( func=ast.Name(id='locals', ctx=ast.Load()), args=[], keywords=[] ) new_node.value = ast.Tuple( elts=[ node.value, node_locals ], ctx=ast.Load() ) ast.copy_location(new_node, node) ast.fix_missing_locations(new_node) self.generic_visit(new_node) return new_node  tree_meta = ast.parse(inspect.getsource(example_2)) transformer = ASTTransformer() transformer.visit(tree_meta) example_2_meta_ast_str = astor.dump_tree(tree_meta) example_2_expected_str = astor.dump_tree(ast.parse(inspect.getsource(example_2_expected))) pprint( list( difflib.unified_diff(example_2_meta_ast_str.splitlines(), example_2_expected_str.splitlines(), n=0) ) )  ['--- \\n', '+++ \\n', '@@ -3 +3 @@\\n', \u0026quot;- FunctionDef(name='example_2',\u0026quot;, \u0026quot;+ FunctionDef(name='example_2_expected',\u0026quot;]  Our new ASTTransformer was able to deal with our new complicated example!\nCreating a new function at runtime We have a ASTTransformer , now we must compile the transformed AST into a new function. In python, we can create a new function using the FunctionType, see below\nfrom types import FunctionType, CodeType def transform_and_compile(func: FunctionType)-\u0026gt;FunctionType: source = inspect.getsource(func) # we put this to remove the line from source code with the decorator source = \u0026quot;\\n\u0026quot;.join([l for l in source.splitlines() if not l.startswith(\u0026quot;@\u0026quot;)]) tree = ast.parse(source) transformer = ASTTransformer() transformer.visit(tree) code_obj = compile(tree, func.__code__.co_filename, 'exec') function_code = [c for c in code_obj.co_consts if isinstance(c, CodeType)][0] # we must to pass the globals context to the function transformed_func = FunctionType(function_code, func.__globals__) return transformed_func  test_meta_example_1(transform_and_compile(example_1), 4, 2) test_meta_example_2(transform_and_compile(example_2), 1, 2)  The transform_and_compile was able to create new functions that passed in all the tests! We can now move further to the final and easy step which is just to integrate this function with our decorator!\nIntegrating the AST manipulation with a decorator We will call the transform_and_compile right after the def decorate to avoid unnecessary compilations every time that the decorated function is called.\ndef report(fmt): def decorate(func): meta_func = transform_and_compile(func) ....  Inside def decorated we call the meta_func and return just the result because we don\u0026rsquo;t want to change our codebase.\ndef report(fmt): def decorate(func): meta_func = transform_and_compile(func) ... def decorated(*_args): _result, internal_locals = meta_func(*_args) .... return _result  With all the stuff we learned in the previous post our report decorator with the above changes will be\ndef report(fmt): def decorate(func): meta_func = transform_and_compile(func) sig = inspect.signature(func) def decorated(*_args): _result, internal_locals = meta_func(*_args) named_args = {} num_args = len(_args) for i, (k, v) in enumerate(sig.parameters.items()): if i \u0026lt; num_args: named_args[k] = repr(_args[i]) else: named_args[k] = repr(v.default) name = func.__name__ result = repr(_result) args_dict = { **internal_locals, **locals(), **named_args } print(fmt.format(**args_dict)) # store the information in some place return result return decorated return decorate  Let\u0026rsquo;s see the result with a dummy function\n@report(fmt='{name}(a={a}, b={b}, c={c}); sum_ab {sum_ab}, diff_ab {dif_ab}; r={result}') def dummy_example(a, b, c=2): sum_ab = a + b dif_ab = a - b r = sum_ab**c + dif_ab**c return r r = dummy_example(2, 3, 1) print(\u0026quot;r:\u0026quot;, r)  dummy_example(a=2, b=3, c=1); sum_ab 5, diff_ab -1; r=4 r: 4  I know this post is quite hard to read, but I think it\u0026rsquo;s worth to share it. I hope you enjoyed it!\n","date":1649376000,"expirydate":-62135596800,"kind":"page","lang":"pt-br","lastmod":1649376000,"objectID":"e74dded76de309afa76cd783d7fd24d3","permalink":"/pt-br/post/python_ast_and_decorators_with_locals/","publishdate":"2022-04-08T00:00:00Z","relpermalink":"/pt-br/post/python_ast_and_decorators_with_locals/","section":"post","summary":"Is python interpreted or compiled? Knowing the nuances of the answer will help us build meta code using AST manipulation at runtime. I'll show you how to construct a decorator that can introspect in the local variables without tracing tricks!","tags":["python","ast","compilers","decorators","debugging","hacks"],"title":"Going meta with python: manipulating ASTs to create an introspective decorator at runtime [draft]","type":"post"},{"authors":null,"categories":["Python"],"content":"Lista de Conte√∫dos    Gaining a deeper understanding about the execution context of a function  The Fluent Python Book example Current issues and limitations   Creating an introspective code with the inspect module  A decorator that validates arguments using mathematical notation Going back to the Fluent python example   How to expose the locals() inside of a decorator?  Call stack and frames in python Using sys.trace to track our frames Let\u0026rsquo;s solve our problem   Conclusion and next steps  \u0026ldquo;\u0026hellip;it depends\u0026rdquo; The next step: we don\u0026rsquo;t need a trace! We can do better using AST manipulation SimpleNamespace for dict.key instead of dict[\u0026ldquo;key] Want to know more about call stack , inspect and trace?         Fluent Python is the best resource to learn to use and love python. Some days ago I was reading a section of the chapter 7: \u0026ldquo;Function Decorators and Closures\u0026rdquo;. This chapter has a lot of interesting and cool examples. Here I\u0026rsquo;ll discuss one of them and how I tried to put more shiny stuff in it.\n  A book that every python programmer should read.   Gaining a deeper understanding about the execution context of a function The Fluent Python Book example Ramalho‚Äôs book presents us with a @clock decorator that can be used to decorate a method, measure the time it takes to execute, and print in a human-readable format the arguments and name of the method. The example is shown below:\nimport time DEFAULT_FMT = '[{elapsed:0.8f}s] {name}({args}) -\u0026gt; {result}' def clock(fmt=DEFAULT_FMT): def decorate(func): def clocked(*_args): t0 = time.time() _result = func(*_args) elapsed = time.time() - t0 name = func.__name__ args = ', '.join(repr(arg) for arg in _args) result = repr(_result) log_string = fmt.format(**locals()) # send to somewhere # csv, ELK, etc print(log_string) return result return clocked return decorate @clock('[{elapsed:0.8f}s] {name}({args})') def snooze(seconds): time.sleep(seconds) return time.time() for _ in range(3): snooze(.123)  [0.12315798s] snooze(0.123) [0.12315822s] snooze(0.123) [0.12317085s] snooze(0.123)  If you don\u0026rsquo;t understand something in the above code I recommend that you take some time searching and reading about each aspect. There are many cool things being used there, for example:\n repr which is a function that returns a string representation of an object.  This is essential because the DEFAULT_FMT is a string, not a f-string, we can\u0026rsquo;t just put a generic object to be printed in DEFAULT_FMT.   log_string = fmt.format(**locals()): instead of creating a repetitive code like fmt.format(**{\u0026quot;result\u0026quot;:result, \u0026quot;args\u0026quot;:args, ...}) we can just use the locals() which is a dictionary that contains all the local variables of the current scope.  When I study something I always like to create a fresh problem with the stuff that I\u0026rsquo;ve learned and try to solve it. Sometimes there is no solution. But even if there is no solution, we still learn other stuff.\nI\u0026rsquo;ve started by creating the following example:\nimport numpy as np @clock('[{elapsed:0.8f}s] {name}({args})') def snooze_and_snore(seconds, snore_loud, min_prob_to_snore=0.4): time.sleep(seconds) to_snore = np.random.uniform() \u0026gt; min_prob_to_snore if to_snore: if snore_loud: pass # r.requets(wake_up_everyone) pass return time.time() for _ in range(3): snooze_and_snore(.4, True, .1) snooze_and_snore(.4, False, .1)  [0.40229130s] snooze_and_snore(0.4, True, 0.1) [0.40049720s] snooze_and_snore(0.4, False, 0.1) [0.40058565s] snooze_and_snore(0.4, True, 0.1) [0.40013075s] snooze_and_snore(0.4, False, 0.1) [0.40052223s] snooze_and_snore(0.4, True, 0.1) [0.40057564s] snooze_and_snore(0.4, False, 0.1)  Ok, what are the problems/issues/limitations that the above code showed me?\nCurrent issues and limitations  We don\u0026rsquo;t have information about the names of the arguments passed to the method.  If the list of arguments is long, trying to understand what is happening becomes a hard task. Because we are increasing the amount of stuff that we must keep in our mind. We are increasing the cognitive load in the terms presented in the excelsior book: A Philosophy of Software Design. A person who is not familiar with the codebase cannot understand what is happening by analyzing the outputs of the decorator. If these outputs are being stored in the ELK stack, this will be unproductive.   We have the locals() information from the decorator which is fed by the result of the decorated method. However, we can\u0026rsquo;t get any information about the locals() of the decorated method. Why is this bad?  The final internal state of the method is commonly used to understand the execution of a method. Sometimes a method depends on random variables defined in the local context. Thus, the same set of arguments can give different executions. Until now, we don\u0026rsquo;t have a way to get the locals() of the decorated method. For example, in the snooze_and_snore we can\u0026rsquo;t know if the person snored or not.    We will attack the first issue using the inspect module. As I\u0026rsquo;ll show you, we can do cool things with this module.\n If you know about sys.trace, call stack and inspect.signatures I recommend you go directly to the section Let\u0026rsquo;s solve our problem   Creating an introspective code with the inspect module The inspect module is a Python standard library that provides several tools to help you to introspect and consequently learn about live objects like functions, modules, classes, instances, frame objects (I\u0026rsquo;ll talk about frames later in this post), etc. Well, what can you do with this? Really, a lot of things. You can use it to automatically create documentation, parse the docstrings, manipulate the AST, etc.\nA decorator that validates arguments using mathematical notation In the last years, we have seen the development of the typing module and the mypy static analysis tool for python. This module and tool can be very useful sometimes. However, it doesn\u0026rsquo;t provide some features that are essential for proper validation. But at least in my experience creating code for my Ph.D., I usually don\u0026rsquo;t need so much sophisticated type theory and validation to be able to write a good code for a mathematical modeling tool. Most of the mathematical validation that I need is just checking if an argument still satisfies some constraints or lives in a proper subspace. If not, I need to raise an exception or perform some kind of regularization.\nLet\u0026rsquo;s create a decorator that will validate arguments using simple mathematical notation.\nWe will create a dictionary that will contain the annotation as a key and the value will be a human-readable description of the annotation and a method responsible for check if everything is right.\nimport inspect MATH_SPACES = { \u0026quot;\\in R\u0026quot;: {\u0026quot;name\u0026quot; : \u0026quot;real space\u0026quot;, \u0026quot;validator\u0026quot;: lambda x: isinstance(x, (int, float))}, \u0026quot;\\in R_+\u0026quot;: {\u0026quot;name\u0026quot;: \u0026quot;space of real numbers greater than zero\u0026quot;, \u0026quot;validator\u0026quot;: lambda x: isinstance(x, (int, float)) and x \u0026gt; 0}, }  We will use the inspect.signature to get the annotations of each argument of the decorated method. For example, if the decorated method is def foo(a: '\\in R', b) the inspect.signature(foo) will return an object which we can use to extract an ordered dictionary with the arguments and the annotations. Like this\ndef foo(a: \u0026quot;\\in R\u0026quot;, b, c:int, d= 2): pass for k, v in inspect.signature(foo).parameters.items(): print(k, v, type(v._annotation), v.default)  a a: '\\\\in R' \u0026lt;class 'str'\u0026gt; \u0026lt;class 'inspect._empty'\u0026gt; b b \u0026lt;class 'type'\u0026gt; \u0026lt;class 'inspect._empty'\u0026gt; c c: int \u0026lt;class 'type'\u0026gt; \u0026lt;class 'inspect._empty'\u0026gt; d d=2 \u0026lt;class 'type'\u0026gt; 2  Let\u0026rsquo;s create our decorator. It should be really simple. Just check if we should verify the argument and if so, check if the value respects the annotated mathematical space.\ndef math_validator(): def decorate(func): def decorated(*_args): sig = inspect.signature(func) # sig parameters is an ordered dict for i, (k, v) in enumerate(sig.parameters.items()): annotation = v._annotation if not isinstance(annotation, str): continue if not annotation in MATH_SPACES: print(f\u0026quot;{annotation} is not implemented in Math Spaces\u0026quot;) continue # skip if we didn't implement this space validation if not MATH_SPACES[annotation][\u0026quot;validator\u0026quot;](_args[i]): raise ValueError(f\u0026quot;{k} doesn't belong to the {MATH_SPACES[annotation]['name']}\u0026quot;) result = func(*_args) print(f\u0026quot;{func.__name__}({_args}) -\u0026gt; {result}\u0026quot;) return result return decorated return decorate  @math_validator() def simple_method(x: \u0026quot;\\in R\u0026quot;, y: \u0026quot;\\in R_+\u0026quot;, z: float = 2) -\u0026gt; float: \u0026quot;\u0026quot;\u0026quot;Simple method to add two numbers together and divide by the last number Args: x: The first number to add. y: The second number to add. z: it is a float number that will be the power of the result. This will not be checked for math spaces. Returns: float: result \u0026quot;\u0026quot;\u0026quot; result = (x+y)/y return result**z simple_method(1, 2) simple_method(1, 0)  simple_method((1, 2)) -\u0026gt; 1.5 ---\u0026gt; 19 simple_method(1, 0) ... \u0026lt;locals\u0026gt;.decorate.\u0026lt;locals\u0026gt;.decorated(*_args) 11 continue 13 if not MATH_SPACES[annotation][\u0026quot;validator\u0026quot;](_args[i]): ---\u0026gt; 14 raise ValueError(f\u0026quot;{k} doesn't belong to the {MATH_SPACES[annotation]['name']}\u0026quot;) 15 result = func(*_args) 16 print(f\u0026quot;{func.__name__}({_args}) -\u0026gt; {result}\u0026quot;) ValueError: y doesn't belong to the space of real numbers greater than zero  Our decorator is quite simple but does the job. You can go deeper into this and use a more sophisticated mathematical notation, printing using latex, etc. But now, let\u0026rsquo;s go back to the Python Fluent example because the inspect.signature already provides us with a way to solve the first limitation!\nGoing back to the Fluent python example Let\u0026rsquo;s remember one thing that I\u0026rsquo;ve pointed out:\n A person who is not familiar with the code base will not be able to understand what is happening just by analyzing the outputs of the decorator.\n It\u0026rsquo;s obvious that we can overcome this issue by using the inspect module. Let\u0026rsquo;s create a more elaborated example using monkeys and a zookeeper that must record and report the information about how the life of the monkeys are going.\nNUM_MONKEYS = 20 def feed_monkeys(n_bananas, n_apples=0): monkeys = { f\u0026quot;monkey_{i}\u0026quot;: {\u0026quot;bananas\u0026quot;: 0, \u0026quot;apples\u0026quot;: 0} for i in range(NUM_MONKEYS) } while n_bananas \u0026gt; 0 and n_apples \u0026gt; 0: if np.random.uniform() \u0026lt; 0.4: continue monkey = monkey[np.random.choice(list(monkeys.keys()))] if n_bananas \u0026gt; 0: monkey[\u0026quot;bananas\u0026quot;] += 1 n_bananas -= 1 if n_apples \u0026gt; 0: monkey[\u0026quot;apples\u0026quot;] += 1 n_apples -= 1 if n_apples == 0 and n_bananas == 0: break  My solution is the @report decorator presented below.\ndef report(fmt=DEFAULT_FMT): def decorate(func): def decorated(*_args): sig = inspect.signature(func) named_args = {} num_args = len(_args) for i, (k, v) in enumerate(sig.parameters.items()): if i \u0026lt; num_args: named_args[k] = repr(_args[i]) else: named_args[k] = repr(v.default) t0 = time.time() _result = func(*_args) elapsed = time.time() - t0 name = func.__name__ result = repr(_result) args_dict = { **locals(), **named_args} del args_dict['_args'] print(fmt.format(**args_dict)) # store the information in some place return result return decorated return decorate  What is important here are the following statements:\nsig = inspect.signature(func) named_args = {} num_args = len(_args) for i, (k, v) in enumerate(sig.parameters.items()): if i \u0026lt; num_args: named_args[k] = repr(_args[i]) else: named_args[k] = repr(v.default)  We are iterating over the signature parameters and checking if it passed the value to func. If not, we extract the default value from the signature.\nUsing the @report decorator in the feed_monkeys we have this output:\nNUM_MONKEYS = 20 @report('The zookeeper feeds the monkeys with {n_bananas} bananas and {n_apples} apples. Time to feed: {elapsed:0.4f}s') def feed_monkeys(n_bananas, n_apples=0): monkeys = { f\u0026quot;monkey_{i}\u0026quot;: {\u0026quot;bananas\u0026quot;: 0, \u0026quot;apples\u0026quot;: 0} for i in range(NUM_MONKEYS) } while n_bananas \u0026gt; 0 and n_apples \u0026gt; 0: if np.random.uniform() \u0026lt; 0.4: continue monkey = monkeys[np.random.choice(list(monkeys.keys()))] if n_bananas \u0026gt; 0: monkey[\u0026quot;bananas\u0026quot;] += 1 n_bananas -= 1 if n_apples \u0026gt; 0: monkey[\u0026quot;apples\u0026quot;] += 1 n_apples -= 1 if n_apples == 0 and n_bananas == 0: break for _ in range(3): feed_monkeys(np.random.randint(10, 100)) feed_monkeys(np.random.randint(10, 100), 10)  The zookeeper feeds the monkeys with 69 bananas and 0 apples. Time to feed: 0.0000s The zookeeper feeds the monkeys with 92 bananas and 10 apples. Time to feed: 0.0011s The zookeeper feeds the monkeys with 58 bananas and 0 apples. Time to feed: 0.0000s The zookeeper feeds the monkeys with 53 bananas and 10 apples. Time to feed: 0.0048s The zookeeper feeds the monkeys with 42 bananas and 0 apples. Time to feed: 0.0000s The zookeeper feeds the monkeys with 51 bananas and 10 apples. Time to feed: 0.0025s  First issue solved! But our decorator is still not useful to the zookeeper and managers. We can‚Äôt know how good any monkey is doing or if there is any monkey that eats too much. You could already know that somehow we must have a way to access the monkeys' dictionary inside our def decorated method. Unfortunately, this is not a trivial task in python because it lacks namespaces decorators. But we also can overcome this with a little trick using a trace tool.\nHow to expose the locals() inside of a decorator? Now we just need to access the local variables of the decorated method. Let\u0026rsquo;s think more deeply about this:\n After the execution of the decorated method, all the information about the local variables is lost. Fortunately, we don\u0026rsquo;t want irrelevant information occupying our system memory. The decorator will call the decorated method and will receive the return value. Thus, there is no way to extract the local variables because now there are no more local variables!  How to solve it? Well, think first about where the local variables have been stored before being erased.\nCall stack and frames in python If you came from a non-CS background, maybe you don\u0026rsquo;t know about an important concept called the call stack. A call stack is a data structure that stores information related to living things in our program.\nIf you call a function in python, a new block of information (frame) is pushed to the top of the call stack. After the function returns the value, this block of information is popped off the call stack. This comprehension can give insights into how to do things in python and how to create good or strange behaviors.\nWell, you can think. If the elements of the call stack are always added on the top if a function (inner) is called by another function (outer) can I access the values of the local variables from the outer function inside of the inner? Yes, you can! Obviously, this is not always a good idea, but it\u0026rsquo;s good to understand this concept. Because this approach can be useful to deal with rigid frameworks like Django.\n%%writefile test_stack.py import inspect N_BANANAS = 12 def outer_call(n_bananas): var_inside_outer_call = 2 n_bananas += 1 inner_call(n_bananas) def inner_call(n_bananas): var_inside_inner_call = {\u0026quot;monkey\u0026quot;: 0} frame_infos = inspect.stack() n_frames = len(frame_infos) frames_var_values = { f.function: [(k, v) for k, v in f.frame.f_locals.items()] for f in frame_infos } for i, (function, frame_local) in enumerate(frames_var_values.items()): print(f'\\n\\t {function} stack position: {n_frames - i}') for var_name, value in frame_local: print(f'\\t\\t Name: {var_name:25s}Type: {type(value)}') if var_name in ('n_bananas', 'N_BANANAS', 'var_inside_outer_call'): print(f'\\t\\t\\t Value: {value}') print(\u0026quot;\\n Before outer_call() call\u0026quot;) outer_call(N_BANANAS) print(\u0026quot;\\n After outer_call() call\u0026quot;) frames = [ [(k, v) for k, v in f.frame.f_locals.items()] for f in inspect.stack() ] for frame_local in frames: for var_name, value in frame_local: print(f'\\t\\t Name: {var_name:25s}Type: {type(value)}')  Overwriting test_stack.py  !python test_stack.py   Before outer_call() call inner_call stack position: 3 Name: n_bananas Type: \u0026lt;class 'int'\u0026gt; Value: 13 Name: var_inside_inner_call Type: \u0026lt;class 'dict'\u0026gt; Name: frame_infos Type: \u0026lt;class 'list'\u0026gt; Name: n_frames Type: \u0026lt;class 'int'\u0026gt; outer_call stack position: 2 Name: n_bananas Type: \u0026lt;class 'int'\u0026gt; Value: 13 Name: var_inside_outer_call Type: \u0026lt;class 'int'\u0026gt; Value: 2 \u0026lt;module\u0026gt; stack position: 1 Name: __name__ Type: \u0026lt;class 'str'\u0026gt; Name: __doc__ Type: \u0026lt;class 'NoneType'\u0026gt; Name: __package__ Type: \u0026lt;class 'NoneType'\u0026gt; Name: __loader__ Type: \u0026lt;class '_frozen_importlib_external.SourceFileLoader'\u0026gt; Name: __spec__ Type: \u0026lt;class 'NoneType'\u0026gt; Name: __annotations__ Type: \u0026lt;class 'dict'\u0026gt; Name: __builtins__ Type: \u0026lt;class 'module'\u0026gt; Name: __file__ Type: \u0026lt;class 'str'\u0026gt; Name: __cached__ Type: \u0026lt;class 'NoneType'\u0026gt; Name: inspect Type: \u0026lt;class 'module'\u0026gt; Name: N_BANANAS Type: \u0026lt;class 'int'\u0026gt; Value: 12 Name: outer_call Type: \u0026lt;class 'function'\u0026gt; Name: inner_call Type: \u0026lt;class 'function'\u0026gt; After outer_call() call Name: __name__ Type: \u0026lt;class 'str'\u0026gt; Name: __doc__ Type: \u0026lt;class 'NoneType'\u0026gt; Name: __package__ Type: \u0026lt;class 'NoneType'\u0026gt; Name: __loader__ Type: \u0026lt;class '_frozen_importlib_external.SourceFileLoader'\u0026gt; Name: __spec__ Type: \u0026lt;class 'NoneType'\u0026gt; Name: __annotations__ Type: \u0026lt;class 'dict'\u0026gt; Name: __builtins__ Type: \u0026lt;class 'module'\u0026gt; Name: __file__ Type: \u0026lt;class 'str'\u0026gt; Name: __cached__ Type: \u0026lt;class 'NoneType'\u0026gt; Name: inspect Type: \u0026lt;class 'module'\u0026gt; Name: N_BANANAS Type: \u0026lt;class 'int'\u0026gt; Name: outer_call Type: \u0026lt;class 'function'\u0026gt; Name: inner_call Type: \u0026lt;class 'function'\u0026gt;  First, draw your attention here\nouter_call stack position: 2 Name: n_bananas Type: \u0026lt;class 'int'\u0026gt; Value: 13 Name: var_inside_outer_call Type: \u0026lt;class 'int'\u0026gt; Value: 2  Even if we don\u0026rsquo;t pass a variable as an argument to the inner_call function, this variable can be accessed because still lives in the call stack! As I‚Äôve told you, after the execution of outer_call the call stack doesn\u0026rsquo;t have any information about what happened inside our functions. This discussion will help us to understand the limitations of our solution. Because our solution is just to watch the call stack and keep the frame before being popped off!\nUsing sys.trace to track our frames Some time ago I\u0026rsquo;ve talked about how to dissect a process using lsof and strace: Dissecting processes and failures in Linux with lsof and strace. The strace is a tracing tool that intercepts and records in someplace any system call made by a process. Python has a built-in tool to do this kind of stuff. Thus, let\u0026rsquo;s use it to track our frames.\nLet\u0026rsquo;s solve our problem We will ask our code to monitor any call made with the decorated function. To do so, we will create a new function that will do this and release the trace after the execution of the decorated function.\nimport sys def call_and_extract_frame(func, *args, **kwargs): frame_var = None trace = sys.gettrace() def update_frame_var(stack_frame, event_name, arg_frame): \u0026quot;\u0026quot;\u0026quot; Args: stack_frame: (frame) The current stack frame. event_name: (str) The name of the event that triggered the call. Can be 'call', 'line', 'return' and 'exception'. arg_frame: Depends on the event. Can be a None type \u0026quot;\u0026quot;\u0026quot; nonlocal frame_var # nonlocal is a keyword which allows us to change the variable in the outer scope if event_name != 'call': return trace frame_var = stack_frame sys.settrace(trace) return trace sys.settrace(update_frame_var) try: func_result = func(*args, **kwargs) finally: sys.settrace(trace) return frame_var, func_result  Now to use this trick, we just need to call the above function in our @report decorator. Like this:\ndef report(formater): def decorate(func): def decorated(*_args): sig = inspect.signature(func) named_args = {} num_args = len(_args) for i, (k, v) in enumerate(sig.parameters.items()): if i \u0026lt; num_args: named_args[k] = repr(_args[i]) else: named_args[k] = repr(v.default) ### Our modifications frame_func, _result = call_and_extract_frame(func, *_args) name = func.__name__ result = repr(_result) args_dict = { **named_args, **locals(), **frame_func.f_locals, } ### print(formater.format(**args_dict)) # do other stuff here return _result return decorated return decorate  Let\u0026rsquo;s see the results:\n@report(' Monkey {gluttonous_monkey} ate too much bananas. Num monkeys {num_monkeys}') def feed_monkeys(n_bananas): num_monkeys = 3 monkeys = { f\u0026quot;monkey_{i}\u0026quot;: {\u0026quot;bananas\u0026quot;: 0} for i in range(num_monkeys) } while n_bananas \u0026gt; 0: if np.random.uniform() \u0026lt; 0.4: continue monkey = monkeys[np.random.choice(list(monkeys.keys()))] if n_bananas \u0026gt; 0: monkey[\u0026quot;bananas\u0026quot;] += 1 n_bananas -= 1 gluttonous_monkey = max(monkeys, key=lambda k: monkeys[k][\u0026quot;bananas\u0026quot;]) for _ in range(3): feed_monkeys(np.random.randint(10, 100))   The monkey monkey_0 eat too much bananas. Num monkeys 3 The monkey monkey_1 eat too much bananas. Num monkeys 3 The monkey monkey_2 eat too much bananas. Num monkeys 3  Conclusion and next steps \u0026ldquo;\u0026hellip;it depends\u0026rdquo; Nice! It worked. But should you use it?\n   We have drawbacks in our approach:  a tracing always creates a cost. Thus, is expected that we will reduce the performance of our system. If you use this just for debugging purposes, it\u0026rsquo;s ok. can have conflicts with other tools and libs that also trying to use the trace tool it seems dirty!    The next step: we don\u0026rsquo;t need a trace! We can do better using AST manipulation  Using the inspect module to get the argument names it\u0026rsquo;s ok but I\u0026rsquo;ve told you the trace tool can be problematic. But we can replace the trace with another approach. Although, it\u0026rsquo;s more conceptually complex don\u0026rsquo;t require dirty tricks and I believe it\u0026rsquo;s far more beautiful. The next post it\u0026rsquo;s about this!  SimpleNamespace for dict.key instead of dict[\u0026ldquo;key] We have a minor issue and point of improvement. If you\u0026rsquo;re an cautious developer, probably you notice a flaw here\nargs_dict = { **named_args, **locals(), **frame_func.f_locals, }  if any of the dicts have common keys, one of them will overwrite the other. This is not what we want. You can use a simple solution like this:\nargs_dict = { \u0026quot;args\u0026quot;: **named_args, **locals(), \u0026quot;func_locals\u0026quot;: **frame_func.f_locals, }  But this is still annoying because we can do this with a format string:\n@report(fmt=\u0026quot;{args['n_bananas']} ...\u0026quot;)  Well, how to solve it? Just use a SimpleNamespace to construct an object!\nfrom types import SimpleNamespace def report(formater): def decorate(func): def decorated(*_args): sig = inspect.signature(func) named_args = {} num_args = len(_args) for i, (k, v) in enumerate(sig.parameters.items()): if i \u0026lt; num_args: named_args[k] = repr(_args[i]) else: named_args[k] = repr(v.default) ### Our modifications frame_func, _result = call_and_extract_frame(func, *_args) name = func.__name__ result = repr(_result) args_dict = { \u0026quot;args\u0026quot;: SimpleNamespace(**named_args), \u0026quot;args_repr\u0026quot;: repr(SimpleNamespace(**named_args)), **locals(), **frame_func.f_locals, } ### print(formater.format(**args_dict)) # do other stuff here return _result return decorated return decorate @report( \u0026quot;\u0026quot;.join(( 'The zookeeper feeds the monkeys with {args.n_bananas},', 'bananas. We loost {n_bananas} bananas. Args {args_repr}' )) ) def feed_monkeys(n_bananas): num_monkeys = 3 monkeys = { f\u0026quot;monkey_{i}\u0026quot;: {\u0026quot;bananas\u0026quot;: 0} for i in range(num_monkeys) } while n_bananas \u0026gt; 0: if np.random.uniform() \u0026gt; .8: # \u0026quot;bananas rotted . Monkeys will not eat any banana any more\u0026quot;) break if np.random.uniform() \u0026lt; 0.4: continue monkey = monkeys[np.random.choice(list(monkeys.keys()))] if n_bananas \u0026gt; 0: monkey[\u0026quot;bananas\u0026quot;] += 1 n_bananas -= 1 gluttonous_monkey = max(monkeys, key=lambda k: monkeys[k][\u0026quot;bananas\u0026quot;]) for _ in range(3): feed_monkeys(np.random.randint(10, 100))  The zookeeper feeds the monkeys with 15,bananas. We loost 15 bananas. Args namespace(n_bananas='15') The zookeeper feeds the monkeys with 80,bananas. We loost 77 bananas. Args namespace(n_bananas='80') The zookeeper feeds the monkeys with 95,bananas. We loost 92 bananas. Args namespace(n_bananas='95')  Want to know more about call stack , inspect and trace?  Call stack and frames: Reza Bagheri explained here how to add a tail-call optimization in python using python stack frames. Fluent Python book by Luciano Ramalho Python documentation: tracebak, inspect and stack.  Stackoverflow discussion  ","date":1649030400,"expirydate":-62135596800,"kind":"page","lang":"pt-br","lastmod":1649030400,"objectID":"39fabeb5afed150382c98e2a98405995","permalink":"/pt-br/post/python_decorator_that_exposes_locals/","publishdate":"2022-04-04T00:00:00Z","relpermalink":"/pt-br/post/python_decorator_that_exposes_locals/","section":"post","summary":"Here I will discuss how to use the inspect module and the sys.trace to introspect in a python method using a decorator. With that, we can create a lot cool things like mathematical validation for our arguments, a report generator or any other kind of introspection.","tags":["python","decorators","inspect","call stack","debugging","SimpleNamespace"],"title":"An introspective python decorator using  stack frames and the inspect module","type":"post"},{"authors":null,"categories":["Data analysis"],"content":"Introdu√ß√£o  Esse post √© bem informal e foi feito para o grupo de estudos de MlOps. O conte√∫do pode mudar significativamente com o passar do tempo.   Quando olhamos uma imagem temos a tend√™ncia de procurar padr√µes o que reduz o esfor√ßo e tempo necess√°rio para identificar do que se trata. Em an√°lise de dados filtros podem ser aplicados com a mesma motiva√ß√£o.\nEnquanto o processo de filtragem em um conjunto de pontos √© apresentado em cursos acad√™micos e tutoriais, existe pouco material em rela√ß√£o a grafos. Portanto, criei esse post para discutir o conceito de filtragem e padr√µes em grafos e as diferentes maneiras de se obter tal filtragem. Tentei ser did√°tico o suficiente para que uma pessoa fora da computa√ß√£o ou exatas (que esteja iniciando em dados) consiga compreender o texto. Sinta-se √† vontade para pular qualquer se√ß√£o do post :)\n Grafos, redes e redes complexas s√£o praticamente o mesmo conceito. Portanto, voc√™ pode encontrar termos como filtering edges on complex networks.   Os exemplos desse post usam python e as seguintes bibliotecas:\n$ python3 -m pip install numpy matplotlib networkx  Lista de Conte√∫dos  Introdu√ß√£o O que √© um grafo? O que √© filtragem? Confus√µes sobre o que √© filtragem em grafos Algumas propriedades de grafos  Componentes Comunidades  Caminho 1: Inferir Caminho 2: Quantificar/Descrever Caminho 3: Visualizar     Filtros  Estrutural: threshold  Pontos positivos Pontos negativos   Estat√≠stico: quebrando a varinha, processo de Dirichlet  Pontos positivos Pontos negativos        O que √© um grafo? Um grafo √© uma estrutura de dados que voc√™ constantemente est√° em contato. Alguns exemplos: sua rede de seguidores e seguidores no twitter, as transa√ß√µes financeiras associadas a sua chave PIX, as rela√ß√µes de reposit√≥rio e contribui√ß√µes no github, etc.\nUm grafo armazena objetos que t√™m rela√ß√µes pares a pares entre si. Sendo que √© poss√≠vel associar a cada objeto ou rela√ß√£o um outro tipo de dado gen√©rico tais como um n√∫mero real, um vetor, uma imagem ou mesmo outro grafo.\nA imagem abaixo representa um grafo dirigido formado por 4 v√©rtices.\ngraph TD; A--\u0026gt;B; B--\u0026gt;A; A--\u0026gt;C; B--\u0026gt;D; C--\u0026gt;D;  Vamos usar a letra $G$ para representar um grafo. A letra $V$ para o conjunto de v√©rtices (objetos) e $E$ para o conjunto de arestas (rela√ß√µes). Na imagem acima nosso grafo seria dado ent√£o pelo conjunto $V=\\{A,B,C,D\\}$ e $E=\\{(A,B), (B,A), (A,C), (B,D), (C,D)\\}$.\nComo disse no in√≠cio desta se√ß√£o √© poss√≠vel associar coisas tanto as arestas quanto os v√©rtices. Por exemplo, o grafo abaixo poderia representar transa√ß√µes financeiras entre 3 pessoas e o valor que cada uma tem em sua conta corrente\ngraph TD; A[A R$100,00]--|R$1|B; B[B R$3,00]--|R$2|A; C[C R$0]--|R$0,50|A;  Tais grafos de transa√ß√µes financeiras s√£o usados, por exemplo, para detectar crimes de lavagem de dinheiro, forma√ß√£o de quadrilhas e fraudes quando o comportamento de um dado cliente √© an√≥malo. Os valores nas arestas s√£o os pesos do grafo.\nO que √© filtragem? Filtro tem origem na palavra feltro. O feltro era o material feito principalmente de l√£ usado antigamente para separar um l√≠quido de suas impurezas. Um filtro em an√°lise de dados √© a mesma coisa: uma ferramenta que separa um conjunto de dados de uma sujeira, ru√≠do. Portanto, assim como para filtrar uma bebida temos que decidir antes algumas coisas:\n O que queremos que seja removido? O qu√£o eficiente √© nosso filtro? Qual √© o resultado esperado?  Filtragem para remover ru√≠dos Talvez a primeira coisa que vem √† sua cabe√ßa quando ouve a palavra filtro √© Instagram. Alguns filtros de fotos feitos para embelezar nada mais s√£o que um filtro para remo√ß√£o de ru√≠dos.\n  Imagem original e imagem com contamina√ß√£o de um ru√≠do.   O que consideramos ru√≠do depende das respostas das perguntas que levantei anteriormente. Um ru√≠do em uma imagem pode ser uma contribui√ß√£o esp√∫ria devido ao sensor de uma c√¢mera ser ruim. Um ru√≠do pode ser tamb√©m algo intr√≠nseco, por exemplo os poros e rugas na sua pele.\nFiltragem para ressaltar caracter√≠sticas e Gestalt  Os princ√≠pios de Gestalt s√£o suposi√ß√µes de certas leis sobre como a mente humana processa imagens atrav√©s do reconhecimento de padr√µes. Em resumo, tal princ√≠pio estabelece que a percep√ß√£o n√£o √© baseada em elementos individuais, mas em padr√µes em que os elementos s√£o arranjados ou t√™m contrastes entre si. Voc√™ n√£o compreende uma imagem analisando cada pixel individualmente, mas como os pixels se organizam e diferem entre si!\n  Os principios da Gestalt s√£o apresentados nessa figura. [].   Como se relaciona com os grafos? Um dos porqu√™s para realizar a filtragem de um grafo consiste em remover rela√ß√µes (arestas) esp√∫rias para ressaltar um dado padr√£o que queremos analisar. Comumente, esse padr√£o s√£o estruturas de comunidades e/ou agrupamentos obtidos via m√©todos de visualiza√ß√£o.\n  Os princ√≠pios da Gestalt s√£o usados para desenvolver m√©todos de processamento de imagens. Imagem retirada de []   Na imagem acima √© mostrado o resultado de um m√©todo baseado na Gestalt para simplificar uma imagem. Em que um algoritmo extrair um padr√£o de linhas de uma imagem. Em redes complexas temos o conceito de backbones que s√£o uma esp√©cie de espinha dorsal, esqueleto, que representa as rela√ß√µes mais importantes entres os v√©rtices (ficar√° mais claro na se√ß√£o sobre backbones . Nesse ponto n√£o necessariamente estamos removendo rela√ß√µes assumindo que elas s√£o um ru√≠do da nossa medida, mas apenas queremos ressaltar esse backbone.\nFiltragem para reduzir o custo computacional Embora a filtragem possa ser usada para remover uma contamina√ß√£o em um dado e/ou facilitar termos insights Conseguimos tamb√©m reduzir o custo computacional de algoritmos que atuam nesses dados. Um exemplo simples √© mostrado no c√≥digo abaixo:\nimport numpy as np import io X, Y = np.meshgrid( np.linspace(-5, 5, 100), np.linspace(-5, 5, 100)) z = np.exp(-0.1*(X**2 + Y**2)) z_noise = z + np.random.normal(0, 0.1, z.shape) z = (z / z.max()*255).astype(np.uint8) z_noise = (z_noise / z_noise.max()*255).astype(np.uint8) data_noisy = io.BytesIO() data = io.BytesIO() np.savez_compressed(data_noisy, z_noise) np.savez_compressed(data, z) print(f\u0026quot;Noisy {data_noisy.getbuffer().nbytes/10**6:.1f} MB\u0026quot;) print(f\u0026quot;Original {data.getbuffer().nbytes/10**6:.1f} MB\u0026quot;)  Noisy 3.6 MB Original 0.2 MB  O output indica que o resultado de contamina√ß√£o por ru√≠do aumenta o custo de armazenamento de um mesmo padr√£o de dados.\nEm grafos, filtrar para reduzir custo computacional costuma ser essencial. Por exemplo, muitos algoritmos escalam com o n√∫mero de arestas. Portanto, um grafo em que cada par de v√©rtices tem uma aresta teria custo computacional $O(n√∫mero\\ \\ de\\ \\ v√©rtices^2)$ o que √© impratic√°vel para apenas algumas dezenas de milhares de v√©rtices. Portanto, tornando a an√°lise de dados imposs√≠vel.\nConfus√µes sobre o que √© filtragem em grafos Antes de entrar mais a fundo na filtragem de grafos √© melhor voc√™ ler com calma a seguinte desambigua√ß√£o para voc√™ n√£o ficar perdido na literatura.\n Desambigua√ß√£o.\nA √°rea de grafos/redes foi/√© √© meio bagun√ßada pois cada campo de estudos (engenharia, computa√ß√£o, matem√°tica, f√≠sica, sociologia, etc) costuma reinventar o mesmo m√©todo com outro nome ou usar nomes iguais para coisas diferentes.\n  Graph coarsening\nEm ci√™ncia da computa√ß√£o: o processo de obter uma representa√ß√£o mais grosseira de um grafo removendo arestas e/ou v√©rtices.\n  Edge filtering:\nEm ci√™ncia da computa√ß√£o: o processo de aplicar um filtro (processamento de sinais) em valores definidos nas arestas. Uma filtragem nos valores associados √†s arestas!\nOutras disciplinas: o processo de remover arestas que n√£o se adequam a um dado padr√£o.\n  Graph sparsification\nTermo usado para representar tanto a remo√ß√£o de v√©rtices quanto arestas (no mesmo sentido de graph coarsening). Por exemplo: ‚Äúspectral edge sparsification‚Äù. Contudo, √© mais utilizado quando voc√™ parte de um grafo vazio (sem rela√ß√µes) e vai adicionando tentando preservar as propriedades espectrais do grafo original.\n  Voc√™ pode encontrar trabalhos com o termo spectral filtering ou spectral coarsening , ambos significando a mesma coisa. Contudo, spectral filters costuma ser usado mais em trabalhos de processamento de sinal em grafos.\n  Quando voc√™ aplica um filtro em uma foto para te deixar mais bonito voc√™ obviamente objetiva que as pessoas ainda te reconhe√ßam. Isto √©, as formas e aspectos mais importantes do seu rosto devem ser preservadas ou pouco alteradas. Vamos representar essas considera√ß√µes por: $$ \\begin{eqnarray} \\mathcal P_{forma}(foto\\ \\ original) \\sim \\mathcal P_{forma}(foto\\ \\ filtrada)\\newline \\mathcal P_{cor}(foto\\ \\ original) \\sim \\mathcal P_{cor}(foto\\ \\ filtrada)\\newline \u0026hellip;etc \\end{eqnarray} $$ Tamb√©m espera-se que o ru√≠do da c√¢mera, rugas e imperfei√ß√µes sejam reduzidas $\\mathcal P_{rugas}(foto\\ \\ original) \\neq \\mathcal P_{rugas}(foto\\ \\ filtrada)$ e $|rugas\\ \\ foto \\ \\ original| \\ll |rugas\\ \\ foto \\ \\ filtrada|$. O s√≠mbolo $|.|$ significa que estamos contando o n√∫mero de rugas da foto, do conjunto de rugas, e $\\ll$ significa muito menor.\nDa mesma maneira que no caso de fotos, se temos um grafo, $G$, queremos que sua vers√£o filtrada, $\\tilde G$, tenha uma ou mais propriedades (definido de antem√£o) preservadas ap√≥s efetuar a filtragem, isto √© $$ \\mathcal P_{algo} (G) \\sim \\mathcal P_{algo} (\\tilde G) $$\nSendo que o objetivo principal costuma ser uma redu√ß√£o dr√°stica no n√∫mero de rela√ß√µes (arestas), $|E| \\le |\\tilde E|$. OK, ent√£o antes de entrar nos m√©todos de filtragem precisamos discorrer sobre quais seriam essas propriedades que queremos preservar.\n Diferente de uma imagem em que filtros s√≥ ocorrem nos valores definidos na posi√ß√£o dos pixels em um grafo, podemos filtrar tanto os valores definidos nos v√©rtices/arestas quanto a pr√≥pria estrutura do grafo em si.\n Novamente: filtrar a estrutura de um grafo $\\neq$ filtrar valores definidos na estrutura de um grafo    Algumas propriedades de grafos Componentes Uma propriedade importante de um grafo √© o n√∫mero de componentes. Um grafo √© fortemente conectado quando √© poss√≠vel sair de qualquer v√©rtice e chegar em qualquer outro. Um grafo fortemente conectado tem apenas uma componente.\nPor exemplo, abaixo √© apresentado um grafo fortemente conectado\ngraph LR; A---B; D---A; B---C A---C; D---E;  Ao remover a aresta $(D , A)$ obtemos o seguinte grafo\ngraph LR; A---B; B---C A---C; D---E;  Como √© imposs√≠vel sair de $D$ ou $E$ e chegar em $A$, $B$ ou $C$ ap√≥s a remo√ß√£o, o grafo n√£o √© mais fortemente conectado e tem duas componentes. Qual a rela√ß√£o disso com filtragem?\n Para muitos problemas, espera-se que m√©todos de filtragem sejam bons em preservar o n√∫mero de componentes. Pois isso afeta em muito as din√¢micas ocorrendo no grafo. Assim como algoritmos de an√°lise de dados. x'\nImagina se ao realizar uma filtragem voc√™ remova uma aresta que impede a contamina√ß√£o por um v√≠rus entre duas cidades no seu modelo?\n  Comunidades Dentro de cada componente de um grafo temos o conceito de comunidade. Intuitivamente, quando pensamos em comunidade no √¢mbito das rela√ß√µes pessoais imaginamos um grupo de pessoas que tem fortes rela√ß√µes entre si, muito mais fortes que as rela√ß√µes com outras pessoas fora do grupo. Por exemplo, fam√≠lia, colegas de trabalho etc. Nesse contexto, qual √© a tarefa de detec√ß√£o de comunidades? Como efetuar tal tarefa?\n Em certos casos queremos que a filtragem n√£o altere a identifica√ß√£o das estruturas de comunidade no nosso grafo.   Suponha que voc√™ queira modelar o grupo de pessoas pertencentes a dois partidos pol√≠ticos, opostos na ideologia. Voc√™ pode representar as rela√ß√µes entre as pessoas usando grafos. Colocando uma aresta entre uma pessoa e outra com o peso representado um grau de concord√¢ncia entre certos assuntos. O que seria um algoritmo de detec√ß√£o de comunidade em tal caso? Se temos o ground truth, isto √©, o partido que cada pessoa se identifica, o algoritmo √© uma fun√ß√£o, $f$, que recebendo as rela√ß√µes , $E$, cospe um ind√≠ce que associa cada pessoa um partido $f: (Pessoa, E) \\mapsto \\{Esquerda,Direita\\}$. Mas como construir essa $f$? Na minha opini√£o existem tr√™s caminhos principais:\n N√£o existe uma √∫nica defini√ß√£o formal para comunidade. Esse conceito muda dependendo da abordagem que voc√™ escolheu para encontrar as comunidades dentro de cada componente.   Caminho 1: Inferir  Pegue por exemplo a distribui√ß√£o normal. Quando trabalhamos com dados que acreditamos que podem ser modelados por tal distribui√ß√£o realizamos um processo de ajuste de par√¢metros, tentando estimar a m√©dia e o desvio padr√£o da popula√ß√£o. A ideia aqui √© similar. Prop√µe-se um modelo capaz de gerar grafos tendo como restri√ß√µes um conjunto de par√¢metros.. O objetivo √© otimizar tais par√¢metros tal que o modelo generativo seja um bom candidato para gerador do grafo original.  O modelo generativo mais famoso √© conhecido como Stocahastic Block Model (SBM). Em portugu√™s, Modelo de Bloco Estoc√°stico. Usando o networkx voc√™ pode gerar uma amostra de um grafo atrav√©s desse modelo usando o seguinte c√≥digo\nimport networkx as nx import matplotlib.pyplot as plt # esses s√£o os par√¢metros que definiram o n√∫mero de indiv√≠duos # dentro de cada comunidade n1, n2, n3 = 30, 40, 60 # esses s√£o os par√¢metros que definem a probabilidade # de conex√£o entre indiv√≠duos da mesma comunidade p11, p22, p33 = 0.4, 0.3, 0.7 # esses s√£o os par√¢metros que definem a probabilidade # de conex√£o entre indiv√≠duos de comunidades distintas p12 = .01 p13 = .1 p23 = .01 sizes = [n1, n2, n3] probs = [[p11, p12, p13], [p12, p22, p23], [p13, p23, p33]] g_sbm = nx.stochastic_block_model(sizes, probs, seed=0) W = nx.adjacency_matrix(g_sbm).todense() plt.imshow(W) plt.show()    A matriz de adjac√™ncia (todos os pesos s√£o 1) do grafo gerado por nosso modelo.   A ideia de infer√™ncia de m√©todos que usam SBM de forma geral √© a seguinte:\n Extraia o conjunto de arestas, $E$, de um grafo qualquer: uma rede social, uma rede de transa√ß√µes financeiras, etc. Pegue um SBM, tente estimar o n√∫mero de parti√ß√µes, probabilidade de conex√µes intra e entre grupos e em qual bloco cada v√©rtice pertence tal que os grafos gerados pelo SBM melhor represente o seu grafo original. No final, voc√™ tem uma maneira de identificar com cada v√©rtice uma comunidade (parti√ß√£o).  O SBM √© poderoso e ao contr√°rio dos outros m√©todos te fornece uma maneira de checar a qualidade das comunidades encontradas. Isto √©, se fazem sentido ou s√≥ s√£o frutos de algo aleat√≥rio. Contudo, por ser uma t√©cnica mais recente com uma implementa√ß√£o dif√≠cil, n√£o s√£o todas as bibliotecas que fornecem esse recurso. A biblioteca mais famosa para SBM √© o Graph Tool que consegue estimar comunidades para grafos com centenas de milhares de v√©rtices. N√£o poderei discorrer mais ou mostrar como usar o SBM pois √© um tema bem complexo, tema para um post separado. Mas o importante agora √© voc√™ ter conseguido absorver pelo menos a ideia.\nCaminho 2: Quantificar/Descrever  Voc√™ parte de uma fun√ß√£o $f$ qualquer. Exemplo, $f$ √© uma fun√ß√£o que identifica todo mundo como esquerda ou direita, um sorteio aleat√≥rio, etc. Com tal identifica√ß√£o voc√™ estipula uma grandeza que vai mensurar o qu√£o forte √© a coes√£o entre as pessoas de cada grupo e qu√£o fraca √© entre os grupos. Um exemplo de grandeza que mensura isso √© a modularidade. Voc√™ ir√° alterar a sua $f$ tentando maximizar tal grandeza.  O networkx por exemplo possui um m√©todo de maximiza√ß√£o de modularidade usando um algoritmo guloso. Vamos usar o grafo gerado pelo sbm para testar esse m√©todo usando o seguinte script:\nfrom networkx.algorithms import community def find_where(n, p): return [i for i in range(len(p)) if n in p[i]][0] def plot(g, community_index, p): labels = [chr(ord('A') + i) for i in range(len(p))] plt.scatter(range(len(g.nodes)), community_index) plt.ylabel('Community') plt.xlabel('Vertex Id') plt.yticks(range(len(p)), labels) plt.show() p = community.greedy_modularity_communities(g_sbm) g_sbm_community_index = [find_where(n, p) for n in g_sbm.nodes] print(f\u0026quot;Found {len(set(g_sbm_community_index))} communities\u0026quot;) plot(g_sbm, g_sbm_community_index, p)    Resultado da identifica√ß√£o de comunidades usando o algoritmo guloso. Parece Ok   Temos um resultado muito bom. Mas ser√° que podemos empregar isso em qualquer caso? Vejamos o que acontece quando aplicamos o mesmo algoritmo para um grafo aleat√≥rio.\n# erdos_reyni √© um modelo de grafo aleat√≥rio g = nx.erdos_renyi_graph(150, 0.1, seed=0) p = community.greedy_modularity_communities(g) g_community_index = [find_where(n, p) for n in g.nodes] plot(g, g_community_index, p)    Resultado da identifica√ß√£o de comunidades usando o algoritmo guloso para o modelo ER.   O algoritmo guloso encontrou 4 comunidades e o ponto ruim √© que n√£o temos como saber o qu√£o confi√°vel √© essa resposta. Mas podemos dizer que provavelmente ela n√£o deveria ser usada pois partimos de um modelo de grafo aleat√≥rio.\nDevemos tomar muito cuidado com m√©todos de detec√ß√£o por maximiza√ß√£o de modularidade e similares. Recomendo ver alguns trabalhos sobre modelos de bloco estoc√°stico, especialmente os feitos pelo Tiago Peixoto.\nNew blog post! This time, on something tame and uncontroversial:\n\u0026quot;Modularity maximization considered harmful\u0026quot;\nIt\u0026#39;s the most popular method used for community detection. It is also one of the most problematic. 1/11\n(Based on https://t.co/iCxFjKOIT1)https://t.co/IRdCFwttQL\n\u0026mdash; Tiago Peixoto (@tiagopeixoto) December 6, 2021   M√©todos de detec√ß√£o de comunidade usando modularidade (Gelphi) s√£o √∫teis. Contudo, podemos identificar comunidades mesmo no caso de um grafo totalmente aleat√≥rio! Tome cuidado.   Caminho 3: Visualizar  Voc√™ utiliza um m√©todo que mapeia cada v√©rtice do seu grafo em um espa√ßo vetorial. Por exemplo t-sne, UMAP, force-directed, spectral embedding etc. Com sua visualiza√ß√£o voc√™ realiza uma inspe√ß√£o (totalmente subjetiva!) para identificar as comunidades (agrupamentos). Em alguns casos √© aceit√°vel realizar um k-means nesse espa√ßo para encontrar os clusters.  O script abaixo gera uma visualiza√ß√£o dos dois grafos usados nos exemplos anteriores: um obtido do SBM e outro do Erdos-Renyi.\nimport numpy as np pos_sbm = np.array([ v for v in nx.layout.spring_layout(g_sbm, iterations=1000).values()]) pos = np.array([ v for v in nx.layout.spring_layout(g, iterations=1000).values()]) fig, (a1, a2) = plt.subplots(1, 2) a1.scatter(pos_sbm[:, 0], pos_sbm[:, 1], c=g_sbm_community_index, cmap='tab20') a2.scatter(pos[:, 0], pos[:, 1], c=g_community_index, cmap='tab20') for ax in (a1, a2): ax.set_yticklabels([]) ax.set_xticklabels([]) a1.set_title('SBM') a2.set_title('ER') plt.show()    Visualiza√ß√£o via force-directed para uma amostra de um SBM e outra Erdos-Renyi. Cores representam as comunidades identificadas pelo m√©todo guloso de maximiza√ß√£o de modularidade   Note que o m√©todo de visualiza√ß√£o mostrou um agrupamento de v√©rtices para o SBM. Contudo, no caso do grafo aleat√≥rio (ER) s√≥ parece uma grande confus√£o. As cores representam as comunidades obtidas via maximiza√ß√£o da modularidade. O que podemos tirar desse exemplo? Que voc√™ deve tomar cuidado quando falar que encontrou uma comunidade ou que existe uma ‚Äúbolha‚Äù na rede social que voc√™ encontrou. Outra coisa que isso nos mostra √© que usar m√©todos diferentes √© uma boa alternativa para evitar ser enganado por seus resultados.\n No caso de visualiza√ß√µes de grafos, especialmente de force-directed, talvez seja melhor voc√™ utilizar algum sistema de visualiza√ß√£o iterativo e 3D. Visualiza√ß√µes em 2D obtidas pelo force-directed podem n√£o ser de grande ajuda e ainda ficarem presas em alguma configura√ß√£o n√£o √≥tima.    Tome cuidado ao interpretar um grafo usando apenas m√©todos de visualiza√ß√£o como force-directed, force-atlas, etc. Lembre que temos a tend√™ncia a reconhecer padr√µes baseado em agrupamentos, contraste etc. A Gestalt tamb√©m atua para nos enganar. Voc√™ pode estar sujeito a pareidolia.    O tema de comunidades merece alguns posts separados para cada caminho, pois √© um assunto denso e com muitos m√©todos diferentes.\nFiltros Estrutural: threshold O m√©todo de threshold √© um m√©todo estrutural, isto √©, um m√©todo de filtragem que depende apenas dos pesos e das arestas. Com certeza, √© o m√©todo mais simples e mais r√°pido, embora o mais controverso. √â aplic√°vel somente se cada rela√ß√£o (aresta) possuir um n√∫mero real associado. O m√©todo de threshold consiste em descartar qualquer aresta cuja o peso ultrapasse um dado valor.\nO m√©todo de threshold √© muito utilizado em neuroci√™ncia (com cr√≠ticas) e para an√°lise de dados em geral quando as arestas representam uma medida de correla√ß√£o (Pearson) entre dois elementos. Como as medidas de correla√ß√µes podem ser negativas √© comum que o threshold seja aplicado no absoluto dos valores associados √†s arestas.\nTome o seguinte grafo como exemplo:\ngraph LR; A--\u0026gt;|-0.5|B; B--\u0026gt;|0.4|C C--\u0026gt;|2|A; D--\u0026gt;|-1|C;  Ao realizar um threshold de $0.5$ iremos remover a rela√ß√£o $(B, C)$ e $(A, B)$. O grafo n√£o √© mais fortemente conectado.\ngraph LR; C--\u0026gt;|2|A; D--\u0026gt;|-1|C; B;  √â comum que ap√≥s o threshold todas as arestas que sobraram sejam truncadas em $1$. Ficar√≠amos com algo assim no final:\ngraph LR; C--\u0026gt;|1|A; D--\u0026gt;|1|C; B;  Uma das maiores limita√ß√µes/perigo de se usar o m√©todo um naive threshold √© que em grafos que modelam situa√ß√µes do mundo real (seja ele direto ou n√£o) a distribui√ß√£o de pesos costuma seguir uma fat-tail e distorcida tal como essa aqui:\n  Distribui√ß√£o de probabilidade dos pesos das arestas em fun√ß√£o do peso. Note que poucas arestas tem um peso relevante. Fonte: Extracting the multiscale backbone of complex weighted networks   Bom, o que acontece se voc√™ tentar passar um threshold no grafo que tem uma distribui√ß√£o parecida com essa na imagem? Vai ser dif√≠cil. Qualquer valor um pouco maior criar√° um monte de componentes desconectados. Al√©m do que, como voc√™ justificaria seu valor de threshold ? N√£o d√° para falar um argumento dois desvios padr√µes a partir da m√©dia. Se fosse uma distribui√ß√£o normal de pesos voc√™ poderia estar bem.\nO threshold tem outro problema, ele √© local. Isto √©, voc√™ poderia penalizar muito as arestas de uma comunidade e nada de outra. Para deixar isso mais claro veja o exemplo de grafo com pesos a seguir:\ngraph LR; *---|0.4|1; 1---|0.8|2; 3---|0.4|2; 1---|0.6|3; 1---|0.6|4; 4---|0.3|3; 4---|...|...; 1---|...|...; *---|0.4|a; a---|1|b; a---|0.8|c; a---|0.8|d; c---|0.7|e; b---|0.7|f; d---|0.8|g; g---|...|?_1; f---|...|?_2; e---|...|?_3; b---|0.3|c; c---|0.3|d;  Se aplic√°ssemos um threshold em $0.5$ ter√≠amos algo do tipo\ngraph LR; *; 1---2; 1---3; 1---4; 4---|...|...; 1---|...|...; a---b; a---c; a---d; c---e; b---f; d---g; g---|...|?_1; f---|...|?_2; e---|...|?_3;  Produzindo 3 componentes no nosso grafo se alter√°ssemos ligeiramente o threshold produziremos mais componentes ainda. Ele √© muito sens√≠vel. Qual o problema disso? Se fossemos aplicar um algoritmo de detec√ß√£o de comunidades ter√≠amos que fazer isso para cada componente. Em uma rede social isso pode ser problem√°tico porque j√° estaremos analisando ‚Äúbolhas‚Äù isoladas. Ent√£o como proceder? Portanto, voc·∫Ω pode at√© usar o threshold para encontrar as arestas que s√£o a sustenta√ß√£o para o grafo. A espinha dorsal do grafo, backbone. Contudo, ele costuma falhar.\nPontos positivos  custo computacional baixo $O(n)$  apenas iterar e comparar os valores.   paraleliz√°vel trivial de implementar apenas um par√¢metro  Pontos negativos  tend√™ncia de produzir muitas componentes desconectadas, par√¢metro arbitr√°rio,  cherry-picking.   A remo√ß√£o de uma aresta s√≥ depende do valor atribu√≠do a ela. Isto √©, local.  Considera√ß√µes finais Outros m√©todos estruturais como o high-salience network tentam reduzir os problemas do threshold adicionando contribui√ß√µes n√£o locais. Isto √©, uma aresta √© mantida/removida dependendo tamb√©m das outras arestas no grafo. Contudo, como o high-salience network √â um filtro definido pelos menores caminhos no grafo ele costuma ser adequado apenas para grafos que esse conceito de filtragem √© √∫til, por exemplo grafos que modelam infraestrutura de transporte.\nEstat√≠stico: quebrando a varinha, processo de Dirichlet M√©todos estat√≠sticos t√™m uma abordagem mais generalista quando comparados aos estruturais. Pois m√©todos estat√≠sticos n√£o dependem de algum conceito direto como caminhos m√≠nimos usados pelo high-salience network para redes de infraestrutura.\nUm m√©todo estat√≠stico muito usado para filtrar arestas faz uso do processo estoc√°stico de Dirichlet. Intuitivamente, podemos usar esse processo para modelar uma situa√ß√£o que temos uma varinha e vamos quebrando ela em $k$ peda√ßos e queremos descobrir a probabilidade de um peda√ßo de tamanho $p$ aparecer no processo,  stick-breaking process.\n O processo de Dirichlet foi redescoberto em 2009 com o nome de filtro de disparidade. Embora os autores do filtro de disparidade n√£o citem trabalhos pr√©vios ou o pr√≥prio processo Dirichlet em si.   Certo, vamos tentar entender como usar esse processo para filtrar arestas.\nCome√ßamos definindo os pesos efetivos para cada v√©rtice e aresta. Esse peso efetivo para uma aresta entre os v√©rtices A e B √© dado pela seguinte express√£o: $$ p_{AB} = \\frac{Peso\\ da\\ aresta\\ (A,B)}{Soma\\ dos\\ pesos\\ de\\ todas\\ as\\ arestas\\ de\\ A} $$ $$ p_{AB}= \\frac{w_{AB}}{\\sum\\limits_C w_{AC}} $$\nPegue o grafo a seguir com os pesos dados nas arestas\ngraph LR; A---|1|B; B---|1|C; A---|2|C; A---|4|D; D---|1|C;  Calculando o peso efetivo para todas as arestas relacionadas ao v√©rtice A. √â f√°cil ver que\n$p_{AB} =1/7$, $p_{AC}=2/7$, e $p_{AD}=4/7$ e claro que $\\sum_B p_{AB}=1$.\ngraph LR; A---|1/7|B; B---C; A---|2/7|C; A---|4/7|D; D---C;  Iremos decidir se removeremos alguma ou mais arestas de A.\nNossos pesos efetivos somam 1. A ideia do filtro √© imaginar que os pesos efetivos s√£o influ√™ncias do v√©rtice A nos seus vizinhos. O modelo parte da hip√≥tese que os pesos efetivos s√£o distribu√≠dos de forma uniforme entres os vizinhos de A. Portanto, podemos modelar a distribui√ß√£o de pesos nas tr√™s arestas de A como um stick-breaking process. Desta maneira, podemos escolher remover as arestas cujo os pesos efetivos tenham uma probabilidade maior de ter vindo desse processo. Estamos mantendo os pesos efetivos dispares do processo!\nOk, como fazer isso? Como os pesos efetivos podem ter qualquer valor entre 0 e 1 precisamos de uma densidade de probabilidade. O stick-breaking deve modelar um processo de quebra de um graveto em $k$ pedacinhos. No nosso caso, os $k$ pedacinhos s√£o as $3$ arestas de A. Ent√£o a densidade de probabilidade precisa ter $k$ como par√¢metro.\n  Demonstrar a densidade de probabilidade desse processo de quebra √© trabalhoso, mas a express√£o final √© bem simples. S√£o fun√ß√µes decrescentes que caem mais r√°pido quanto maior o $k$. O que faz sentido, j√° que quanto mais pedacinhos quebrarmos menos prov√°vel √© achar um pedacinho com um tamanho pr√≥ximo do original do graveto.\nA filtragem via stick-breaking (disparidade) baseia-se ent√£o em remover somente as arestas cujo os pesos efetivos s√£o mais prov√°veis (um p-teste) dado um fator $\\alpha$ , um n√∫mero real entre 0 e 1. Isto √©, a aresta AB √© mantida se a inequa√ß√£o abaixo √© verificada: $$ (1-p_{AB})^{k_A-1} \u0026lt; \\alpha $$\nA tabela abaixo mostra o que acontece com as arestas de $A$ a medida que o par√¢metro $\\alpha$ √© alterado\n   Aresta/$\\alpha$ 0. 19 0.52 0.74     A,B Removida Removida Mantida   A,C Removida Mantida Mantida   A,D Mantida Mantida Mantida    OK, parece muito bom. Mas veja o seguinte: ressaltei v√°rias vezes A no texto. Isto por que o filtro √© definido por v√©rtice. Bom, e o que acontece se olharmos a partir do v√©rtice B?\nPartindo de $B$ teremos $p_{BC}=1/2$ e $p_{BA}=1/2$!\ngraph LR; A---|1/2|B; B---|1/2|C; A---C; A---D; D---C;  Ent√£o $(1-p_{BA})^{k_b-1} = (1-1/2)^1 = 1/2$. Ok , ent√£o se escolhermos $\\alpha$ igual 0.52 a tabela anterior (para **A**) diz para remover a aresta (A,B) enquanto por **B** o m√©todo nos diz que √© para manter. Isso causa uma ambiguidade em como decidir se vamos manter ou n√£o as arestas. Voc√™ pode escolher manter se os dois concordam ou manter se apenas um passar no teste. **Essa ambiguidade n√£o aparece no caso de grafos direcionados!**\nPontos positivos  √© estabelecido dentro de uma formaliza√ß√£o matem√°tica robusta tenta evitar que o grafo se desconecte custo computacional baixo  Pontos negativos   podemos argumentar que o teste de hip√≥tese √© arbitr√°rio\n  par√¢metro $\\alpha$ precisa ser escolhido, embora mais robusto do que apenas o par√¢metro de threshold\n  ","date":1644883200,"expirydate":-62135596800,"kind":"page","lang":"pt-br","lastmod":1644883200,"objectID":"bb38cd9da136a83e6ca223a77e4d7907","permalink":"/pt-br/post/edge_graph_filtering/","publishdate":"2022-02-15T00:00:00Z","relpermalink":"/pt-br/post/edge_graph_filtering/","section":"post","summary":"Assim como filtramos imagens para melhorar a visualiza√ß√£o ou ressaltar caracter√≠sticas tamb√©m podem filtrar rela√ß√µes (arestas) em um grafo com o mesmo objetivo. Veremos aqui alguns tipos de filtragens.","tags":["graphs","statistics","ml","data analysis","python","Complex Networks","matrix"],"title":"Grafos e filtragem de arestas: conceitos e confus√µes. Parte I","type":"post"},{"authors":null,"categories":["Linux","DevOps","MlOps","Databases"],"content":"Lista de Conte√∫dos  Conceitos  ‚ÄúEverything is a file.‚Äù mantra UNIX  LSOF   System Calls e strace   Investigando problemas  Identificando problemas de conex√£o  O meu servi√ßo est√° on? O pip ou um cliente qualquer est√° engasgado esperando uma resposta de algu√©m?   Problemas com arquivos  Quais processos est√£o usando esse arquivo? Deletei o csv e agora? Erros silenciosos: arquivo n√£o existente ou permiss√£o Esse processo est√° salvando algo que n√£o deveria? Onde?   Extras envolvendo arquivos (/proc/) e strace  Gerando um sum√°rio de SYS CALL O processo foi iniciado com as vari√°veis de ambiente corretas? Esqueci de redirecionar os outputs do processo para um arquivo. O que fazer? Qual comando gerou o processo e onde √© o seu working dir?   Agradecimentos \u0026amp; Sugest√µes      Linux n√£o √© um SO opaco. Ele n√£o ser opaco significa que √© f√°cil ver o que acontece por tr√°s dos processos. O que permite identificar um problema ou pelo menos saber se voc√™ realmente tem um problema.\nIniciei meu aprendizado em MlOps (Machine Learning Operations). Embora tenha pouca experi√™ncia foi f√°cil aceitar que MlOps envolve um workflow extremamente intricado com muitos poss√≠veis pontos de falhas. Tais pontos podem n√£o estar relacionados com os operadores. Portanto, saber identificar se existe uma falha e o que est√° causando ela √© de suma import√¢ncia. Isso vai desde compreender o comportamento de um processo criado pelos pr√≥prios operadores ou o que o pip/conda e demais depend√™ncias externas est√£o aprontando debaixo dos panos.\nO primeiro passo para entender um problema com um processo √© analisar o output (a sa√≠da na sess√£o do seu terminal). Contudo, algumas vezes isso n√£o te fornece a informa√ß√£o suficiente. Neste texto vou discorrer do b√°sico sobre como debugar processos usando o strace e lsof. Iremos criar alguns exemplos patol√≥gicos usando python para simular problemas que podemos encontrar e como eles s√£o dissecados pelo strace e o lsof.\nConceitos ‚ÄúEverything is a file.‚Äù mantra UNIX Quando voc√™ pensa em arquivo voc√™ talvez relacione com um CSV, uma planilha ou imagem. Mas na abordagem UNIX de fazer SO o conceito de arquivo aparece em todos os lugares. Por exemplo, at√© conex√µes de rede s√£o associadas a um arquivo. Em casos que um elemento em si n√£o √© um arquivo tal elemento tem a ele associado um descritor de arquivo (file descriptor). Como isso se relaciona com debugar processos? **Se tudo √© um arquivo analisar um processo pode ser feito com o mesmo conjunto de ferramentas e conceitos que usamos para listar, compreender e comunicar com arquivo inclusive com a mesma API. ** Aqui abordaremos uma ferramenta para listagem de arquivos, o lsof.\nLSOF A ferramenta lsof √© um comando que pode ser usado para listar os file descriptors abertos e os processos que foram respons√°veis por tal a√ß√£o. Desta maneira voc√™ pode listar os file descriptors de um usu√°rio que est√£o associados a uma porta via conex√£o ou processo. O nome desse comando √© um acr√¥nimo para list open files.\nO exemplo mais simples de uso jogando os resultados para um arquivo √© esse\nmeuusuario:/$ lsof \u0026gt; lsof_tudo.txt  O comando acima ir√° criar uma tabela (imensa) dentro de lsof_tudo.txt\nEssa tabela ser√° mais ou menos assim\nCOMMAND PID TID TASKCMD USER FD TYPE DEVICE SIZE/OFF NODE NAME systemd 1 root cwd unknown /proc/1/cwd (readlink: Permission denied) systemd 1 root rtd unknown /proc/1/root (readlink: Permission denied) systemd 1 root txt unknown /proc/1/exe (readlink: Permission denied)  Se voc√™ olhar com cuidado ver√° que aparecem linhas de diferentes usu√°rios. As primeiras s√£o do root e uma das colunas mostra que voc√™ n√£o tem permiss√£o para ler os file descriptors desse usu√°rio, ainda bem! Para pedir apenas a listagem do seu usu√°rio fa√ßa\nmeuusuario:/$ lsof -u meuusuario \u0026gt; lsof_meu.txt  O arquivo ainda √© enorme, mas abra ele com seu editor de texto. Tente procurar nomes de arquivos e processos que voc√™ esta usando agora.\nTemos muitas colunas no output, voc√™ pode ver o significado detalhado de cada uma digitando man lsof . Mas eu acho mais interessante voc√™ focar nas seguintes colunas:\n COMAND  O nome do comando associado ao processo que abriu o arquivo   PID  Um n√∫mero que identifica unicamente o processo. Voc√™ pode usar esse n√∫mero para matar o processo usando pkill, usar ele no strace etc.   TID  Se o arquivo foi aberto por uma thread de um processo. Quando n√£o tem nada nessa coluna significa que a a√ß√£o foi feita por um processo.   USER  O usu√°rio respons√°vel pelo processo que efetuou a a√ß√£o.   TYPE  Essa coluna √© bem √∫til. Tal coluna te diz o tipo de n√≥ associado ao arquivo. Por exemplo, se o arquivo for associado com protocolos voc√™ vera aqui coisas do tipo: IPV4, IPV6. Se for um arquivo normal haver√° na coluna o identificador **REG. **Existem algumas dezenas de possibilidades de valores para essa coluna, eu nunca lembro o que elas significam, mas √© f√°cil consultar online ou no man.   NODE  O identificador do n√≥ do arquivo. No caso desse arquivo envolver protocolos de internet haver√° coisas como TCP, UDP   NAME  Tamb√©m bastante √∫til. Ele muda bastante dependendo do que o arquivo se refere. Pode ser o endere√ßo do servidor (www.google.com, localhost:5000) assim como o endere√ßo do arquivo.    O lsof tem muitos argumentos poss√≠veis, veremos alguns utilizando alguns casos que eu acho interessante e que acontecem.\nSystem Calls e strace O system call √© o mecanismo de comunica√ß√£o entre processos e o kernel do seu SO. Tal mecanismo permite que um processo requisite recursos do kernel disponibilizados pelo seu hardware. Para ler um arquivo armazenado em seu hardwre √© necess√°rio que ocorra antes um system call. Portanto, tendo uma maneira de interceptar essas chamadas entre um processo e o kernel temos como compreender o que tal processo est√° fazendo. Um comando que permite essa intercepta√ß√£o √© o strace.\n$ man strace  Se o strace n√£o estiver dispon√≠vel instale\n$ apt install strace  O strace pode ser executado de duas formas. A primeira √© usando o comando a ser interceptado como argumento do strace\n$ strace ARGS COMANDO_A_SER_INTERCEPTADO  a segunda, bastante √∫til, √© interceptando um processo j√° iniciado usando o PID de tal processo,\n$ strace ARGS -p PID_DO_PROCESSO  Para descobrir o PID de um processo use o htop ou o seguinte comando ps aux | grep -i '[n]ome_do_processo'.\nVeja um exemplo simples do strace e seu output\n$ strace -t ls  O resultado ser√° algo do tipo\n18:02:23 execve(\u0026quot;/usr/bin/ls\u0026quot;, [\u0026quot;ls\u0026quot;], 0x7fffa727a418 /* 54 vars */) = 0 18:02:23 brk(NULL) = 0x55ebef60c000 18:02:23 access(\u0026quot;/etc/ld.so.preload\u0026quot;, R_OK) = -1 ENOENT (No such file or directory) 18:02:23 openat(AT_FDCWD, \u0026quot;/etc/ld.so.cache\u0026quot;, O_RDONLY|O_CLOEXEC) = 3 ...  Cada linha representa uma system call e o seu respectivo resultado. O argumento -t diz para imprimir na primeira coluna o instante de tempo que o system call foi chamado.\nDe forma resumida o formato das linhas segue o padr√£o:\nNome da SYS CALL(Argumentos usados na SYS CALL) = O resultado\nO output √© dif√≠cil se n√£o humanamente imposs√≠vel de compreender tudo sem um guia externo. Um guia poss√≠vel √© o comando man. O comando abaixo mostra a documenta√ß√£o do sys call openat\n$ man 2 openat  O openat √© o sys call que requisita a abertura de um arquivo, o resultado na √∫ltima linha ( O_RDONLY|O_CLOEXEC) = 3) significa que a chamada do sistema foi bem sucedida. Caso fosse -1 alguma coisa teria dado errado quando o processo requisitou o recurso.\nInvestigando problemas Veremos aqui problemas e falhas relacionados a arquivos regulares e conex√µes de rede. Contudo podemos usar as mesmas tecnicas para outros tipos de problemas.\nIdentificando problemas de conex√£o O conda est√° travado? O pip t√° baixando os pacotes do servidor ou existe algum servidor engasgando? Para onde minhas requisi√ß√µes est√£o indo? Antes de tentar iniciar um modo verboso e ter que matar seu processo voc√™ pode usar o lsof para responder essas perguntas. Para come√ßar nosso tutorial e realizar as simula√ß√µes instale o flask e requests\n$ python -m pip install requests flask  Crie o arquivo server_mlops.py\n# server_mlops.py import time import flask app = flask.Flask(__name__) @app.route('/') def hello_world(): sleep_time = flask.request.args.get('sleep', default=10, type=int) print('sleep_time:', sleep_time) time.sleep(sleep_time) return 'Hello World!' if __name__ == '__main__': app.run()  Inicie duas sess√µes no terminal. Na primeira inicie o servidor\n$ python server_mlops.py  Na segunda execute\n$ ps aux | grep -i '[s]erver_mlops.py'  voc√™ vera um output do tipo\ndevmess+ 19321 18.0 0.3 29716 24792 pts/5 S+ 14:27 0:00 python server_mlops.py  O n√∫mero na frente do seu username (19321) √© o PID do processo.\nO meu servi√ßo est√° on? O argumento -a pede que o lsof use todos os argumentos de filtragem com o operador AND isto √©, todas as condi√ß√µes devem ser v√°lidas. O argumento -i pede para que ele filtre apenas arquivos associados a conex√µes e o argumento -p 19321 pede que use apena o processo com o PID 19321.\n$ lsof -a -i -p 19321  Voc√™ vera um output mais ou menos assim\n   COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME     python 19321 devmessias 4u IPv4 16108218 0t0 TCP localhost:5000 (LISTEN)    Est√° tudo ok com o seu servi√ßo. Tente remover um dos argumentos (remova o -a por exemplo) ou usar eles isolados, veja como o output muda.\nO pip ou um cliente qualquer est√° engasgado esperando uma resposta de algu√©m? Esse tipo de problema pode acontecer quando estamos gerenciando uma depend√™ncia, requisitando algum tipo de dado de um servidor e em in√∫meros outros casos em que n√£o temos acesso a m√°quina que executa o servi√ßo. Portanto, precisamos analisar do nosso lado se o processo est√° travado por alguma falha nossa.\nCrie o arquivo client_mlops.py\n#!/usr/bin/env python #client_mlops.py import requests import argparse parser = argparse.ArgumentParser() parser.add_argument( '--sleep', type=int, help='time to sleep', default=0) args = parser.parse_args() print('Ask for localhost:5000 to sleep for {} seconds'.format(args.sleep)) r = requests.get('http://localhost:5000', params={'sleep': int(args.sleep)}) print(r.text)  No c√≥digo acima temos o argumento sleep que pedira para o server_mlops.py esperar alguns segundos antes de enviar a resposta.\nSimularemos um problema de um servidor pregui√ßoso. Pedindo que ele durma por 20 segundos. Se voc√™ matou o processo do servidor inicie ele novamente.\nExecute o client_mlops.py com o strace\n$ strace -e poll,select,connect,recvfrom,sendto python client_mlops.py --sleep=20  aqui estamos pedindo para que o strace nos mostre apenas chamadas do tipo poll,select,connect,recvfrom e sendto.\nO output ser√° algo do tipo\nconnect(4, {sa_family=AF_INET, sin_port=htons(5000), sin_addr=inet_addr(\u0026quot;127.0.0.1\u0026quot;)}, 16) = 0 connect(4, {sa_family=AF_INET6, sin6_port=htons(5000), inet_pton(AF_INET6, \u0026quot;::1\u0026quot;, \u0026amp;sin6_addr), sin6_flowinfo=htonl(0), sin6_scope_id=0}, 28) = 0 connect(4, {sa_family=AF_INET6, sin6_port=htons(5000), inet_pton(AF_INET6, \u0026quot;::1\u0026quot;, \u0026amp;sin6_addr), sin6_flowinfo=htonl(0), sin6_scope_id=0}, 28) = -1 ECONNREFUSED (Connection refused) connect(4, {sa_family=AF_INET, sin_port=htons(5000), sin_addr=inet_addr(\u0026quot;127.0.0.1\u0026quot;)}, 16) = 0 sendto(4, \u0026quot;GET /?sleep=10 HTTP/1.1\\r\\nHost: l\u0026quot;..., 154, 0, NULL, 0) = 154 recvfrom(4,  Note que temos uma SYS_CALL engasgada, recvfrom (se voc√™ quiser obter mais informa√ß√µes sobre uma SYS_CALL digite man 2 recvfrom) . Quem t√° engasagando √© o servidor e n√£o o cliente.\nVoc√™ pode tamb√©m usar o lsof para checar se voc√™ est√° com esse tipo de problema. Para isso, execute o cliente em uma sess√£o separada\n$ python client_mlops.py --sleep=100  pegue o PID com ps aux | grep -i '[c]lient_mlops.py' e execute o lsof\nlsof -a -i -p 19321  O resultado ser√° algo do tipo\n   COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME     python 31551 devmessias 4u IPv4 16622065 0t0 TCP localhost:57314-\u0026gt;localhost:5000 (ESTABLISHED)    Note que uma conex√£o foi estabelecida (coluna NAME). Se o servi√ßo estivesse enviado a resposta n√£o ter√≠amos obtido nada na sa√≠da do lsof.\nProblemas com arquivos Vamos simular alguns problemas com arquivos regulares: csv, txt, bin, jpg etc. Copie um csv para pasta /tmp/, ou execute o comando abaixo para criar um txt dummy contendo o manual do comando strace.\n$ man strace \u0026gt; /tmp/arquivo.csv  Quais processos est√£o usando esse arquivo? O objetivo aqui √© saber quais processos est√£o acessando um arquivo. Isto √© √∫til quando queremos identificar processos que j√° deveriam ter \u0026ldquo;fechado\u0026rdquo; o arquivo ou inentificar acessos indenvidos. Tamb√©m pode ser √∫til para descobrir qual processo est√° criando um arquivo gigantesco no seu sistema para que voc√™ possa dar um kill.\nCrie o script a seguir em uma pasta.\n#!/usr/bin/env python # file_open.py import time f = open('/tmp/arquivo.csv', 'r') input('Press Enter to continue...')  Depois abra duas sess√µes no terminal e rode o comando python file_open.py. Agora basta listar os processos que est√£o com arquivo.csv abertos\n$ lsof /tmp/arquivo.csv  O output ser√° algo do tipo\nCOMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAME python 15411 devmessias 3r REG 8,2 0 2911031 /tmp/arquivo.csv python 20777 devmessias 3r REG 8,2 0 2911031 /tmp/arquivo.csv  Temos dois processos distintos (dois PID) utilizando nosso arquivo.\nDeletei o csv e agora? Suponha uma situa√ß√£o em que acidentalmente um arquivo foi apagado. Contudo, existe um processo que ainda est√° fazendo uso de tal recurso.\nCrie um arquivo qualquer, aqui vou chamar de acidente.txt\nAbra uma sess√£o no terminal e execute o comando a seguir. N√£o feche a sess√£o!\n$ python -c 'f=open(\u0026quot;acidente.txt\u0026quot;, \u0026quot;r\u0026quot;);input(\u0026quot;...\u0026quot;)'  Simularemos o acidente que outro processo remove o arquivo. Execute os comandos abaixo\n$ rm acidente.txt $ ls acidente.txt  Nosso arquivo foi embora :(\nls: cannot access 'acidente.txt': No such file or directory  Mas n√£o se preocupe! Uma coisa legal do linux: todos os processos do sistema tem a eles associados um diret√≥rio dentro da pasta /proc (everthing is a file). E o que tem nesses diret√≥rios ? Muitas coisas, incluindo o file descriptor do acidente.txt. Utilizado pelo nosso processo python. Para encontrar esse file descriptor usaremos o lsof\n$ lsof -u nomedeusuario | grep 'acidente.txt'  No meu caso obtive o seguinte output\npython 22465 devmessias 3r REG 8,2 37599 14288174 caminho/acidente.txt (deleted)  Ent√£o o PID √© 22465 e o n√∫mero que descreve o arquivo (file descriptor) √© 3 (o que vem antes do r no output acima). Para obter uma c√≥pia do acidente.txt deletado basta chamar um simples cp\n$ cp /proc/22465/fd/3 recuperado.txt  Abra o arquivo recuperado.txt e veja que tudo est√° no seu devido lugar. N√£o √© m√°gica, procure por process pseudo-filesystem na web ou digite man proc .\nErros silenciosos: arquivo n√£o existente ou permiss√£o Em alguns casos voc√™ pode ter um processo criado por uma depend√™ncia externa que tenta acessar um arquivo com permiss√£o errada ou mesmo n√£o existente. Criaremos essas duas situa√ß√µes com o script file_404.py.\n#!/usr/bin/env python # file_404.py import time try: f = open('/tmp/arquivo_404.csv', 'r') except FileNotFoundError: pass try: # um arquivo que vc nao tem permissao, crie como sudo e mude com chmod 700 f = open('/tmp/arquivo_permission.csv', 'r') except PermissionError: pass input('Press Enter to continue...')  Execute ele com python file_404.py veja que nenhum problema √© informado.\nPara traquear as chamadas do sistema do tipo arquivo feitas por python file_404.py  basta digitar o comando abaixo no terminal\n$ strace -f -e trace=file python file_404.py  o argumento -f diz para o strace monitorar tamb√©m qualquer processo filho criado. Em python, isso seria por exemplo os processos criados por os.fork.\nA sa√≠da do exemplo ser√° algo do tipo\nlstat(\u0026quot;SEU DIRETORIO/file_404.py\u0026quot;, {st_mode=S_IFREG|0644, st_size=242, ...}) = 0 openat(AT_FDCWD, \u0026quot;file_404.py\u0026quot;, O_RDONLY) = 3 openat(AT_FDCWD, \u0026quot;/tmp/arquivo_404.csv\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/tmp/arquivo_permission.csv\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 EACCES (Permission denied)  Note que temos no output informa√ß√µes que n√£o queremos investigar, mas nas √∫ltimas linhas os erros de permiss√£o e aus√™ncia de arquivo apareceram.\nUma maneira de filtrar o resultado e tornar sua vida mais f√°cil √© usar o awk redirecionado a sa√≠da do strace com o pipe |.\n$ strace -f -e trace=file python file_404.py 2\u0026gt;\u0026amp;1 | awk '/^open/ \u0026amp;\u0026amp; /= -1/ {print}'  O comando acima diz para mostrar apenas as linhas que come√ßam com a string open e em alguma parte da linha tenha o padr√£o = -1.\nO comando com awk concatenado produzir√° um output mais limpo, veja s√≥\nopenat(AT_FDCWD, \u0026quot;/home/devmessias/anaconda3/pyvenv.cfg\u0026quot;, O_RDONLY) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/tmp/arquivo_404.csv\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory) openat(AT_FDCWD, \u0026quot;/tmp/arquivo_permission.csv\u0026quot;, O_RDONLY|O_CLOEXEC) = -1 EACCES (Permission denied)  Esse processo est√° salvando algo que n√£o deveria? Onde? Talvez voc√™ queira monitorar o que uma depend√™ncia externa anda fazendo no seu sistema de arquivos. Outro problema que pode ocorrer √© caso voc√™ delete um arquivo usado por uma depend√™ncia, contudo tal depend√™ncia fez um cache em algum lugar antes de voc√™ efetuar a remo√ß√£o. O que te impede de ressetar a depend√™ncia.\nUsando o mesmo comando anterior √© poss√≠vel buscar onde esses caches e arquivos est√£o\n$ strace -f -e trace=file comando 2\u0026gt;\u0026amp;1 | awk '/^open/{print}'  se voc√™ quiser pegar apenas as chamadas que n√£o retornaram em falha digite\n$ strace -f -e trace=file comando 2\u0026gt;\u0026amp;1 | awk '/^open/ \u0026amp;\u0026amp; !/= -1/ {print}'  Extras envolvendo arquivos (/proc/) e strace Usando problemas comuns envolvendo arquivos e conex√µes conversamos um pouco sobre o strace e lsof. Conceitos como SYS CALL e a pasta /proc/ tamb√©m foram mencioandos. Darei alguns exemplos de algumas outras quest√µes que podemos responder usando esses outros elementos.\nGerando um sum√°rio de SYS CALL Voc√™ pode sumarizar todas as sys call feitas por um processo usando o argumento -c. Isso pode te ajudar a economizar tempo numa pre-an√°lise.\nO comando abaixo retorna as sys calls efetuadas pelo comando make sync-env\n$ strace -c -e trace=!\\wait4 make sync-env  outro argumento que foi alterado aqui √© o operador !\\ que diz para o strace ignorar as sys call do tipo wait4. O ouput ser√° algo do tipo:\n% time seconds usecs/call calls errors syscall ------ ----------- ----------- --------- --------- ---------------- 14,54 0,000209 6 33 13 openat 13,01 0,000187 17 11 vfork 12,32 0,000177 7 25 mmap 8,49 0,000122 3 31 close 8,42 0,000121 5 21 rt_sigprocmask 8,14 0,000117 6 17 read 6,89 0,000099 5 19 11 stat 5,85 0,000084 3 23 fstat 2,85 0,000041 8 5 mprotect 2,64 0,000038 9 4 write 2,51 0,000036 2 16 fcntl 2,02 0,000029 3 9 rt_sigaction 1,95 0,000028 14 2 readlink 1,95 0,000028 14 2 getdents64 1,25 0,000018 4 4 brk 1,25 0,000018 18 1 1 access 1,25 0,000018 3 5 pipe 1,11 0,000016 4 4 ioctl 0,84 0,000012 6 2 getcwd 0,70 0,000010 10 1 munmap 0,49 0,000007 7 1 lstat 0,49 0,000007 7 1 execve 0,49 0,000007 3 2 prlimit64 0,35 0,000005 5 1 chdir 0,21 0,000003 3 1 arch_prctl ------ ----------- ----------- --------- --------- ---------------- 100.00 0,001437 241 25 total  A coluna time diz que make sync-env gastou $14$% do tempo (com exce√ß√£o do wait4) em sys calls do tipo openat e $13$ das $33$ chamadas n√£o foram bem sucedidas.\nO processo foi iniciado com as vari√°veis de ambiente corretas? Os pr√≥ximos exemplos envolvem situa√ß√µes em que um processo foi iniciado, mas voc√™ quer verificar algumas informa√ß√µes sobre o mesmo sem que seja necess√°rio matar e reiniciar processo. Imagine fazer isso em produ√ß√£o? Ou com um modelo de ML que j√° gastou muitos R$ para chegar no est√°gio atual.\nVamos continuar com o nosso server_mlops.py. Suponha que o processo foi iniciado usando uma vari√°vel de ambiente extra, ANSWER.\n$ ANSWER=42 python server_mlops.py  Ap√≥s o inicio do processo como saber com quais vari√°veis de ambiente ele est√° usando? Essa vari√°veis setam por exemplo bibliotecas de otimiza√ß√£o(BLAS, LAPACK), env\u0026rsquo;s python etc.\nComo dito em um exemplo anterior, a pasta /proc cont√™m arquivos representado o estado dos processos em execu√ß√£o. Supondo que o PID do processo √© 4031 voc√™ pode acessar as vari√°veis de ambiente do mesmo atrav√©s de cat /proc/4031/environ. Mas o output √© meio feio, vamos usar tr para trocar os caracteres nulos \\0 por quebras de linhas, \\n.\n$ tr '\\0' '\\n' \u0026lt; /proc/4031/environ  Voc√™ ter√° um output do tipo\nANSWER=42 SHELL=/bin/bash LANGUAGE=en_US JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/bin/java ...more stuff  Se voc√™ quiser filtrar apenas linhas que comecem com a string CONDA fa√ßa\n$ tr '\\0' '\\n' \u0026lt; /proc/4031/environ 2\u0026gt;\u0026amp;1 | awk '/^CONDA/ {print}'  o output no meu caso foi algo do tipo\nCONDA_EXE=/home/devmessias/anaconda3/bin/conda CONDA_PREFIX=/home/devmessias/anaconda3 CONDA_PROMPT_MODIFIER=(base) CONDA_SHLVL=1 CONDA_PYTHON_EXE=/home/devmessias/anaconda3/bin/python CONDA_DEFAULT_ENV=base  Esqueci de redirecionar os outputs do processo para um arquivo. O que fazer? Suponha que voc√™ iniciou um processo e n√£o redirecionou os outputs para um arquivo de texto por esquecimento ou por subestimar problemas. Se reiniciar o processo n√£o √© uma op√ß√£o voc√™ est√° com problemas. Felizmente √© poss√≠vel usar o strace para interceptar os outputs e salva-los em um arquivo externo.\nA SYS CALL respons√°vel por requisitar a escrita no stdin, stdout e stderr √© a write . Veja o manual dessa chamada\n$ man 2 write  NAME write - write to a file descriptor SYNOPSIS #include \u0026lt;unistd.h\u0026gt; ssize_t write(int fd, const void *buf, size_t count);  O primeiro argumento √© um inteiro que representa o file descriptor. Sendo que fd=1 implica que a chamada escrever√° no stdout e fd=2 no stderr . Portanto, n√£o existe nenhum segredo aqui. Se voc√™ quiser capturar os outputs basta filtrar as SYS CALL do tipo write e file descriptor 1 ou 2 e envia-las para o arquivo desejado. Temos que tomar cuidado s√≥ com as algumas coisas aqui. No manual do strace (man strace) voc√™ vera que por padr√£o ele printa apenas $32$ caracteres em uma string. Portanto, precisamos aumentar o limite com o argumento -s. Tamb√©m √© interessante traquear os forks. No caso do server_mlops.py por exemplo, qualquer print dentro de um m√©todo n√£o ser√° executado na main, ent√£o o -f √© obrigat√≥rio.\nO comando para redirecionar as saidas do stdout e stderr no arquivo out.txt pode ser colocado da seguinte maneira com o log dos tempos (-t) opicional.\n$ strace -f -t -etrace=write -s 666 -p PID_DO_PROCESSO 2\u0026gt;\u0026amp;1 | grep --line-buffered -e 'write(2, ' -e 'write(1, ' \u0026gt;\u0026gt; out.txt  O c√≥digo abaixo tem uma altera√ß√£o no server_mlops.py , e execute ele assim como o client_mlops.py. Pegando o PID do serve_mlops voc√™ conseguir√° explorar esse exemplo\n# server_mlops.py import time import flask import sys app = flask.Flask(__name__) @app.route('/') def hello_world(): sleep_time = flask.request.args.get('sleep', default=10, type=int) print('sleep_time:', sleep_time) for i in range(sleep_time): print(f'INFO: {i} of sleep_time \\n asdf \\t ') print(f'ERROR: Example msg {i}', file=sys.stderr) time.sleep(1) return 'Hello World!' if __name__ == '__main__': app.run()  Qual comando gerou o processo e onde √© o seu working dir? Essa pergunta talvez n√£o seja t√£o dif√≠cil de responder se voc√™ tem o htop instalado. Mas supondo que voc√™ n√£o lembra as informa√ß√µes sobre o comando que gerou o processo execute o comando abaixo\n$ tr '\\0' '\\t' \u0026lt; /proc/PID_CLIENT_MLOPS/cmdline  o output ser√°\npython\tclient_mlops.py\t--sleep\t1000  Para descobrir o diret√≥rio do client_mlops.py basta executar\n$ readlink /proc/PID_CLIENT_MLOPS/cwd  Agradecimentos \u0026amp; Sugest√µes  Achou um erro? Tem alguma sugest√£o ou dica? mande um email para devmessias@gmail.com.\n  Obrigado Elisa Ribeiro por ter corrigido os typos da primeira vers√£o do post.  Reynaldo Allan Fulin pelas discuss√µes sempre √∫teis sobre linux.  ","date":1643974260,"expirydate":-62135596800,"kind":"page","lang":"pt-br","lastmod":1643974260,"objectID":"73936ee0ebb3d5b836118509449c8b2e","permalink":"/pt-br/post/investigando_processos_e_bugs_strace_lsof_no_linux/","publishdate":"2022-02-04T08:31:00-03:00","relpermalink":"/pt-br/post/investigando_processos_e_bugs_strace_lsof_no_linux/","section":"post","summary":"Aprenda a identificar problemas silenciosos que envolvam conex√µes e arquivos usando o strace e lsof. Recupere arquivos deletados e redirecione o output de um processo on-the-fly","tags":["linux","strace","lsof","system calls","machine learning operations","python","debugging"],"title":"Dissecando processos e erros no Linux com o lsof e strace: casos para DevOps/MlOps","type":"post"},{"authors":null,"categories":["Data analysis"],"content":" Dispon√≠vel em https://opencodecom.net/\n No c√©lebre trabalho ‚ÄúCan One Hear the Shape of a Drum?‚Äù[1] Kack questiona se conhecendo o espectro (som) de um certo operador que define as oscila√ß√µes de uma membrana (tambor) seria poss√≠vel identificar o formato de tal membrana de maneira un√≠voca. Discutiremos aqui como √© poss√≠vel ouvir matrizes de correla√ß√£o usando seu espectro e como podemos remover o ru√≠do desse som usando resultados da teoria de matrizes aleat√≥rias. Veremos como essa filtragem pode aprimorar algoritmos de constru√ß√£o de carteiras de investimentos.\n Minhas motiva√ß√µes para escrever esse texto foram o movimento Learn In Public-Sibelius Seraphini e o Nobel de F√≠sica de 2021. Um dos temas de Giorgio Parisi √© o estudo de matrizes aleat√≥rias www.nobelprize.org 2021.\n ..\n Jupyter notebook dispon√≠vel aqui\n 1-Introdu√ß√£o: teorema central do limite O teorema central do limite est√° no cora√ß√£o da an√°lise estat√≠stica. Em poucas palavras o mesmo estabelece o seguinte.\n Suponha uma amostra $A = (x_1, x_2, \\dots, x_n)$ de uma vari√°vel aleat√≥ria com m√©dia $\\mu$ e vari√¢ncia $\\sigma^2$ finita. Se a amostragem √© $i.i.d.$ o teorema central do limite estabelece que a distribui√ß√£o de probababilidade da m√©dia amostral converge para uma distribui√ß√£o normal com vari√¢ncia $\\sigma^2/n$ e m√©dia $\\mu$ a medida que $n$ aumenta.\n Note que eu n√£o disse nada a respeito de como tal amostra foi gerada; em nenhum momento citei distribui√ß√£o de Bernoulli, Gauss, Poisson, etc. Desta maneira podemos dizer que tal converg√™ncia √© uma propriedade universal de amostras aleat√≥rias $i.i.d.$. Essa universalidade √© poderosa, pois garante que √© poss√≠vel estimar a m√©dia e vari√¢ncia de uma popula√ß√£o atrav√©s de um conjunto de amostragens.\nN√£o √© dif√≠cil fazer um experimento computacional onde a implica√ß√£o desse teorema apare√ßa\nimport numpy as np import matplotlib.pyplot as plt import pandas as pd import seaborn as sns import warnings from matplotlib import style warnings.filterwarnings('ignore') style.use('seaborn-white') np.random.seed(22)  Usaremos uma amostragem de uma distribui√ß√£o exponencial com m√©dia $\\mu = 4$. Tal distribui√ß√£o tem uma vari√¢ncia dada por $1/\\mu^2$. Faremos $10000$ experimentos com amostras de tamanho $500$. Posteriormente calcularemos a media de cada experimento, mean_by_exp\nrate = 0.25 mu = 1/rate sample_size=500 exponential_sample = np.random.exponential(mu, size=(sample_size, 30000)) mean_by_exp = exponential_sample.mean(axis=0)  Agora basta plotar o histograma em compara√ß√£o com a distribui√ß√£o normal dada pelo teorema central do limite\nsns.distplot(mean_by_exp, norm_hist=True, label='sample') x = np.linspace(2.5, 5.5, 100) var = mu**2/(sample_size) y = np.exp(-(x-mu)**2/(2*var))/np.sqrt(2*np.pi*var) plt.plot(x, y, label=r'$N(\\mu, \\sigma)$', c='tomato') plt.legend() plt.xlim(3., 5) plt.savefig('exponential_distribution.png', facecolor='w') plt.close()  Note na figura acima que o plot para a fun√ß√£o $\\frac{e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}}{\\sqrt(2\\pi\\sigma^2)}$ e o histograma coincidem. Voc√™ pode testar essa coincid√™ncia com outras distribui√ß√µes, o mesmo comportamento se repetira. √â isso que quero dizer com universalidade.\nUm questionamento v√°lido √© que estamos tratando apenas de uma vari√°vel aleat√≥ria e sua amostragem. Mas no mundo real existem outras estruturas mais intricadas. Por exemplo pegue um conjunto de vari√°veis aleat√≥rias $\\mathcal C=(X_{1 1}, X_{1 2}, \\cdots, X_{N N})$, suponha que exista uma certa **simetria** nesse conjunto, uma possibilidade √© $X_{i j} = X_{j i}$. N√£o √© dif√≠cil imaginar situa√ß√µes onde tal conjunto apare√ßa.\nPodemos armazenar uma realiza√ß√£o de $\\mathcal C$ em uma matriz que nada mais √© que um grafo completo com pesos. Ao estudar essas matrizes oriundas desse tipo de amostragem entramos em um novo campo da matem√°tica, o campo das matrizes aleat√≥rias. Nesse campo de estudos uma amostragem n√£o retorna um n√∫mero, mas sim uma matriz.\nA fun√ß√£o normalRMT apresentada abaixo √© um gerador de matrizes aleat√≥rias conhecidas como Gaussianas ortogonais.\ndef normalRMT(n=100): \u0026quot;\u0026quot;\u0026quot;Generate a random matrix with normal distribution entries Args: n : (int) number of rows and columns Returns: m : (numpy.ndarray) random matrix \u0026quot;\u0026quot;\u0026quot; std = 1/np.sqrt(2) m = np.random.normal(size=(n,n), scale=std) m = (m+m.T) m /= np.sqrt(n) return m np.set_printoptions(precision=3) print(f'{normalRMT(3)},\\n\\n{normalRMT(3)}')  [[-1.441e+00 -2.585e-01 -1.349e-01] [-2.585e-01 -2.304e-01 1.166e-03] [-1.349e-01 1.166e-03 -1.272e+00]], [[-0.742 0.607 -0.34 ] [ 0.607 0.678 0.277] [-0.34 0.277 -0.127]]  Sabemos que quando estamos trantando de vari√°veis aleat√≥rias o teorema central do limite √© important√≠ssimo. O que voc√™ pode se perguntar agora √©: Existe um an√°logo para o teorema central do limite para matrizes aleat√≥rias?\n2-N√∫cleos at√¥micos, g√°s de n√∫meros primos e universalidade Para o bem e para o mal o conhecimento da f√≠sica at√¥mica foi um dos temas mais importantes desenvolvidos pela humanidade. Portanto, n√£o √© de se estranhar que ap√≥s o ano de 1930 iniciou-se uma grande corrida para compreender n√∫cleos at√¥micos pesados e a f√≠sica de n√™utrons [13].\nPara compreender essa nova f√≠sica de n√™utrons era necess√°rio conhecer a organiza√ß√£o do espectro de resson√¢ncia dos n√∫cleos pesados (esse espectro nada mais √© que os autovalores de um operador muito especial). Uma maneira de se fazer isso √© do jeito que muitas das coisas s√£o estudadas na f√≠sica: pegando se uma coisa e jogando na dire√ß√£o da coisa a ser estudada. Essa metodologia experimental torna poss√≠vel amostrar alguns valores poss√≠veis para o espectro. Contudo, acredito que n√£o preciso argumentar que fazer isso naquela √©poca era extremamente dif√≠cil e caro. Poucos centros conseguiam realizar alguns experimentos e ainda com uma resolu√ß√£o muito baixa para obter resultados suficientes para uma compreens√£o adequada dos n√∫cleos. Era preciso uma sa√≠da mais barata e ela foi encontrada. Tal sa√≠da dependeu apenas de f√≠sica-matem√°tica e ma√ßos de papel.\nDentre os pioneiros que decidiram atacar o problema de n√∫cleos pesados usando matem√°tica temos Eugene Paul Wigner (Nobel de 1963). A grande sacada de Wigner foi perceber que o fato das intera√ß√µes nucleares serem t√£o complicadas e a infinitude de graus de liberdade seria poss√≠vel tentar compreender essas intera√ß√µes como uma amostragem sujeita a certas condi√ß√µes de simetria.[10 , 11]\nAqui com simetria queremos dizer que as matrizes envolvidas possuem certas restri√ß√µes tais como\nnp.assert_equal(A, A.T)  Na pr√≥xima se√ß√£o veremos qual o impacto dessas restri√ß√µes na distribui√ß√£o de autovalores das matrizes envolvidas.\n2-a) Universalidade e lei do semic√≠rculo A fun√ß√£o normalRMT gera uma matriz sim√©trica onde as entradas s√£o extra√≠das de uma distribui√ß√£o normal. A fun√ß√£o laplaceRMT gera tamb√©m uma matriz sim√©trica, contudo as entradas s√£o amostras de uma distribui√ß√£o de Laplace.\ndef laplaceRMT(n=100): \u0026quot;\u0026quot;\u0026quot;Generate a random matrix with Laplace distribution Args: n : (int) size of the matrix Returns: m : (numpy.ndarray) random matrix with Laplace distribution \u0026quot;\u0026quot;\u0026quot; # we know that the variance of the laplace distribution is 2*scale**2 scale = 1/np.sqrt(2) m = np.zeros((n,n)) values = np.random.laplace(size=n*(n-1)//2, scale=scale) m[np.triu_indices_from(m, k=1)] = values # copy the upper diagonal to the lower diagonal m[np.tril_indices_from(m, k=-1)] = values np.fill_diagonal(m, np.random.laplace(size=n, scale=scale)) m = m/np.sqrt(n) return m  As propriedades universais que iremos explorar aqui est√£o ligadas aos autovalores das matrizes que foram amostradas. Como nossas matrizes s√£o sim√©tricas esses autovalores s√£o todos reais.\nComo cada matriz √© diferente os autovalores tamb√©m ser√£o, eles tamb√©m s√£o vari√°veis aleat√≥rias.\nvals_laplace = np.array([ np.linalg.eigh(laplaceRMT(n=100))[0] for i in range(100) ]) vals_normal = np.array([ np.linalg.eigh(normalRMT(n=100))[0] for i in range(100) ])  Na dec√°da de 50 n√£o havia poder computacional suficiente para realizar investiga√ß√µes n√∫mericas, mas voc√™ pode facilmente investigar como os autovalores se distribuem usando seu computador e gerando os histogramas\nt = 1 x = np.linspace(-2*t, 2*t, 100) y = np.zeros_like(x) x0 = x[4*t-x*2\u0026gt;0] y[4*t-x*2\u0026gt;0] = np.sqrt(4*t-x0**2)/(2*np.pi*t) plt.figure(facecolor='white') plt.hist(vals_laplace.flatten(), bins=50, hatch ='|', density=True, label='laplace', alpha=.2) plt.hist(vals_normal.flatten(), bins=50, hatch ='o', density=True, label='normal', alpha=.2) #sns.distplot(vals_laplace, norm_hist=True, label='Laplace') #sns.distplot(vals_normal, norm_hist=True, label='Normal') #sns.distplot(vals2, norm_hist=True, label='sample2') plt.plot(x, y, label='analytical') plt.xlabel(r'$\\lambda$') plt.ylabel(r'$\\rho(\\lambda)$') plt.legend() plt.savefig('RMT_distribution.png', facecolor='w') plt.close()  Veja na figura acima que a distribui√ß√£o de autovalores de matrizes sim√©tricas relacionadas com a distribui√ß√£o normal e de Laplace coincidem. O que estamos vendo aqui √© uma propriedade universal! Espero que voc√™ acredite em mim, mas dado que voc√™ tenha uma matriz aleat√≥ria sim√©trica, quadrada e se as entradas s√£o $i.i.d.$ a distribui√ß√£o de autovalores seguem o que √© conhecido como lei de semic√≠rculo de Wigner. Se a m√©dia e vari√¢ncia das entradas da matriz s√£o $0$ e $1$ respectivamente, ent√£o tal lei tem a seguinte express√£o para a distribui√ß√£o de probabilidade dos autovalores $$ \\rho(\\lambda) = \\begin{cases} \\frac{\\sqrt{4-\\lambda^2}}{(2\\pi)} \\textrm{ se } 4-\\lambda^2 \\leq 0\\newline 0 \\textrm{ caso contr√°rio.} \\end{cases} $$\nSe trocarmos as simetrias, restri√ß√µes ou formato (array.shape[0]!=array.shape[1]) das matrizes podemos encontrar varia√ß√µes da distribui√ß√£o apresentada acima. Exemplo se a matriz √© complexa mas Hermitiana, ou se √© \u0026ldquo;retangular\u0026rdquo; e real tal como algums matrizes que s√£o usadas para otimizar carteiras de investimento. A pr√≥xima se√ß√£o mostrar√° um caso com outro formato para universalidade.\n2-b) Repuls√£o entre n√∫meros primos Inciamos nosso texto falando sobre como a teoria de matrizes aleat√≥rias floreceu com os estudos estat√≠sticos de n√∫cleos at√¥micos pesados, especificamente nos trabalhos de Wigner. Embora tenha essa origem, muitas vezes ferramentas matem√°ticas desenvolvidas apenas por motiva√ß√µes pr√°ticas alcan√ßam outros ramos da matem√°tica. Brevemente discutirei aqui alguns pontos e rela√ß√µes com uma das conjecturas mais famosas da matem√°tica: a hip√≥tese de Riemann.\nQualquer pessoa com alguma curiosidade sobre matem√°tica j√° ouviu falar sobre a hip√≥tese de Riemann. Essa hip√≥tese estabele uma rela√ß√£o entre os zeros da fun√ß√£o zeta de Riemann e a distribui√ß√£o de n√∫meros primos. Dada sua import√¢ncia os maiores ci√™ntistas do s√©culo XX se debru√ßaram sobre ela almejando a imortalidade. Um desses ci√™ntistas foi Hugh Montgomery[4].\nPor volta de 1970 Montgomery notou que os zeros da fun√ß√£o zeta tinham uma certa propriedade cuirosa, pareciam repelir uns aos outros. Uma express√£o foi obtidada, que √© a seguinte\n$$ 1 - \\left( \\frac{\\sin (\\pi u)}{\\pi u}\\right)^2 + \\delta(u) $$\nN√£o se preocupe em entender a express√£o acima, ela est√° aqui apenas for motivos est√©ticos. O que importa √© que ela √© simples, t√£o simples que quando Freeman Dyson - um dos gigantes da f√≠sica-matem√°tica - colocou os olhos sobre tal equa√ß√£o ele notou imediatamente que tal equa√ß√£o era id√™ntica a obtida no contexto de matrizes aleat√≥rias Hermitianas (uma matriz √© hermitiana se ela √© igual a sua transporta conjugada) utilizadas para compreender o comportamento de n√∫cleos de √°tomos pesados, tais como ur√¢nio. A imagem abaixo √© uma carta escrita por Dyson.\nAs conex√£o entre um ferramental desenvolvido para estudar n√∫cleos at√¥micos e n√∫meros primos era realmente inesperada e talvez seja um dos caminhos para a prova da hipotese de Riemann[5, 2]. Contudo deixemos a hist√≥ria de lado, e voltemos ao ponto principal que √© te dar outro exemplo de universalidade.\nLembra que Montgomery disse que parecia haver uma repuls√£o entre os zeros da fun√ß√£o Zeta? O que seria esse conceito de repuls√£o em matrizes aleat√≥rias? Vamos checar numericamente\nVoltaremos a usar nossas matrizes aleat√≥rias geradas por distribui√ß√µes Gaussianas e Laplacianas. Usando o mesmo conjunto de autovalores que obtivemos anteriormente iremos calular o espa√ßamento entre cada par de autovalores para cada realiza√ß√£o de uma matriz aleat√≥ria. √â bem f√°cil, basta chamar a fun√ß√£o diff do numpy\ndiff_laplace = np.diff(vals_laplace, axis=1) diff_normal = np.diff(vals_normal, axis=1)  Agora o que faremos √© estimar a densidade de probabilidade usnado KDE. Mas antes disso aqui vai uma dica:\n Evite o KDE do sklearn no seu dia a dia, a implementa√ß√£o √© lenta e n√£o flexiv√©l. Dif√≠cilmente voc√™ conseguir√° bons resultados com milh√µes de pontos. Aqui vou usar uma implementa√ß√£o de KDE mais eficiente voc√™ pode instalar ela execuntando o comando abaixo\n !pip install KDEpy  from KDEpy import FFTKDE estimator_normal = FFTKDE( bw='silverman').fit(diff_normal.flatten()) x_normal, probs_normal = estimator_normal.evaluate(100) mu_normal = np.mean(diff_normal, axis=1).mean() estimator_laplace = FFTKDE( bw='silverman').fit(diff_laplace.flatten()) x_laplace, probs_laplace = estimator_laplace.evaluate(100) mu_laplace = np.mean(diff_laplace, axis=1).mean()  goe_law = lambda x: np.pi*x*np.exp(-np.pi*x**2/4)/2 spacings = np.linspace(0, 4, 100) p_s = goe_law(spacings) plt.plot(spacings, p_s, label=r'GOE anal√≠tico', c='orange', linestyle='--') plt.plot( x_normal/mu_normal, probs_normal*mu_normal, linestyle=':', linewidth=2, zorder=1, label='normal', c='black') plt.plot(x_laplace/mu_laplace, probs_laplace*mu_laplace, zorder=2, linestyle='--', label='laplace', c='tomato') plt.legend() plt.savefig('RMT_diff_distribution.png', facecolor='w') plt.close()  O que as distribui√ß√µes acima dizem √© que dado sua matriz ser $i.i.d.$ quadrada e sim√©trica ent√£o a probabilidade que voc√™ encontre dois autovalores iguais √© $0$ (zero). Al√©m do mais, existe um ponto de m√°ximo global em rela√ß√£o a distribui√ß√£o de espa√ßamentos. Esse comportamento que balanceia repuls√£o e atra√ß√£o dos autovalores lembra o comportamento de part√≠culas em um flu√≠do. N√£o √© de espantar que o m√©todo matem√°tico desenvolvido por Wigner para compreender tais matrizes foi denominado G√°s de Coloumb[2].\nAgora que voc√™ tem pelo menos uma ideia do que seria essa repuls√£o para o caso que j√° abordamos (matrizes sim√©tricas quadradas) voltemos ao problema dos n√∫meros primos.\nO comando a seguir baixa os primeiros 100k zeros da fun√ß√£o zeta\n!wget http://www.dtc.umn.edu/~odlyzko/zeta_tables/zeros1  Um pequeno preprocessamento dos dados:\nzeros = [] with open('zeros1', 'r') as f: for line in f.readlines(): # remove all spaces in the line and convert it to a float zeros.append(float(line.replace(' ', ''))) zeta_zeros = np.array(zeros)  Iremos calcular os espa√ßamentos entre os zeros, a m√©dia de tais espa√ßamento e executar um KDE\nfrom KDEpy import FFTKDE diff_zeta = np.diff(zeta_zeros[10000:]) m = np.mean(diff_zeta) estimator = FFTKDE( bw='silverman').fit(diff_zeta)  x, probs = estimator.evaluate(100) p = np.pi goe_law = lambda x: p*x*np.exp(-p*x**2/4)/2 def gue(xs): arg = -4/np.pi*np.power(xs,2) vals = 32/np.pi**2*xs**2*np.exp(arg) return vals spacings = np.linspace(0, 4, 100) p_s = gue(spacings) p_s2 = goe_law(spacings) plt.plot(x/m, probs*m, label='zeros zeta', linestyle='--') plt.plot(spacings, p_s, label=r'GUE anal√≠tico', c='blue', linestyle='-.') plt.plot(spacings, p_s2, label=r'GOE analitico', c='orange', linestyle='-.') plt.xlim(-0.1, 4) plt.legend() plt.savefig('zeta.png', facecolor='w') plt.close()  Veja que a propriedade de repuls√£o apareceu novamente. Note que dentro do plot eu coloquei uma outra curva GOE anal√≠tico, essa curva √© aquela que melhor descreve a distribui√ß√£o de espa√ßamentos quando suas matrizes aleat√≥rias s√£o sim√©tricas. Isso √© uma li√ß√£o importante aqui e resalta o que eu j√° disse anteriormente. N√£o temos apenas \u0026ldquo;um limite central para matrizes aleat√≥rias\u0026rdquo;, mas todo um zool√≥gico que mudar√° dependendo do tipo do seu problema..\n3-Usando RMT para encontrar e filtrar ru√≠dos em matrizes Na se√ß√£o 1 relembramos o resultado do teorema central do limite. Na se√ß√£o 2 foi mostrado que devemos ter em mente as simetrias e restri√ß√µes do nosso problema para analisar qual regra de universalidade √© respeitada. Isto √©: a depender da simetria e restri√ß√µes das nossas matrizes temos um outro \u0026ldquo;timbre de universalidade\u0026rdquo;.\nUm exemplo de outro timbre surge no espectro de matrizes de correla√ß√£o; matrizes que s√£o comumente utilizadas para an√°lise de carteiras de investimento. Tais matrizes tem pelo menos a seguinte estrutura:\n$$ \\mathbf C = \\mathbf X \\mathbf X^T $$ onde $\\mathbf X$ √© uma matriz real $N\\times M$ e $M\u0026gt;N$.\nO c√≥digo abaixo permite explorar em um exemplo o espectro de matrizes aleat√≥rias $N\\neq M$ com entradas dadas pela distribui√ß√£o normal.\ndef get_marchenko_bounds(Q, sigma=1): \u0026quot;\u0026quot;\u0026quot;Computes the Marchenko bounds for a given Q and sigma. Args: Q : (float) The Q-value. sigma : (float) The std value. Returns: (float, float): The lower and upper bounds for the eigenvalues. \u0026quot;\u0026quot;\u0026quot; QiSqrt = np.sqrt(1/Q) lp = np.power(sigma*(1 + QiSqrt),2) lm = np.power(sigma*(1 - QiSqrt),2) return lp, lm def marchenko_pastur(l, Q, sigma=1): \u0026quot;\u0026quot;\u0026quot;Return the probability of a Marchenko-Pastur distribution for a given Q , sigma and eigenvalue. Args: l : (float) The eigenvalue. Q : (float) The Q-value. sigma : (float) The std value. Returns: (float): The probability \u0026quot;\u0026quot;\u0026quot; lp, lm = get_marchenko_bounds(Q, sigma) # outside the interval [lm, lp] if l \u0026gt; lp or l \u0026lt; lm: return 0 return (Q/(2*np.pi*sigma*sigma*l))*np.sqrt((lp-l)*(l-lm)) def plot_marchenko_pastur(ax, eigen_values, Q, sigma=1, bins=100, just_the_bulk=False): \u0026quot;\u0026quot;\u0026quot;Plots the Marchenko-Pastur distribution for a given Q and sigma Args: ax : (matplotlib.axes) The axes to plot on. eigen_values : (np.array) The eigenvalues. Q : (float) : The Q-value. sigma : (float) std bins : (int) The number of bins to use. just_the_bulk : (bool) If True, only the eigenvalues inside of the Marchenko-Pastur bounds are plotted. \u0026quot;\u0026quot;\u0026quot; l_max, l_min = get_marchenko_bounds(Q, sigma) eigenvalues_points = np.linspace(l_min, l_max, 100) pdf = np.vectorize(lambda x : marchenko_pastur(x, Q, sigma))(eigenvalues_points) if just_the_bulk: eigen_values = eigen_values[ (eigen_values \u0026lt; l_max)] ax.plot(eigenvalues_points, pdf, color = 'r', label='Marchenko-Pastur') ax.hist(eigen_values, label='sample', bins=bins , density=True) ax.set_xlabel(r\u0026quot;$\\lambda$\u0026quot;) ax.set_ylabel(r\u0026quot;$\\rho$\u0026quot;) ax.legend() N = 1000 T = 4000 Q = T/N X = np.random.normal(0,1,size=(N,T)) cor = np.corrcoef(X) vals = np.linalg.eigh(cor)[0] fig, ax = plt.subplots(1,1) plot_marchenko_pastur(ax, vals, Q, sigma=1, bins=100) plt.legend() plt.savefig('Marchenko_Pastur.png', facecolor='w') plt.close()  A fun√ß√£o em vermelho na figura acima √© a universalidade que aparece em matrizes com a restri√ß√£o $N\\times M$ e entradas $i.i.d.$ e m√©dia $0$. Tal universalidade tem como formato a distribui√ß√£o de Marchenko-Pastur que √© dada por\n$$ \\rho (\\lambda) = \\frac{Q}{2\\pi \\sigma^2}\\frac{\\sqrt{(\\lambda_{\\max} - \\lambda)(\\lambda - \\lambda_{\\min})}}{\\lambda} $$ onde $$ \\lambda_{\\max,\\min} = \\sigma^2(1 \\pm \\sqrt{\\frac{1}{Q}})^2. $$\nNote os par√¢metros como $Q$ e $\\sigma$. Tais par√¢metros precisam ser ajustados para obter um melhor fit com dados reais.\nAgora iremos para um caso real. Vamos usar dados obtidos via Yahoo Finance com a biblioteca yfinance para consturir uma matriz de correla√ß√£o com dados de ativos financeiros\n# voc√™ precisa desse pacote para baixar os dados !pip install yfinance  Isso aqui √© um post bem informal, ent√£o peguei peguei uma lista aleat√≥ria com alguns tickers que encontrei na internet\n!wget https://raw.githubusercontent.com/shilewenuw/get_all_tickers/master/get_all_tickers/tickers.csv  selecionei apenas 500 para evitar que o processo de download seja muito demorado\ntickers = np.loadtxt('tickers.csv', dtype=str, delimiter=',').tolist() tickers = np.random.choice(tickers, size=500, replace=False).tolist()  vamos baixar agora os dados em um peri√≥do espec√≠fico\nimport yfinance as yf df = yf.download (tickers, start=\u0026quot;2017-01-01\u0026quot;, end=\u0026quot;2019-10-01\u0026quot;, interval = \u0026quot;1d\u0026quot;, group_by = 'ticker', progress = True)  o yfinance vai gerar um dataframe com multiindex, ent√£o precisamos separar da forma que queremos\ntickers_available = list(set([ ticket for ticket, _ in df.columns.T.to_numpy()])) prices = pd.DataFrame() for ticker in tickers_available: try: prices[ticker] = df[(ticker, 'Adj Close')] except KeyError: pass  Agora iremos calcular o retorno. Aqui entra um ponto delicado. Voc√™ poder√° achar alguns posts na internet ou mesmo artigos argumentando que √© necess√°rio calcular o retorno como $\\log (r+1)$ pois assim as entradas da sua matriz seguir√° uma distribui√ß√£o normal o que permitir√° a aplica√ß√£o de RMT. J√° vimos no presente texto que n√£o precisamos que as entradas da matrizes venham de uma distribui√ß√£o normal para que a universalidade apare√ßa. A escolha ou n√£o de usar $\\log$ nos retornos merece mais aten√ß√£o, inclusive com cr√≠ticas em rela√ß√£o ao uso[6, 7, 8]. Mas esse post n√£o pretende te vender nada, por isso vou ficar com o mais simples.\n# calculamos os retornos returns_all = prices.pct_change() # a primeira linha n√£o faz sentido, n√£o existe retorno no primeiro dia returns_all = returns_all.iloc[1:, :] # vamos limpar todas as linhas se mnegocia√ß√£o e dropar qualquer coluna com muitos NaN returns_all.dropna(axis = 1, thresh=len(returns_all.index)/2, inplace=True) returns_all.dropna(axis = 0, inplace=True) # seleciona apenas 150 colunas returns_all = returns_all[np.random.choice(returns_all.columns, size=120, replace=False)] #returns_all = returns_all.iloc[150:]  Com o df pronto calcularemos a matriz de correla√ß√£o e seus autovalores\ncorrelation_matrix = returns_all.interpolate().corr() vals = np.linalg.eigh(correlation_matrix.values)[0]  Vamos usar os par√¢metros padr√µes para $Q$ e $\\sigma$ e torcer para que funcione\nT, N = returns_all.shape Q=T/N sigma= 1 fig, ax = plt.subplots(1,1) plot_marchenko_pastur(ax, vals, Q, sigma=1, bins=200, just_the_bulk=False) plt.legend() plt.savefig('Marchenko_Pastur_all.png', facecolor='w') plt.close()  Usando todo o intervalo de tempo do nosso df obtivemos o que parece um ajuste razo√°vel. √â claro que voc√™ poderia (deveria) rodar algum teste estatistico para verificar tal ajuste. Existem alguns trabalhos que fizeram essa an√°lise de forma rigorosa, comparando mercados e peri√≥dos espec√≠ficos em rela√ß√£o a distribui√ß√£o de Marchenko-Pastur[9].\nSe voc√™ for uma pessoa atenta notar√° que na imagem acima existem alguns autovalores fora do suporte da Marchenko-Pastur. A ideia de filtragem via RMT √© como dito em [9] testar seus dados em rela√ß√£o a \u0026ldquo;hip√≥tese nula\u0026rdquo; da RMT. No caso se seus autovalores est√£o dentro do bulk da distribui√ß√£o que descreve um modelo de entradas i.i.d..\nComo isso foi aplicado em alguns trabalhos? Vamos ver na pr√°tica.\nUsaremos $70$% da s√©rie hist√≥rica para calcular uma nova matriz de correla√ß√£o. Com a matriz de correla√ß√£o em m√£os vamos computar os autovalores e autovetores.\n# iremos usar 70% da serie para realizar a filtragem returns_all.shape[0]*0.70 n_days = returns_all.shape[0] n_days_in = int(n_days*(1-0.70)) returns = returns_all.copy() sample = returns.iloc[:(returns.shape[0]-n_days_in), :].copy() correlation_matrix = sample.interpolate().corr() vals, vecs = np.linalg.eigh(correlation_matrix.values)  Os autovalores e autovetores podem ser compreendidos como a decomposi√ß√£o de uma dada matriz. Portanto, o seguinte teste precisa passar\nassert np.abs( np.dot(vecs, np.dot(np.diag(vals), np.transpose(vecs))).flatten() - correlation_matrix.values.flatten() ).max() \u0026lt; 1e-10  A distribui√ß√£o de Marchenko-Pastur serve como um indicativo para nossa filtragem. O que faremos √© jogar fora todos os autovalores que est√£o dentro da distribui√ß√£o de Marchenko-Pastur, posteriormente reconstruiremos a matriz de correla√ß√£o.\nT, N = returns.shape Q=T/N sigma = 1 lp, lm = get_marchenko_bounds(Q, sigma) # Filter the eigenvalues out vals[vals \u0026lt;= lp ] = 0 # Reconstruct the matrix filtered_matrix = np.dot(vecs, np.dot(np.diag(vals), np.transpose(vecs))) np.fill_diagonal(filtered_matrix, 1)  Com a matriz de correla√ß√£o filtrada voc√™ pode fazer o que bem entender com ela - existem outras maneiras de se realizar uma filtragem - uma das poss√≠veis aplica√ß√µes que precisa ser utilizada com cuidado √© usar tal matriz filtrada como input para algoritmos de otimiza√ß√£o de carteira. Talvez fa√ßa um outro post descrevendo essa otimiza√ß√£o de forma mais clara, mas esse n√£o √© meu enfoque nesse post e nem minha especialidade. Portanto, se voc√™ quiser dar uma lida recomendo os seguintes posts: [17, 18]\nO que voc√™ precisa saber √© que uma matriz de covari√¢ncia, $\\mathbf C_\\sigma$, adimite uma decomposi√ß√£o em rela√ß√£o a matriz de correla√ß√£o atr√°ves da seguinte forma\n$$ \\mathbf C_\\sigma = \\mathbf D^{-1/2} \\mathbf C \\mathbf D^{-1/2} $$ onde $\\mathbf D^{-1/2}$ √© uma matriz diagonal com as entradas sendo os desvios padr√£o para cada serie de dados, isto √©\n$$ \\begin{bmatrix} \\sigma_{1} \u0026amp;0 \u0026amp;\\cdots \u0026amp;0 \\\n0 \u0026amp;\\sigma_{2} \u0026amp;\\cdots \u0026amp;0 \\\n\\vdots \u0026amp;\\vdots \u0026amp;\\ddots \u0026amp;\\vdots \\\n0 \u0026amp;0 \u0026amp;\\cdots \u0026amp;\\sigma_{M} \\end{bmatrix} $$\nDiscutimos uma maneira de obter uma matriz de correla√ß√£o filtrada, $\\mathbf{\\tilde C}$, atrav√©s de RMT, a ideia √© plugar essa nova matriz na equa√ß√£o anterior e obter uma nova matriz de covari√¢ncia onde as informa√ß√µes menos relevantes foram eliminadas.\n$$ \\mathbf{\\tilde C_\\sigma} = \\mathbf D^{-1/2} \\mathbf{\\tilde C} \\mathbf D^{-1/2}. $$\nTendo essa nova matriz de cov√¢riancia filtrada agora basta voc√™ ingerir ela em algum m√©todo preferido para otimiza√ß√£o e comparar com o resultado obtido usando a matriz original. Aqui usaremos o cl√°ssico Markowitz\n# Reconstruct the filtered covariance matrix covariance_matrix = sample.cov() inv_cov_mat = np.linalg.pinv(covariance_matrix) # Construct minimum variance weights ones = np.ones(len(inv_cov_mat)) inv_dot_ones = np.dot(inv_cov_mat, ones) min_var_weights = inv_dot_ones/ np.dot( inv_dot_ones , ones) variances = np.diag(sample.cov().values) standard_deviations = np.sqrt(variances) D = np.diag(standard_deviations) filtered_cov = np.dot(D ,np.dot(filtered_matrix,D)) filtered_cov = filtered_matrix filtered_cov = (np.dot(np.diag(standard_deviations), np.dot(filtered_matrix,np.diag(standard_deviations)))) filt_inv_cov = np.linalg.pinv(filtered_cov) # Construct minimum variance weights ones = np.ones(len(filt_inv_cov)) inv_dot_ones = np.dot(filt_inv_cov, ones) filt_min_var_weights = inv_dot_ones/ np.dot( inv_dot_ones , ones) def get_cumulative_returns_over_time(sample, weights): weights[weights \u0026lt;= 0 ] = 0 weights = weights / weights.sum() return (((1+sample).cumprod(axis=0))-1).dot(weights) cumulative_returns = get_cumulative_returns_over_time(returns, min_var_weights).values cumulative_returns_filt = get_cumulative_returns_over_time(returns, filt_min_var_weights).values  in_sample_ind = np.arange(0, (returns.shape[0]-n_days_in+1)) out_sample_ind = np.arange((returns.shape[0]-n_days_in), returns.shape[0]) f = plt.figure() ax = plt.subplot(111) points = np.arange(0, len(cumulative_returns))[out_sample_ind] ax.plot(points, cumulative_returns[out_sample_ind], 'orange', linestyle='--', label='original') ax.plot(points, cumulative_returns_filt[out_sample_ind], 'b', linestyle='-.', label='filtrado') ymax = max(cumulative_returns[out_sample_ind].max(), cumulative_returns_filt[out_sample_ind].max()) ymin = min(cumulative_returns[out_sample_ind].min(), cumulative_returns_filt[out_sample_ind].min()) plt.legend() plt.savefig('comp.png', facecolor='w') plt.close()  Obtivemos uma melhora, mas novamente ressaltamos que uma analise mais criteriosa deveria ter sido feita. Vamos listar alguns pontos\n Em rela√ß√£o a quest√£o da escolha do intervalo de tempo. Isto √©, se o tamanho foi pequeno de mais para capturar a correla√ß√£o ou se foi grande de mais tal que as correla√ß√µes entre ativos n√£o s√£o estacion√°rias. O (n√£o) uso do $\\log$-retorno e seu impacto Uma escolha n√£o aleat√≥ria do que seria analisado M√©todos de unfolding dos autovalores (tema para outro post)  5 - Vantagens, cr√≠ticas e sugest√µes Voc√™ poder√° encontrar alguns trabalhos e posts descrevendo o uso de matrizes aleat√≥rias para filtragem de matrizes de correla√ß√£o sem uma boa cr√≠tica ou explicita√ß√£o das limita√ß√µes vou linkar aqui alguns pontos positivos e negativos e limita√ß√µes\nOnde realmente RMT se mostrou √∫til  Obviamente a RMT √© indiscutivelmente bem sucedida na matem√°tica e f√≠sica permitindo compreender sistemas apenas analisando a estat√≠stica dos gases matriciais. Em machine learning a RMT tamb√©m est√° provando ser uma ferramenta √∫til para compreender e melhorar o processo de aprendizado [15]. Entender comportamentos de sistemas sociais, biol√≥gicos e econ√¥micos. Aqui com entender o comportamento digo apenas saber se um dado segue uma caracter√≠stica dada por alguma lei espec√≠fica como a lei de semic√≠rculo. Isto √©, n√£o existe discuss√£o em voc√™ pegar um dado sistema que √© representado por uma matriz, estudar o comportamento do seu espectro de autovalores e autovetores e verificar que seguem algumas lei de universalidade. Isso √© bem diferente de dizer que se voc√™ filtrar uma matriz de correla√ß√£o via RMT voc√™ ir√° obter sempre resultados melhores.  Limita√ß√µes  Note que n√£o realizamos nenhum tipo de teste para decidir se realmente a distribui√ß√£o de autovalores era a distribui√ß√£o desejada. Baseamos isso s√≥ no olhometro, obviamente n√£o √© uma boa ideia. A filtragem apenas removendo os autovalores apesar de simples √© limitada e pode ser contra produtiva, outros m√©todos de filtragem podem ser inclusive melhores[14]. Inclusive n√£o √© uma das √∫nicas aplica√ß√µes de RMT para tratamento desse tipo de dado [16]  Para conhecer mais Ci√™ntistas  Alguns grandes nomes de RMT: Madan Lal Mehta, Freeman Dyson e Terrence Tao Alguns brasileiros: Marcel Novaes autor do livro Introduction to Random Matrices - Theory and Practice- arxiv; Fernando Lucas Metz trabalhou com o Nobel Giorgio Parisi.  Encontrou um erro ou quer melhorar esse texto?  Fa√ßa sua contribui√ß√£o criando uma issue ou um PR editando esse arquivo aqui random_matrix_theory/index.md.  6-Refer√™ncias   [1] M. Kac, ‚ÄúCan One Hear the Shape of a Drum?,‚Äù The American Mathematical Monthly, vol. 73, no. 4, p. 1, Apr. 1966, doi: 10.2307/2313748.\n  [2] Wigner, E.P., 1957. Statistical properties of real symmetric matrices with many dimensions (pp. 174-184). Princeton University.\n  [4] ‚ÄúFrom Prime Numbers to Nuclear Physics and Beyond,‚Äù Institute for Advanced Study. https://www.ias.edu/ideas/2013/primes-random-matrices (accessed Sep. 30, 2020).\n  [5] ‚ÄúGUE hypothesis,‚Äù What‚Äôs new. https://terrytao.wordpress.com/tag/gue-hypothesis/ (accessed Nov. 22, 2021).\n  [6] R. Hudson and A. Gregoriou, ‚ÄúCalculating and Comparing Security Returns is Harder than you Think: A Comparison between Logarithmic and Simple Returns,‚Äù Social Science Research Network, Rochester, NY, SSRN Scholarly Paper ID 1549328, Feb. 2010. doi: 10.2139/ssrn.1549328.\n  [7] A. Meucci, ‚ÄúQuant Nugget 2: Linear vs. Compounded Returns ‚Äì Common Pitfalls in Portfolio Management,‚Äù Social Science Research Network, Rochester, NY, SSRN Scholarly Paper ID 1586656, May 2010. Accessed: Dec. 01, 2021. [Online]. Available: https://papers.ssrn.com/abstract=1586656\n  [8] Lidian, ‚ÄúAnalysis on Stocks: Log(1+return) or Simple Return?,‚Äù Medium, Sep. 18, 2020. https://medium.com/@huangchingchiu/analysis-on-stocks-log-1-return-or-simple-return-371c3f60fab2 (accessed Nov. 25, 2021).\n  [9] N. A. Eterovic and D. S. Eterovic, ‚ÄúSeparating the Wheat from the Chaff: Understanding Portfolio Returns in an Emerging Market,‚Äù Social Science Research Network, Rochester, NY, SSRN Scholarly Paper ID 2161646, Oct. 2012. doi: 10.2139/ssrn.2161646.\n  [10] E. P. Wigner, ‚ÄúCharacteristic Vectors of Bordered Matrices With Infinite Dimensions,‚Äù Annals of Mathematics, vol. 62, no. 3, pp. 548‚Äì564, 1955, doi: 10.2307/1970079.\n  [11] E. P. Wigner, ‚ÄúOn the statistical distribution of the widths and spacings of nuclear resonance levels,‚Äù Mathematical Proceedings of the Cambridge Philosophical Society, vol. 47, no. 4, pp. 790‚Äì798, Oct. 1951, doi: 10.1017/S0305004100027237.\n  [13] F. W. K. Firk and S. J. Miller, ‚ÄúNuclei, Primes and the Random Matrix Connection,‚Äù Symmetry, vol. 1, no. 1, pp. 64‚Äì105, Sep. 2009, doi: 10.3390/sym1010064.\n  [14] L. Sandoval, A. B. Bortoluzzo, and M. K. Venezuela, ‚ÄúNot all that glitters is RMT in the forecasting of risk of portfolios in the Brazilian stock market,‚Äù Physica A: Statistical Mechanics and its Applications, vol. 410, pp. 94‚Äì109, Sep. 2014, doi: 10.1016/j.physa.2014.05.006.\n  [15] M. E. A. Seddik, C. Louart, M. Tamaazousti, and R. Couillet, ‚ÄúRandom Matrix Theory Proves that Deep Learning Representations of GAN-data Behave as Gaussian Mixtures,‚Äù arXiv:2001.08370 [cs, stat], Jan. 2020, Accessed: Dec. 05, 2021. [Online]. Available: http://arxiv.org/abs/2001.08370\n  [16] D. B. Aires, ‚ÄúAn√°lise de crises financeiras brasileiras usando teoria das matrizes aleat√≥rias,‚Äù Universidade Estadual Paulista (Unesp), 2021. Accessed: Dec. 05, 2021. [Online]. Available: https://repositorio.unesp.br/handle/11449/204550\n  [17] S. Rome, ‚ÄúEigen-vesting II. Optimize Your Portfolio With Optimization,‚Äù Scott Rome, Mar. 22, 2016. http://srome.github.io//Eigenvesting-II-Optimize-Your-Portfolio-With-Optimization/ (accessed Dec. 05, 2021).\n  [18] ‚Äú11.1 Portfolio Optimization ‚Äî MOSEK Fusion API for Python 9.3.10.‚Äù https://docs.mosek.com/latest/pythonfusion/case-studies-portfolio.html (accessed Dec. 05, 2021).\n  ","date":1638748800,"expirydate":-62135596800,"kind":"page","lang":"pt-br","lastmod":1638748800,"objectID":"5d162fdfde616b228ca7c28d61afde39","permalink":"/pt-br/post/random_matrix_portfolio/","publishdate":"2021-12-06T00:00:00Z","relpermalink":"/pt-br/post/random_matrix_portfolio/","section":"post","summary":"Como √© poss√≠vel ouvir matrizes de correla√ß√£o usando seu espectro?Como podemos analisar esse \"*barulho*\" usando resultados da teoria de matrizes aleat√≥rias para aprimorar algoritmos de constru√ß√£o de carterias de investimento?","tags":["portfolio","matrizes aleat√≥rias","random matrix","matrix","graphs","spectral data analysis","physics","statistics","eigenvalues","python","otmiza√ß√£o","autovalores","hist√≥ria da ci√™ncia"],"title":"Varia√ß√µes do teorema central do limite para matrizes aleat√≥rias.","type":"post"},{"authors":null,"categories":["Data Analysis"],"content":"Helios repo  Helios is a Python library aiming to provide an easy way to visualize huge networks dynamically. Helios also provides visualizations through an interactive Stadia-like streaming system in which users can be collaboratively access (and share) visualizations created in a server or through Jupyter Notebook/Lab environments. It incorporates state-of-the-art layout algorithms and optimized rendering techniques (powered by FURY).\n General Information ‚Ä¢ Key Features ‚Ä¢ Installation ‚Ä¢ Usage ‚Ä¢ History ‚Ä¢ Credits General Information  Website and Documentation: https://heliosnetwork.io/ Examples: https://heliosnetwork.io/examples_gallery/index.html Blog: https://heliosnetwork.io/blog.html Free software: MIT license Community: Come to chat on Discord  Key Features  Force-directed layout using octrees Minimum-distortion embeddings ForceAtlas2 using cugraph Interactive local and Remote rendering in Jupyter Notebooks WebRTC or MJPEG interactive streaming system  Installation Use pip install pointed to this repository:\npip git+https://github.com/fury-gl/helios.git  As an alternative, Helios can be installed from the source code through the following steps:\n  Step 1. Get the latest source by cloning this repo:\ngit clone https://github.com/fury-gl/helios.git    Step 2. Install requirements:\npip install -r requirements.txt    Step 3. Install Helios\nAs a local project installation using:\n pip install .  Or as an \u0026ldquo;editable\u0026rdquo; installation using:\n pip install -e .    Step 4: Enjoy!\n  For more information, see also installation page on heliosnetwork.io\nDependencies Helios requires Python 3.7+ and the following mandatory dependencies:\n numpy \u0026gt;= 1.7.1 vtk \u0026gt;= 8.1.0 fury  To enable WebRTC streaming and enable optimizations to the streaming system, install the following optional packages:\n  Required for WebRTC streaming\n aiohttp aiortc    Optional packages that may improve performance\n cython opencv    Testing After installation, you can install test suite requirements:\npip install -r requirements_dev.txt  And to launch test suite:\npytest -svv helios  Usage There are many ways to start using Helios:\n Go to Getting Started Explore our Examples or API.  Example usage:\nfrom helios import NetworkDraw from helios.layouts import HeliosFr import numpy as np vertex_count = 8 edges = np.array([ [0,1], [0,2], [1,2], [2,3], [3,4], [3,5], [4,5], [5,6], [6,7], [7,0] ]); centers = np.random.normal(size=(vertex_count, 3)) network_draw = NetworkDraw( positions=centers, edges=edges, colors=(0.25,0.25,0.25), scales=1, node_edge_width=0, marker='s', edge_line_color=(0.5,0.5,0.5), window_size=(600, 600) ) layout = HeliosFr(edges, network_draw) layout.start() network_draw.showm.initialize() network_draw.showm.start()  History Helios project started as a replacement to the desktop version of the Networks 3D tools. The project evolved quickly along the summer of 2021 due to the GSoC‚Äô21 under the responsibility of the Python Software Foundation and the FURY team. The majority of the initial work has been done by @devmessias mentored by @filipinascimento and @skoudoro. The GSoC‚Äô21 project associated with Helios is ‚ÄúA system for collaborative visualization of large network layouts using FURY‚Äù. Check out the final report for more information.\n","date":1631555002,"expirydate":-62135596800,"kind":"page","lang":"pt-br","lastmod":1631555002,"objectID":"372c9a2d4ece303d7ca7e05ee48f8a33","permalink":"/pt-br/project/helios/","publishdate":"2021-09-13T17:43:22Z","relpermalink":"/pt-br/project/helios/","section":"project","summary":"Helios is a Python library that provides an easy way to visualize huge networks dynamically. Helios also provides visualizations through an interactive Stadia-like streaming using WebRTC ","tags":["graphs","python","WebRTC","Complex Networks","open-software","data visualization"],"title":"Helios: graph layout viz and streaming","type":"project"},{"authors":null,"categories":null,"content":" Detailed weekly tasks, progress and work done can be found here\n Abstract We have changed some points of my project in the first meeting. Specifically, we focused the efforts into developing a streaming system using the WebRTC protocol that could be used in more generic scenarios than just the network visualization. In addition to that, we have opted to develop the network visualization for fury as a separated repository and package available here. The name Helios was selected for this new network visualization system based on the Fury rendering pipeline.\nProposed Objectives  Create a streaming system (stadia-like) for FURY  Should work in a low-bandwidth scenario Should allow user interactions and collaboration across the Internet using a web-browser   Helios Network System objectives:  Implement the Force-Directed Algorithm with examples Implement the ForceAtlas2 algorithm using cugraph with examples Implement Minimum-Distortion Embeddings algorithm (PyMDE) and examples Non-blocking network algorithms computation avoiding the GIL using the Shared Memory approach Create the documentation and the actions for the CI   Stretch Goals:  Create an actor in FURY to draw text efficiently using shaders Add support to draw millions of nodes using FURY Add support to control the opengl state on FURY    Objectives Completed   Create a streaming system (stadia-like) for FURY\nThere are several reasons to have a streaming system for data visualization. Because I am doing my Ph.D.¬†in developing country, I always need to think of the less expensive solutions to use the computational resources available. For example, with the GPU\u0026rsquo;s prices increasing, it is necessary to share the a single machine with GPU with other users at different locations.\nTo construct the streaming system for my project we have opted to follow three main properties and behaviors:\n avoid blocking the code execution in the main thread (where the vtk/fury instance resides) work inside of a low bandwidth environment make it easy and cheap to share the rendering result. For example, using the free version of ngrok  To achieve the first property we need to circumvent the GIL and allow python code to execute in parallel. Using the threading module alone is not good enough to reach real parallelism as Python calls in the same process can not execute concurrently. In addition to that, to achieve better organization it is desirable to define the server system as an uncoupled module from the rendering pipeline. Therefore, I have chosen to employ the multiprocessing approach for that. The second and third property can be only achieved choosing a suitable protocol for transfering the rendered results to the client. We have opted to implement two streaming protocols: the MJPEG and the WebRTC. The latter is more suitable for low-bandwidth scenarios [1].\nThe image below shows a simple representation of the streaming system.\n\u0026lt;center\u0026gt;  \u0026lt;/center\u0026gt;  The video below shows how our streaming system works smothly and can be easily integrated inside of a Jupyter notebook.\n   Video: WebRTC Streaming + Ngrok\n Video: WebRTC Streaming + Jupyter\nPull Requests: * https://github.com/fury-gl/fury/pull/480\n  2D and 3D marker actor\nThis feature gave FURY the ability to efficiently draw millions of markers and impostor 3D spheres. This feature was essential for the development of Helios. This feature work with signed distance fields (SDFs) you can get more information about how SDFs works here [4] .\nThe image below shows 1 million of markers rendered using an Intel HD graphics 3000.\n    Fine-Tunning the OpenGl State\nSometimes users may need to have finer control on how OpenGL will render the actors. This can be useful when they need to create specialized visualization effects or to improve the performance.\nIn this PR I have worked in a feature that allows FURY to control the OpenGL context created by VTK\nPull Request:\n https://github.com/fury-gl/fury/pull/432    Helios Network Visualization Lib: Network Layout Algorithms\nCase 1: Suppose that you need to monitor a hashtag and build a social graph. You want to interact with the graph and at the same time get insights about the structure of the user interactions. To get those insights you can perform a node embedding using any kind of network layout algorithm, such as force-directed or minimum distortion embeddings.\nCase 2: Suppose that you are modelling a network dynamic such as an epidemic spreading or a Kuramoto model. In some of those network dynamics a node can change the state and the edges related to the node must be deleted. For example, in an epidemic model a node can represent a person who died due to a disease. Consequently, the layout of the network must be recomputed to give better insights.\nIn the described cases, if we want a better (UX) and at the same time a more practical and insightful application of Helios, the employed layout algorithms should not block any kind of computation in the main thread.\nIn Helios we already have a lib written in C (with a python wrapper) which performs the force-directed layout algorithm using separated threads avoiding the GIL problem and consequently avoiding blocking the main thread. But what about the other open-source network layout libs available on the internet? Unfortunately, most of those libs have not been implemented like Helios force-directed methods and consequently, if we want to update the network layout the Python interpreter will block the computation and user interaction in your network visualization.\nMy solution for having PyMDE and CuGraph-ForceAtlas not blocking the main thread was to break the network layout method into two different types of processes: A and B and communicate both process using the Shared Memory approach. You can more information about this PR through my following posts [2], [3].\n  The image below show an example that I made and is available at https://github.com/fury-gl/helios/blob/main/docs/examples/viz_mde.py\nPull Requests:\n MDE Layout: https://github.com/fury-gl/helios/pull/6 CuGraph ForceAtlas2 https://github.com/fury-gl/helios/pull/13 Force-Directed and MDE improvements https://github.com/fury-gl/helios/pull/14 Helios Network Visualization Lib: Visual Aspects  I\u0026rsquo;ve made several stuffs to give Helios a better visual aspects. One of them was to give a smooth real-time network layout animations. Because the layout computations happens into a different process that the process responsible to render the network was necessary to record the positions and communicate the state of layout between both process.\nThe GIF below shows how the network layout through IPC behaved before these modification\n\u0026lt;center\u0026gt;  \u0026lt;/center\u0026gt;  below, you can see how after those modifications the visual aspect is better.\n\u0026lt;center\u0026gt;  \u0026lt;/center\u0026gt;  Pull Requests:\n OpenGL SuperActors: https://github.com/fury-gl/helios/pull/1 Fixed the flickering effect https://github.com/fury-gl/helios/pull/10 Improvements in the network node visual aspects https://github.com/fury-gl/helios/pull/15 Smooth animations when using IPC layouts https://github.com/fury-gl/helios/pull/17 Helios Network Visualization Lib: CI and Documentation  Because Helios was an project that begins in my GSoC project It was necessary to create the documentation, hosting and more. Now we have a online documentation available at https://heliosnetwork.io/ altough the documentation still need some improvements.\nBelow is presented the Helios Logo which was developed by my mentor Filipi Nascimento.\n\u0026lt;center\u0026gt;  \u0026lt;/center\u0026gt;  Pull Requests:\n  CI and pytests: https://github.com/fury-gl/helios/pull/5, https://github.com/fury-gl/helios/pull/20\n  Helios Logo, Sphinx Gallery and API documentation https://github.com/fury-gl/helios/pull/18\n  Documentation improvements: https://github.com/fury-gl/helios/pull/8\n  Objectives in Progress\n  Draw texts on FURY and Helios\nThis two PRs allows FURY and Helios to draw millions of characters in VTK windows instance with low computational resources consumptions. I still working on that, finishing the SDF font rendering which the theory behinds was developed here [5].\nPull Requests:\n  https://github.com/fury-gl/helios/pull/24\n  https://github.com/fury-gl/fury/pull/489\n\u0026lt;center\u0026gt;  \u0026lt;/center\u0026gt;      GSoC weekly Blogs\nWeekly blogs were added to the FURY Website.\nPull Requests:\n First Evaluation: https://github.com/fury-gl/fury/pull/476 Second Evaluation: TBD    References [1] ( Python GSoC - Post #1 - A Stadia-like system for data visualization - demvessias s Blog, n.d.; https://blogs.python-gsoc.org/en/demvessiass-blog/post-1-a-stadia-like-system-for-data-visualization/\n[2] Python GSoC - Post #2: SOLID, monkey patching a python issue and network layouts through WebRTC - demvessias s Blog, n.d.; https://blogs.python-gsoc.org/en/demvessiass-blog/post-2-solid-monkey-patching-a-python-issue-and-network-layouts-through-webrtc/\n[3] Python GSoC - Post #3: Network layout algorithms using IPC -demvessias s Blog, n.d.)https://blogs.python-gsoc.org/en/demvessiass-blog/post-3-network-layout-algorithms-using-ipc/\n[4] Rougier, N.P., 2018. An open access book on Python, OpenGL and Scientific Visualization [WWW Document]. An open access book on Python, OpenGL and Scientific Visualization. URL https://github.com/rougier/python-opengl (accessed 8.21.21).\n[5] Green, C., 2007. Improved alpha-tested magnification for vector textures and special effects, in: ACM SIGGRAPH 2007 Courses on -SIGGRAPH \u0026lsquo;07. Presented at the ACM SIGGRAPH 2007 courses, ACM Press, San Diego, California, p.¬†9. https://doi.org/10.1145/1281500.1281665\n","date":1629676800,"expirydate":-62135596800,"kind":"page","lang":"pt-br","lastmod":1629676800,"objectID":"587bf63e7f1ec241ef33817156c03abb","permalink":"/pt-br/post/2021-23-08-gsoc-devmessias-final-report/2021-23-08-gsoc-devmessias-final-report/","publishdate":"2021-08-23T00:00:00Z","relpermalink":"/pt-br/post/2021-23-08-gsoc-devmessias-final-report/2021-23-08-gsoc-devmessias-final-report/","section":"post","summary":"Detailed weekly tasks, progress and work done can be found here\n Abstract We have changed some points of my project in the first meeting. Specifically, we focused the efforts into developing a streaming system using the WebRTC protocol that could be used in more generic scenarios than just the network visualization.","tags":["gsoc","python","open-source","data-viz","webrtc"],"title":"GSoC-  Google Summer of Code 2021 Final Work Product","type":"post"},{"authors":null,"categories":null,"content":"What did I do this week? FURY   PR fury-gl/fury#489:  | I\u0026rsquo;ve created the PR that will allow FURY to draw hundreds thousands of labels using texture maps. By default, this PR give to FURY three pre-built texture maps using different fonts. However, is quite easy to create new fonts to be used in a visualization. | It\u0026rsquo;s was quite hard to develop the shader code and find the correct positions of the texture maps to be used in the shader. Because we used the freetype-py to generate the texture and packing the glyps. However, the lib has some examples with bugs. But fortunelly, now everthing is woking on FURY. I\u0026rsquo;ve also created two different examples to show how this PR works.\n* The first example, viz_huge_amount_of_labels.py, shows that feature has a realy good performance. The user can draw hundreds of thounsands of characters in a regular computer. ![](https://user-images.githubusercontent.com/6979335/129643743-6cb12c06-3415-4a02-ba43-ccc97003b02d.png) * The second example, viz_billboad_labels.py, shows the different behaviors of the label actor. In addition, presents to the user how to create a new texture atlas font to be used across different visualizations.    PR fury-gl/fury#437:   Fix: avoid multiple OpenGl context on windows using asyncio\nThe streaming system must be generic, but opengl and vtk behaves in uniques ways in each Operating System. Thus, can be tricky to have the same behavior acrros different OS. One hard stuff that we founded is that was not possible to use my TimeIntervals objects (implemented with threading module) with vtk. The reason for this impossibility is because we can't use vtk in windows in different threads. But fortunely, moving from the threading (multithreading) to the asyncio approcach (concurrency) have fixed this issue and now the streaming system is ready to be used anywhere.    Flickering\nFinally, I could found the cause of the flickering effect on the streaming system. This flickering was appearing only when the streaming was created using the Widget object. The cause seems to be a bug or a strange behavior from vtk. Calling      iren.MouseWheelForwardEvent() or\niren.MouseWheelBackwardEvent() inside of a thread without invoking the Start method from a vtk instance produces a memory corruption. Fortunately, I could fix this behavior and now the streaming system is working without this glitch effect.\nFURY/Helios    PR fury-gl/helios#24 :  This uses the PRfury-gl/fury#489: to give the network label feature to helios. Is possible to draw node labels, update the colors, change the positions at runtime. In addition, when a network layout algorithm is running this will automatically update the node labels positions to follow the nodes across the screen.\n  PR fury-gl/helios#23: Merged.  This PR granted compatibility between IPC Layouts and Windows. Besides that , now is quite easier to create new network layouts using inter process communication\nDid I get stuck anywhere? I did not get stuck this week.\n","date":1629072000,"expirydate":-62135596800,"kind":"page","lang":"pt-br","lastmod":1629072000,"objectID":"edd2b8c5458d59f4a45a245581d0bf0e","permalink":"/pt-br/post/2021-16-08-gsoc-devmessias-11/2021-16-08-gsoc-devmessias-11/","publishdate":"2021-08-16T00:00:00Z","relpermalink":"/pt-br/post/2021-16-08-gsoc-devmessias-11/2021-16-08-gsoc-devmessias-11/","section":"post","summary":"What did I do this week? FURY   PR fury-gl/fury#489:  | I\u0026rsquo;ve created the PR that will allow FURY to draw hundreds thousands of labels using texture maps. By default, this PR give to FURY three pre-built texture maps using different fonts.","tags":["gsoc","python","open-source","data-viz","webrtc"],"title":"GSoC-  SDF fonts and OpenGL","type":"post"},{"authors":null,"categories":null,"content":"Hi all. In the past weeks, I\u0026rsquo;ve been focusing on developing Helios; the network visualization library for FURY. I improved the visual aspects of the network rendering as well as implemented the most relevant network layout methods.\nIn this post I will discuss the most challenging task that I faced to implement those new network layout methods and how I solved it.\nThe problem: network layout algorithm implementations with a blocking behavior Case 1: Suppose that you need to monitor a hashtag and build a social graph. You want to interact with the graph and at the same time get insights about the structure of the user interactions. To get those insights you can perform a node embedding using any kind of network layout algorithm, such as force-directed or minimum distortion embeddings.\nCase 2: Suppose that you are modelling a network dynamic such as an epidemic spreading or a Kuramoto model. In some of those network dynamics a node can change the state and the edges related to the node must be deleted. For example, in an epidemic model a node can represent a person who died due to a disease. Consequently, the layout of the network must be recomputed to give better insights.\nIn described cases if we want a better (UX) and at the same time a more practical and insightful application of Helios layouts algorithms shouldn\u0026rsquo;t block any kind of computation in the main thread.\nIn Helios we already have a lib written in C (with a python wrapper) which performs the force-directed layout algorithm using separated threads avoiding the GIL problem and consequently avoiding the blocking. But and the other open-source network layout libs available on the internet? Unfortunately, most of those libs have not been implemented like Helios force-directed methods and consequently, if we want to update the network layout the python interpreter will block the computation and user interaction in your network visualization. How to solve this problem?\nWhy is using the python threading is not a good solution? One solution to remove the blocking behavior of the network layout libs like PyMDE is to use the threading module from python. However, remember the GIL problem: only one thread can execute python code at once. Therefore, this solution will be unfeasible for networks with more than some hundreds of nodes or even less! Ok, then how to solve it well?\nIPC using python As I said in my previous posts I\u0026rsquo;ve created a streaming system for data visualization for FURY using webrtc. The streaming system is already working and an important piece in this system was implemented using the python SharedMemory from multiprocessing. We can get the same ideas from the streaming system to remove the blocking behavior of the network layout libs.\nMy solution to have PyMDE and CuGraph-ForceAtlas without blocking was to break the network layout method into two different types of processes: A and B. The list below describes the most important behaviors and responsibilities for each process\nProcess A:\n Where the visualization (NetworkDraw) will happen Create the shared memory resources: edges, weights, positions, info.. Check if the process B has updated the shared memory resource which stores the positions using the timestamp stored in the info_buffer Update the positions inside of NetworkDraw instance  Process B:\n Read the network information stored in the shared memory resources: edges , weights, positions Execute the network layout algorithm Update the positions values inside of the shared memory resource Update the timestamp inside of the shared memory resource  I used the timestamp information to avoid unnecessary updates in the FURY/VTK window instance, which can consume a lot of computational resources.\nHow have I implemented the code for A and B? Because we need to deal with a lot of different data and share them between different processes I\u0026rsquo;ve created a set of tools to deal with that, take a look for example in the ShmManagerMultiArrays Object , which makes the memory management less painful.\nI'm breaking the layout method into two different processes. Thus I\u0026rsquo;ve created two abstract objects to deal with any kind of network layout algorithm which must be performed using inter-process-communication (IPC). Those objects are: NetworkLayoutIPCServerCalc ; used by processes of type B and NetworkLayoutIPCRender ; which should be used by processes of type A.\nI\u0026rsquo;ll not bore you with the details of the implementation. But let\u0026rsquo;s take a look into some important points. As I\u0026rsquo;ve said saving the timestamp after each step of the network layout algorithm. Take a look into the method _check_and_sync from NetworkLayoutIPCRender here. Notice that the update happens only if the stored timestamp has been changed. Also, look at this line helios/layouts/mde.py#L180, the IPC-PyMDE implementation This line writes a value 1 into the second element of the info_buffer. This value is used to inform the process A that everything worked well. I used that info for example in the tests for the network layout method, see the link helios/tests/test_mde_layouts.py#L43\nResults Until now Helios has three network layout methods implemented: Force Directed , Minimum Distortion Embeddings and Force Atlas 2. Here docs/examples/viz_helios_mde.ipynb you can get a jupyter notebook that I\u0026rsquo;ve a created showing how to use MDE with IPC in Helios.\nIn the animation below we can see the result of the Helios-MDE application into a network with a set of anchored nodes.\nNext steps I\u0026rsquo;ll probably focus on the Helios network visualization system. Improving the documentation and testing the ForceAtlas2 in a computer with cuda installed. See the list of opened issues\nSummary of most important pull-requests:  IPC tools for network layout methods (helios issue #7) fury-gl/helios/pull/6 New network layout methods for fury (helios issue #7) fury-gl/helios/pull/9 fury-gl/helios/pull/14 fury-gl/helios/pull/13 Improved the visual aspects and configurations of the network rendering(helios issue #12) https://github.com/devmessias/helios/tree/fury_network_actors_improvements Tests, examples and documentation for Helios (helios issues #3 and #4) fury-gl/helios/pull/5 Reduced the flickering effect on the FURY/Helios streaming system fury-gl/helios/pull/10 fury-gl/fury/pull/437/commits/a94e22dbc2854ec87b8c934f6cabdf48931dc279  ","date":1626048000,"expirydate":-62135596800,"kind":"page","lang":"pt-br","lastmod":1626048000,"objectID":"eafb66139737e8521897630ffd7e2538","permalink":"/pt-br/post/2021-07-12-gsoc-devmessias-6/2021-07-12-gsoc-devmessias-6/","publishdate":"2021-07-12T00:00:00Z","relpermalink":"/pt-br/post/2021-07-12-gsoc-devmessias-6/2021-07-12-gsoc-devmessias-6/","section":"post","summary":"Hi all. In the past weeks, I\u0026rsquo;ve been focusing on developing Helios; the network visualization library for FURY. I improved the visual aspects of the network rendering as well as implemented the most relevant network layout methods.","tags":["gsoc","python","open-source","data-viz","webrtc"],"title":"GSoC-  Network layout algorithms using IPC","type":"post"},{"authors":null,"categories":null,"content":"What did you do this week?   PR fury-gl/fury#422 (merged): Integrated the 3d impostor spheres with the marker actor.  PR fury-gl/fury#422 (merged): Fixed some issues with my maker PR which now it's merged on fury.  PR fury-gl/fury#432 I've made some improvements in my PR which can be used to fine tune the opengl state on VTK.  PR fury-gl/fury#437 I've made several improvements in my streamer proposal for FURY related to memory management.  PR fury-gl/helios#1 First version of async network layout using force-directed.  Did I get stuck anywhere? A python-core issue I've spent some hours trying to discover this issue. But now it's solved through the commit devmessias/fury/commit/071dab85\nThe¬†SharedMemory from python\u0026gt;=3.8 offers a new a way to share memory resources between unrelated process. One of the advantages of using the SharedMemory instead of the RawArray from multiprocessing is that the SharedMemory allows to share memory blocks without those processes be related with a fork or spawm method. The SharedMemory behavior allowed to achieve our jupyter integration and simplifies the use of the streaming system. However, I saw a issue in the shared memory implementation.\nLet\u0026rsquo;s see the following scenario:\n1-Process A creates a shared memory X 2-Process A creates a subprocess B using popen (shell=False) 3-Process B reads X 4-Process B closes X 5-Process A kills B 4-Process A closes X 5-Process A unlink() the shared memory resource X  The above scenario should work flawless. Calling unlink() in X is the right way as discussed in the python official documentation. However, there is a open issue related the unlink method\n  Issue: https://bugs.python.org/issue38119  PR python/cpython/pull/21516  Fortunately, I could use a monkey-patching solution to fix that meanwhile we wait to the python-core team to fix the resource_tracker (38119) issue.\nWhat is coming up next? I'm planning to work in the fury-gl/fury#432 and fury-gl/helios#1.\n","date":1624233600,"expirydate":-62135596800,"kind":"page","lang":"pt-br","lastmod":1624233600,"objectID":"4a844b52d2be7b6fa36201effef1381a","permalink":"/pt-br/post/2021-06-21-gsoc-devmessias-3/2021-06-21-gsoc-devmessias-3/","publishdate":"2021-06-21T00:00:00Z","relpermalink":"/pt-br/post/2021-06-21-gsoc-devmessias-3/2021-06-21-gsoc-devmessias-3/","section":"post","summary":"What did you do this week?   PR fury-gl/fury#422 (merged): Integrated the 3d impostor spheres with the marker actor.  PR fury-gl/fury#422 (merged): Fixed some issues with my maker PR which now it's merged on fury.","tags":["gsoc","python","open-source","data-viz","webrtc"],"title":"GSoC-  Bugs!","type":"post"},{"authors":null,"categories":null,"content":"Hi all! In this post I'll talk about the PR #437.\nThere are several reasons to have a streaming system for data visualization. Because I\u0026rsquo;m doing a PhD in a developing country I always need to think of the cheapest way to use the computational resources available. For example, with the GPUs prices increasing, it\u0026rsquo;s necessary to share a machine with a GPU with different users in different locations. Therefore, to convince my Brazilian friends to use FURY I need to code thinking inside of the (a) low-budget scenario.\nTo construct the streaming system for my project I\u0026rsquo;m thinking about the following properties and behaviors:\n I want to avoid blocking the code execution in the main thread (where the vtk/fury instance resides). The streaming should work inside of a low bandwidth environment. I need an easy way to share the rendering result. For example, using the free version of ngrok.  To achieve the property 1. we need to circumvent the GIL problem. Using the threading module alone it\u0026rsquo;s not good enough because we can\u0026rsquo;t use the python-threading for parallel CPU computation. In addition, to achieve a better organization it\u0026rsquo;s better to define the server system as an uncoupled module. Therefore, I believe that multiprocessing-lib in python will fit very well for our proposes.\nFor the streaming system to work smoothly in a low-bandwidth scenario we need to choose the protocol wisely. In the recent years the WebRTC protocol has been used in a myriad of applications like google hangouts and Google Stadia aiming low latency behavior. Therefore, I choose the webrtc as my first protocol to be available in the streaming system proposal.\nTo achieve the third property, we must be economical in adding requirements and dependencies.\nCurrently, the system has some issues, but it's already working. You can see some tutorials about how to use this streaming system here. After running one of these examples you can easily share the results and interact with other users. For example, using the ngrok For example, using the ngrok\n./ngrok http 8000  |\nHow does it works? The image below it's a simple representation of the streaming system.\nAs you can see, the streaming system is made up of different processes that share some memory blocks with each other. One of the hardest part of this PR was to code this sharing between different objects like VTK, numpy and the webserver. I'll discuss next some of technical issues that I had to learn/circumvent.\nSharing data between process We want to avoid any kind of unnecessary duplication of data or expensive copy/write actions. We can achieve this economy of computational resources using the multiprocessing module from python.\nmultiprocessing RawArray | The RawArray from multiprocessing allows to share resources between different processes. However, there are some tricks to get a better performance when we are dealing with RawArray's. For example, take a look at my PR in a older stage. In this older stage my streaming system was working well. However, one of my mentors (Filipi Nascimento) saw a huge latency for high-resolutions examples. My first thought was that latency was caused by the GPU-CPU copy from the opengl context. However, I discovered that I've been using RawArray's wrong in my entire life! | See for example this line of code fury/stream/client.py#L101 The code below shows how I've been updating the raw arrays\nraw_arr_buffer[:] = new_data  This works fine for small and medium sized arrays, but for large ones it takes a large amount of time, more than GPU-CPU copy. The explanation for this bad performance is available here : Demystifying sharedctypes performance. The solution which gives a stupendous performance improvement is quite simple. RawArrays implements the buffer protocol. Therefore, we just need to use the memoryview:\nmemview(arr_buffer)[:] = new_data  The memview is really good, but there it's a litle issue when we are dealing with uint8 RawArrays. The following code will cause an exception:\nmemview(arr_buffer_uint8)[:] = new_data_uint8  There is a solution for uint8 rawarrays using just memview and cast methods. However, numpy comes to rescue and offers a simple and a generic solution. You just need to convert the rawarray to a np representation in the following way:\narr_uint8_repr = np.ctypeslib.as_array(arr_buffer_uint8) arr_uint8_repr[:] = new_data_uint8  You can navigate to my repository in this specific commit position and test the streaming examples to see how this little modification improves the performance.\nMultiprocessing inside of different Operating Systems Serge Koudoro, who is one of my mentors, has pointed out an issue of the streaming system running in MacOs. I don't know many things about MacOs, and as pointed out by Filipi the way that MacOs deals with multiprocessing is very different than the Linux approach. Although we solved the issue discovered by Serge, I need to be more careful to assume that different operating systems will behave in the same way. If you want to know more,I recommend that you read this post Python: Forking vs Spawm. And it's also important to read the official documentation from python. It can save you a lot of time. Take a look what the official python documentation says about the multiprocessing method\nSource:https://docs.python.org/3/library/multiprocessing.html\n","date":1623456000,"expirydate":-62135596800,"kind":"page","lang":"pt-br","lastmod":1623456000,"objectID":"f5f9e0ba565c2e10a1d5adefe30e7377","permalink":"/pt-br/post/2021-06-12-gsoc-devmessias-2/2021-06-12-gsoc-devmessias-2/","publishdate":"2021-06-12T00:00:00Z","relpermalink":"/pt-br/post/2021-06-12-gsoc-devmessias-2/2021-06-12-gsoc-devmessias-2/","section":"post","summary":"Hi all! In this post I'll talk about the PR #437.\nThere are several reasons to have a streaming system for data visualization. Because I\u0026rsquo;m doing a PhD in a developing country I always need to think of the cheapest way to use the computational resources available.","tags":["gsoc","python","open-source","data-viz","webrtc"],"title":"GSoC-  A Stadia-like system for data visualization","type":"post"},{"authors":["Bruno Messias {F. de Resende}","Luciano da {F. Costa}"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"pt-br","lastmod":1577836800,"objectID":"fa6c6f0e355b3cd839405544f5cc3f49","permalink":"/pt-br/publication/2020-01-01_chaos2019/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/pt-br/publication/2020-01-01_chaos2019/","section":"publication","summary":"","tags":[],"title":"Characterization and comparison of large directed networks through the spectra of the magnetic Laplacian","type":"publication"},{"authors":["Thomas Peron","Bruno {Messias F. de Resende}","Angelica S. Mata","Francisco A. Rodrigues","Yamir Moreno"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"pt-br","lastmod":1546300800,"objectID":"78143e923f622c57af2cfd671f6004b7","permalink":"/pt-br/publication/2019-01-01_pre_2019/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/pt-br/publication/2019-01-01_pre_2019/","section":"publication","summary":"","tags":[],"title":"Onset of synchronization of Kuramoto oscillators in scale-free networks","type":"publication"},{"authors":null,"categories":null,"content":"Bibcure helps in boring tasks by keeping your bibfile up to date and normalized...also allows you to easily download all papers inside your bibtex  Requirements/Install $ sudo python /usr/bin/pip install bibcure  scihub2pdf(beta) If you want download articles via a DOI number, article title or a bibtex file, using the database of arxiv, libgen or sci-hub, see\n bibcure/scihub2pdf\nFeatures and how to use bibcure Given a bib file\u0026hellip;\n$ bibcure -i input.bib -o output.bib    check sure the Arxiv items have been published, then update them(requires internet connection)\n  complete all fields(url, journal, etc) of all bib items using DOI number(requires internet connection)\n  find and create DOI number associated with each bib item which has not DOI field(requires internet connection)\n  abbreviate jorunals names\n  arxivcheck Given a arxiv id\u0026hellip;\n$ arxivcheck 1601.02785   check if has been published, and then returns the updated bib (requires internet connection)  Given a title\u0026hellip;\n$ arxivcheck --title An useful paper published on arxiv  search papers related and return a bib the first item. You can easily append a bib into a bibfile, just do\n$ arxivcheck --title An useful paper published on arxiv \u0026gt;\u0026gt; file.bib  You also can interact with results, just pass \u0026ndash;ask parameter\n$ arxivcheck --ask --title An useful paper published on arxiv  scihub2pdf Given a bibtex file\n$ scihub2pdf -i input.bib  Given a DOI number\u0026hellip;\n$ scihub2pdf 10.1038/s41524-017-0032-0  Given arxivId\u0026hellip;\n$ scihub2pdf arxiv:1708.06891  Given a title\u0026hellip;\n$ scihub2bib --title An useful paper  or arxiv\u0026hellip;\n$ scihub2bib --title arxiv:An useful paper  Location folder as argument\n$ scihub2pdf -i input.bib -l somefoler/  Use libgen instead sci-hub\n$ scihub2pdf -i input.bib --uselibgen  doi2bib Given a DOI number\u0026hellip;\n$ doi2bib 10.1038/s41524-017-0032-0   get bib item given a doi(requires internet connection)  You can easily append a bib into a bibfile, just do\n$ doi2bib 10.1038/s41524-017-0032-0 \u0026gt;\u0026gt; file.bib  You also can generate a bibtex from a txt file containing a list of DOIs\n$ doi2bib --input file_with_dois.txt --output refs.bib  title2bib Given a title\u0026hellip;\n$ title2bib An useful paper   search papers related and return a bib for the selected paper(requires internet connection)  You can easily append a bib into a bibfile, just do\n$ title2bib An useful paper --first \u0026gt;\u0026gt; file.bib  You also can generate a bibtex from a txt file containing a list of \u0026ldquo;titles\u0026rdquo;\n$ title2bib --input file_with_titles.txt --output refs.bib --first  Sci-Hub vs LibGen Sci-hub:  Stable Annoying CAPTCHA Fast  Libgen  Unstalbe No CAPTCHA Slow  ","date":1505324602,"expirydate":-62135596800,"kind":"page","lang":"pt-br","lastmod":1505324602,"objectID":"5103dd7eacb3c3f954b0ad79984f81a6","permalink":"/pt-br/project/bibcure/","publishdate":"2017-09-13T17:43:22Z","relpermalink":"/pt-br/project/bibcure/","section":"project","summary":"Bibcure helps in boring tasks by keeping your bibfile up to date and normalized...also allows you to easily download all papers inside your bibtex","tags":["latex","bibtex","scihub","python"],"title":"Bibcure","type":"project"},{"authors":["B. {Messias de Resende}","F. Crasto {de Lima}","R. H. Miwa","E. Vernek","G. J. Ferreira"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"pt-br","lastmod":1483228800,"objectID":"86d70a8558fcdf1ec7abbf25d2534a80","permalink":"/pt-br/publication/2017-01-01_prb_2017/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/pt-br/publication/2017-01-01_prb_2017/","section":"publication","summary":"","tags":[],"title":"Confinement and fermion doubling problem in Dirac-like Hamiltonians","type":"publication"},{"authors":[],"categories":[],"content":"","date":1471271380,"expirydate":-62135596800,"kind":"page","lang":"pt-br","lastmod":1471271380,"objectID":"c1e8be5e3161f44b3cefc541fa1fdea8","permalink":"/pt-br/project/python_triangulo/","publishdate":"2016-08-15T11:29:40-03:00","relpermalink":"/pt-br/project/python_triangulo/","section":"project","summary":"A comunidade python tri√¢ngulo foi criada em 2016 com o intuito de promover a divulga√ß√£o da linguagem python e o uso de software-livre em Uberl√¢ndia-MG e regi√£o","tags":["python","code","software-livre","free-software"],"title":"Python Tri√¢ngulo","type":"project"},{"authors":[],"categories":[],"content":" O novo sistema de peri√≥dicos da Capes (assim como o CAFe) reduz drasticamente a agilidade de busca e acesso a refer√™ncias bibliogr√°ficas, pois √© necess√°rio que os usu√°rios acessem o portal de buscas da Capes. O que acaba inutilizando o acesso a publica√ß√µes via links diretos, links internos dos arquivos PDF e sistemas como o DOI.\nPor isso o Prof. Gerson Ferreira e esse que vos fala desenvolveram uma ferramenta que facilita a vida de nos, j√° aterefados cientistas. A extens√£o est√° dispon√≠vel tanto para firefox quanto para chrome\n Link para Chrome\n Link para Firefox\nEstamos recebendo um excelente feedback da comunidade ci√™ntifica, com um total de mais de 5 mil usu√°rios(firefox \u0026amp; chrome)\n Como n√£o fazemos a parte do time de TI da CAPES n√£o possu√≠mos a lista oficial, e ainda n√£o obtivemos resposta do time de TI da CAPES\n    ","date":1468851944,"expirydate":-62135596800,"kind":"page","lang":"pt-br","lastmod":1468851944,"objectID":"b31b5730bf55af6a59998ce2471bc3df","permalink":"/pt-br/project/capes/","publishdate":"2016-07-18T14:25:44Z","relpermalink":"/pt-br/project/capes/","section":"project","summary":"Extens√£o que permite ganhar agilidade ao utilizar o novo sistema de peri√≥dicos CAPES","tags":["open-science","free-software"],"title":"Projeto Capes - 5K cientistas utilizando","type":"project"},{"authors":[],"categories":["inclus√£o digital","social"],"content":"O Recicla Aqui √© um projeto que desenvolvo em parceria com um amigo(R√©gis Maicon). Tal projeto visa inserir tecnologia no processo da coleta seletiva e aumentar a qualidade de vida dos catadores de material recicl√°vel.\nTemos tr√™s eixos norteadores:\nFornecer tecnologias de Gerenciamento e Forma√ß√£o t√©cnica para as Cooperativas Cria√ß√£o de um aplicativo que permita conectar Cidad√£es e Coletores \u0026ldquo;Revitaliza√ß√£o\u0026rdquo; da imagem dos coletores \u0026ldquo;Alfabetiza√ß√£o Tecnol√≥gica\u0026rdquo; O aplicativo voc√™ encontra na Goolge Play (futuramente na Apple Store)\nObjetivamos utilizar as redes sociais para conscientizar a popula√ß√£o da import√¢ncia dos catadores\n","date":1464691556,"expirydate":-62135596800,"kind":"page","lang":"pt-br","lastmod":1464691556,"objectID":"84ab7124f98afcddc54e5951da01bbae","permalink":"/pt-br/project/reciclaaqui/","publishdate":"2016-05-31T10:45:56Z","relpermalink":"/pt-br/project/reciclaaqui/","section":"project","summary":"O Recicla Aqui √© um projeto que foi desenvolvido em parceria com um amigo(R√©gis Maicon) e o CIAEM-UFU para alfabetiza√ß√£o tecnol√≥gica e aprimoramento de processos das cooperativas em Uberl√¢ndia-MG","tags":["python","django","social","inclus√£o digital","react"],"title":"ReciclaAqui","type":"project"},{"authors":["B. M. F. {de Resende}","E. Vernek"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"pt-br","lastmod":1325376000,"objectID":"2264bab5383f449a27d3a6c3eaf85fc6","permalink":"/pt-br/publication/2012-01-01_apl_2012/","publishdate":"2012-01-01T00:00:00Z","relpermalink":"/pt-br/publication/2012-01-01_apl_2012/","section":"publication","summary":"","tags":[],"title":"Parity oscillations of Kondo temperature in a single molecule break junction","type":"publication"},{"authors":null,"categories":["Data Analysis"],"content":"eMaTe it is a python package which the main goal is to provide methods capable of estimating the spectral densities and trace functions of large sparse matrices. eMaTe can run in both CPU and GPU and can estimate the spectral density and related trace functions, such as entropy and Estrada index, even in directed or undirected networks with million of nodes.\nInstall pip install emate  If you a have a GPU you should also install cupy.\nKernel Polynomial Method (KPM) The Kernel Polynomial Method can¬†estimate the spectral density of large sparse Hermitan matrices with a computational cost almost linear. This method combines three key ingredients: the Chebyshev expansion + the stochastic trace estimator + kernel smoothing.\nExample import networkx as nx import numpy as np n = 3000 g = nx.erdos_renyi_graph(n , 3/n) W = nx.adjacency_matrix(g) vals = np.linalg.eigvals(W.todense()).real  from emate.hermitian import tfkpm num_moments = 40 num_vecs = 40 extra_points = 10 ek, rho = tfkpm(W, num_moments, num_vecs, extra_points)  import matplotlib.pyplot as plt plt.hist(vals, density=True, bins=100, alpha=.9, color=\u0026quot;steelblue\u0026quot;) plt.scatter(ek, rho, c=\u0026quot;tomato\u0026quot;, zorder=999, alpha=0.9, marker=\u0026quot;d\u0026quot;)  If the CUPY package it is available in your machine, you can also use the cupy implementation. When compared to tf-kpm, the Cupy-kpm is slower for median matrices (100k) and faster for larger matrices (\u0026gt; 10^6). The main reason it\u0026rsquo;s because the tf-kpm was implemented in order to calc all te moments in a single step.\nimport matplotlib.pyplot as plt from emate.hermitian import cupykpm num_moments = 40 num_vecs = 40 extra_points = 10 ek, rho = cupykpm(W.tocsr(), num_moments, num_vecs, extra_points) plt.hist(vals, density=True, bins=100, alpha=.9, color=\u0026quot;steelblue\u0026quot;) plt.scatter(ek.get(), rho.get(), c=\u0026quot;tomato\u0026quot;, zorder=999, alpha=0.9, marker=\u0026quot;d\u0026quot;)  Stochastic Lanczos Quadrature (SLQ)  The problem of estimating the trace of matrix functions appears in applications ranging from machine learning and scientific computing, to computational biology.[2]\n Example Computing the Estrada index from emate.symmetric.slq import pyslq import tensorflow as tf def trace_function(eig_vals): return tf.exp(eig_vals) num_vecs = 100 num_steps = 50 approximated_estrada_index, _ = pyslq(L_sparse, num_vecs, num_steps, trace_function) exact_estrada_index = np.sum(np.exp(vals_laplacian)) approximated_estrada_index, exact_estrada_index  The above code returns\n(3058.012, 3063.16457163222)  Entropy import scipy import scipy.sparse def entropy(eig_vals): s = 0. for val in eig_vals: if val \u0026gt; 0: s += -val*np.log(val) return s L = np.array(G.laplacian(normalized=True), dtype=np.float64) vals_laplacian = np.linalg.eigvalsh(L).real exact_entropy = entropy(vals_laplacian) def trace_function(eig_vals): def entropy(val): return tf.cond(val\u0026gt;0, lambda:-val*tf.log(val), lambda: 0.) return tf.map_fn(entropy, eig_vals) L_sparse = scipy.sparse.coo_matrix(L) num_vecs = 100 num_steps = 50 approximated_entropy, _ = pyslq(L_sparse, num_vecs, num_steps, trace_function) approximated_entropy, exact_entropy  (-509.46283, -512.5283224633046)   [1] Hutchinson, M. F. (1990). A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines. Communications in Statistics-Simulation and Computation, 19(2), 433-450.\n [2] Ubaru, S., Chen, J., \u0026amp; Saad, Y. (2017). Fast Estimation of tr(f(A)) via Stochastic Lanczos Quadrature. SIAM Journal on Matrix Analysis and Applications, 38(4), 1075-1099.\n [3] The Kernel Polynomial Method applied to tight binding systems with time-dependence\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"pt-br","lastmod":-62135596800,"objectID":"08099961c7d99d15f06ef7f9c03942f1","permalink":"/pt-br/project/emate/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/pt-br/project/emate/","section":"project","summary":"eMaTe is a python package which can estimate spectral propreties of very large matrices and networks","tags":["graphs","python","CUDA","Complex Networks","open-software","spectral analysis","eigenvalues","autovalores"],"title":"eMaTe","type":"project"}]