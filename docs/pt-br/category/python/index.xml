<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Python | Bruno Messias</title><link>/pt-br/category/python/</link><atom:link href="/pt-br/category/python/index.xml" rel="self" type="application/rss+xml"/><description>Python</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>pt-br</language><copyright>Bruno Messias</copyright><lastBuildDate>Mon, 18 Apr 2022 00:00:00 +0000</lastBuildDate><image><url>/images/icon_hucd6a3d413e7b81060a1d462b35f64cf9_5018_512x512_fill_lanczos_center_3.png</url><title>Python</title><link>/pt-br/category/python/</link></image><item><title>Grafos e modelo de blocos aninhados para matrizes de correlação: clusterização do mercado de ações</title><link>/pt-br/post/nsbm_sp500_stock_market_disparity_filter/</link><pubDate>Mon, 18 Apr 2022 00:00:00 +0000</pubDate><guid>/pt-br/post/nsbm_sp500_stock_market_disparity_filter/</guid><description>&lt;details
class="toc-inpage d-print-none d-none d-sm-block d-md-none " open>
&lt;summary class="font-weight-bold">Lista de Conteúdos&lt;/summary>
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>
&lt;ul>
&lt;li>&lt;a href="#introdução">Introdução&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#grafos">Grafos&lt;/a>&lt;/li>
&lt;li>&lt;a href="#matrizes-de-correlação">Matrizes de correlação&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#baixando-e-criando-nosso-grafo">Baixando e criando nosso grafo&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#extraindo-o-preço-de-fechamento">Extraindo o preço de fechamento&lt;/a>&lt;/li>
&lt;li>&lt;a href="#retorno-e-matrizes-de-correlação">Retorno e matrizes de correlação&lt;/a>&lt;/li>
&lt;li>&lt;a href="#criando-o-grafo-completo-e-filtrando">Criando o grafo completo e filtrando&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#nsbm-buscando-hierarquia-e-comunidades">nSBM: buscando hierarquia e comunidades&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#convertendo-o-igraph-em-graph-tool">Convertendo o iGraph em graph-tool&lt;/a>&lt;/li>
&lt;li>&lt;a href="#inferência-dos-blocos">Inferência dos blocos&lt;/a>&lt;/li>
&lt;li>&lt;a href="#como-analisar">Como analisar?&lt;/a>&lt;/li>
&lt;li>&lt;a href="#outras-aplicações-de-nsbm">Outras aplicações de nSBM&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#extras-mst">Extras: MST&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#convertendo-correlações-em-distâncias">Convertendo correlações em distâncias&lt;/a>&lt;/li>
&lt;li>&lt;a href="#extraindo-o-mst">Extraindo o MST&lt;/a>&lt;/li>
&lt;li>&lt;a href="#visualizando-o-mst">Visualizando o MST&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#agradecimentos">Agradecimentos&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/details>
&lt;p>Esse post é da série sobre filtragem em grafos (esparsificação). O post anterior pode ser acessado em:
&lt;a href="/pt-br/post/edge_graph_filtering/" title="Grafos e filtragem de arestas I: conceitos e confusões.">Grafos e filtragem de arestas: conceitos e confusões.&lt;/a>&lt;/p>
&lt;p>O objetivo é mostrar como usar o modelo de bloco estocástico aninhado (nSBM) para o processo de análise exploratória do mercado de ações. O nSBM e SBM são modelos não-paramétricos estabelecidos numa sólida base estatística. Vou te ensinar na prática como usar eles no python e como analisar os outputs, que a primeira vista podem parecer artísticos ou complexos. Veja só:&lt;/p>
&lt;figure >
&lt;a data-fancybox="" href="/pt-br/post/nsbm_sp500_stock_market_disparity_filter/nsbm_final_2018-01-01_2018-06-01_hudcff6362c16284ef400b42c5fe2e1b69_266080_0x500_resize_lanczos_3.png" >
&lt;img data-src="/pt-br/post/nsbm_sp500_stock_market_disparity_filter/nsbm_final_2018-01-01_2018-06-01_hudcff6362c16284ef400b42c5fe2e1b69_266080_0x500_resize_lanczos_3.png" class="lazyload" alt="" width="100%" height="500px">
&lt;/a>
&lt;/figure>
&lt;p>A ordem que seguiremos nesse post é:&lt;/p>
&lt;ol>
&lt;li>Uma introdução meio longa para te situar em grafos e o porquê usar eles aqui.&lt;/li>
&lt;li>Código
&lt;ol>
&lt;li>Construção da matriz de correlação entre os retornos dos ativos&lt;/li>
&lt;li>Filtragem da matriz de correlação via um filtro de grafos&lt;/li>
&lt;li>Inferência e visualização do nSBM&lt;/li>
&lt;/ol>
&lt;/li>
&lt;li>Como analisar o nSBM?&lt;/li>
&lt;li>Extra: MST&lt;/li>
&lt;/ol>
&lt;p>Para reproduzir esse post recomendo usar um ambiente conda, pois uma das bibliotecas depende de diversas coisas além de libs usais do python&lt;/p>
&lt;p>Comece checando se você tem as seguintes bibliotecas instaladas&lt;/p>
&lt;pre>&lt;code>matplotlib, pandas, yfinance
&lt;/code>&lt;/pre>
&lt;p>Instale o igraph com&lt;/p>
&lt;pre>&lt;code>$ pip install python-igraph
&lt;/code>&lt;/pre>
&lt;p>O graph-tool, do excelente
&lt;a href="https://twitter.com/tiagopeixoto" target="_blank" rel="noopener">Tiago Peixoto&lt;/a> via conda-forge&lt;/p>
&lt;pre>&lt;code>$ conda install -c conda-forge graph-tool
&lt;/code>&lt;/pre>
&lt;p>Não menos importante, você precisa instalar minha biblioteca de filtragem de grafos, o &lt;code>edgeseraser&lt;/code> deixe seu star
&lt;a href="https://github.com/devmessias/edgeseraser" target="_blank" rel="noopener">aqui&lt;/a> :).&lt;/p>
&lt;pre>&lt;code>$ pip install edgeseraser
&lt;/code>&lt;/pre>
&lt;h2 id="introdução">Introdução&lt;/h2>
&lt;p>Análise exploratória é usada tanto como o objetivo final em si como uma ferramenta que fornece subsídios para melhores tomadas de decisões para escolha de modelos preditivos ou pré-seleção de instâncias para serem analisadas com mais detalhes.&lt;/p>
&lt;p>Contudo, muitas das técnicas exploradas e ensinadas na web se restringem àquelas que podem ser empregadas quando o conjunto de dados vive em algum espaço organizado (como o $\mathbb R^n$) e cujos dados não têm relação entre si. Um conjunto de pontos. Mas e os dados que não se enquadram nisso?&lt;/p>
&lt;p>Um exemplo de conjunto de dados extremamente complicado são as redes sociais. Redes sociais são conjuntos de pessoas e a existência de pelo menos relações pares a pares (hyper-grafos é um assunto para outro post) podendo ser negativas, positivas ou algo mais complicado. Cada pessoa em uma rede social pode ser identificada por um conjunto de features tais como gostos pessoais, horário de uso do sistema, etc. Representar uma rede social por pontinhos é reducionista. É para isso que grafos podem ser empregados&lt;/p>
&lt;h3 id="grafos">Grafos&lt;/h3>
&lt;p>Um grafo armazena objetos que têm relações pares a pares entre si. Sendo possível associar a cada objeto ou relação um outro tipo de dado genérico tais como um número real, um vetor ou mesmo outro grafo. Mas é importante ressaltar que grafos estão em todo lugar, por exemplo em matrizes de correlação. Portanto, usar grafos para analisar correlações é válido, especialmente quando muitas dessas correlações podem ou queremos que sejam descartadas.&lt;/p>
&lt;h3 id="matrizes-de-correlação">Matrizes de correlação&lt;/h3>
&lt;p>No &lt;em>OpenCode&lt;/em> matrizes de correlação já apareceram em diversos posts:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>
&lt;a href="https://opencodecom.net/post/2021-12-14-variacoes-do-teorema-central-do-limite-para-matrizes-aleatorias-de-nucleos-atomicos-a-filtragem-de-matrizes-de-correlaca/" target="_blank" rel="noopener">Variações do teorema central do limite para matrizes aleatórias: de núcleos atômicos a filtragem de matrizes de correlação para construção de carteiras&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>
&lt;a href="https://opencodecom.net/post/2021-09-01-correlacao-entre-ativos-no-python/" target="_blank" rel="noopener">Correlação entre Ativos no Python&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Mas o que é uma matriz de correlação se não um conjunto de relações pares a pares com valores reais? Bom, então a questão aqui fica evidente: &lt;strong>Uma matriz de correlação pode ser analisada usando ferramentas feitas para analisar grafos! Ok, isso pode ser feito, mas você pode se perguntar o porquê de fazer isso.&lt;/strong>&lt;/p>
&lt;p>Uma atividade muito comum quando exploramos matrizes de correlação é tentar encontrar grupos de elementos fortemente/fracamente correlacionados, isso não é uma tarefa trivial à medida que o número de elementos aumenta. Além disso, é comum jogarmos fora as relações que são muito fracas. Quando fazemos isso estamos esparsificando a matriz, na terminologia de grafos estamos filtrando arestas!
&lt;a href="/pt-br/post/edge_graph_filtering/#estrutural-threshold">No post anterior eu discuti o porquê disso poder ser bem perigoso.&lt;/a>.&lt;/p>
&lt;p>Uma maneira mais elaborada de se analisar matrizes de correlação é através da construção de
&lt;a href="https://www.youtube.com/watch?v=jMioOe2eTcY" target="_blank" rel="noopener">árvores de expansão mínima (MST)&lt;/a>, apesar do nome complicado é um processo bem simples de construir um grafo e você pode encontrar diversos tutoriais sobre MST e o mercado de ações na internet.&lt;/p>
&lt;p>Devido a tutoriais com MST estarem já espalhados, decidi fazer algo diferente aqui e propor usar um método pouco conhecido para exploração de grafos e aplicar ele em matrizes de correlação de ativos. Esse método é conhecido pela sigla &lt;em>nSBM&lt;/em>, modelo de bloco estocástico aninhado (nested Stochastic Block Model) e é um método não-paramétrico para inferência de comunidades em grafos que permite analisar a hierarquia de comunidades.&lt;/p>
&lt;div class="alert alert-">
&lt;div>
No final do post vou mostrar a mesma matriz analisada pelo MST só para você ter uma ideia do porquê o nSBM ser bem mais interessante.
&lt;/div>
&lt;/div>
&lt;p>Uma das grandes qualidades dos SBM e variantes é que eles são construídos em cima de um arcabouço estatístico rigoroso e ao mesmo tempo é possível detectar comunidades com pouquíssimos vértices. Isso é ótimo, pois duas coisas que não queremos é que o método que escolhamos diga que certas coisas formam comunidades mesmo que não passe de um amontoado de coisas aleatórias e que ele bote coisas onde não devia só porque são pequenas demais, &lt;strong>isso é uma crítica aos métodos de detecção por maximização de modularidade&lt;/strong>&lt;/p>
&lt;h2 id="baixando-e-criando-nosso-grafo">Baixando e criando nosso grafo&lt;/h2>
&lt;h3 id="extraindo-o-preço-de-fechamento">Extraindo o preço de fechamento&lt;/h3>
&lt;p>Vamos começar importando o que for necessário&lt;/p>
&lt;pre>&lt;code class="language-python">import yfinance as yf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import igraph as ig
from edgeseraser.disparity import filter_ig_graph
mpl.rcParams.update(_VSCode_defaultMatplotlib_Params)
plt.style.context('classic')
mpl.rcParams['figure.facecolor'] = 'white'
&lt;/code>&lt;/pre>
&lt;p>Usaremos uma tabela contendo os simbolos de um conjunto de ativos e os setores. O csv tem a seguinte organização, e está disponível
&lt;a href="https://raw.githubusercontent.com/datasets/s-and-p-500-companies/master/data/constituents.csv" target="_blank" rel="noopener">aqui&lt;/a>.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Symbol&lt;/th>
&lt;th>Name&lt;/th>
&lt;th>Sector&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>MMM&lt;/td>
&lt;td>3M&lt;/td>
&lt;td>Industrials&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>AOS&lt;/td>
&lt;td>A. O. Smith&lt;/td>
&lt;td>Industrials&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ABT&lt;/td>
&lt;td>Abbott Laboratories&lt;/td>
&lt;td>Health Care&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>ABBV&lt;/td>
&lt;td>AbbVie&lt;/td>
&lt;td>Health Care&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;pre>&lt;code class="language-python">!wget https://raw.githubusercontent.com/datasets/s-and-p-500-companies/master/data/constituents.csv
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">df = pd.read_csv(&amp;quot;constituents.csv&amp;quot;)
all_symbols = df['Symbol'].values
all_sectors = df['Sector'].values
all_names = df['Name'].values
# Criaremos um dicionário para mapear um simbolo para seu
# setor e uma cor
symbol2sector = dict(zip(all_symbols, all_sectors))
symbol2name = dict(zip(all_symbols, all_names))
&lt;/code>&lt;/pre>
&lt;p>Hora de baixar as informações sobre os ativos. Iremos computar as correlações numa janela de um semestre.&lt;/p>
&lt;pre>&lt;code class="language-python">start_date = '2018-01-01'
end_date = '2018-06-01'
try:
prices = pd.read_csv(
f&amp;quot;sp500_prices_{start_date}_{end_date}.csv&amp;quot;, index_col=&amp;quot;Date&amp;quot;)
tickers_available = prices.columns.values
except FileNotFoundError:
df = yf.download(
list(all_symbols),
start=start_date,
end=end_date,
interval=&amp;quot;1d&amp;quot;,
group_by='ticker',
progress=True
)
tickers_available = list(
set([ticket for ticket, _ in df.columns.T.to_numpy()]))
prices = pd.DataFrame.from_dict(
{
ticker: df[ticker][&amp;quot;Adj Close&amp;quot;].to_numpy()
for ticker in tickers_available
}
)
prices.index = df.index
prices = prices.iloc[:-1]
del df
prices.to_csv(
f&amp;quot;sp500_prices_{start_date}_{end_date}.csv&amp;quot;)
&lt;/code>&lt;/pre>
&lt;h3 id="retorno-e-matrizes-de-correlação">Retorno e matrizes de correlação&lt;/h3>
&lt;p>A correlação será calculada para todos os ativos considerando o retorno. O retorno que estamos calculando aqui é simplesmente a mudança percentual do preço de fechamento do ativo.&lt;/p>
&lt;pre>&lt;code class="language-python">returns_all = prices.pct_change()
# a primeira linha não faz sentido, não existe retorno no primeiro dia
returns_all = returns_all.iloc[1:, :]
returns_all.dropna(axis=1, thresh=len(returns_all.index)//2., inplace=True)
returns_all.dropna(axis=0, inplace=True)
symbols = returns_all.columns.values
&lt;/code>&lt;/pre>
&lt;p>Para calcular a correlação é fácil&lt;/p>
&lt;pre>&lt;code class="language-python"># plot the correlation matrix with ticks at each item
correlation_matrix = returns_all.corr()
plt.title(f&amp;quot;Correlation matrix from {start_date} to {end_date}&amp;quot;)
plt.imshow(correlation_matrix)
plt.colorbar()
plt.savefig(&amp;quot;correlation.png&amp;quot;, dpi=150)
plt.clf()
&lt;/code>&lt;/pre>
&lt;figure id="figure-matriz-de-correlação-entre-ativos-do-sp500-para-o-primeiro-semestre-de-2018-sim-uma-bagunça">
&lt;a data-fancybox="" href="/pt-br/post/nsbm_sp500_stock_market_disparity_filter/correlation_hu6c68fc9d1aa2b5f7307017b57ffa4d90_521823_0x500_resize_lanczos_3.png" data-caption="Matriz de correlação entre ativos do s&amp;amp;amp;p500 para o primeiro semestre de 2018. Sim, uma bagunça!">
&lt;img data-src="/pt-br/post/nsbm_sp500_stock_market_disparity_filter/correlation_hu6c68fc9d1aa2b5f7307017b57ffa4d90_521823_0x500_resize_lanczos_3.png" class="lazyload" alt="" width="100%" height="500px">
&lt;/a>
&lt;figcaption>
Matriz de correlação entre ativos do s&amp;amp;p500 para o primeiro semestre de 2018. Sim, uma bagunça!
&lt;/figcaption>
&lt;/figure>
&lt;p>Ok, você seria louco de analisar essa matriz manualmente. Vamos partir para o motivo desse post que é usar nSBM.&lt;/p>
&lt;h3 id="criando-o-grafo-completo-e-filtrando">Criando o grafo completo e filtrando&lt;/h3>
&lt;p>Como queremos explorar as comunidades usaremos apenas as correlações positivas,&lt;/p>
&lt;pre>&lt;code class="language-python">pos_correlation = correlation_matrix.copy()
# vamos considerar apenas as correlações positivas pois queremos
# apenas as comunidades
pos_correlation[pos_correlation &amp;lt; 0.] = 0
# diagonal principal é setada a 0 para evitar auto-arestas
np.fill_diagonal(pos_correlation.values, 0)
&lt;/code>&lt;/pre>
&lt;p>Agora basta construir o grafo não direcionado associando os pesos das arestas com a correlação entre os ativos.&lt;/p>
&lt;pre>&lt;code class="language-python">g = ig.Graph.Weighted_Adjacency(pos_correlation.values, mode='undirected')
# criamos uma feature symbol para cada vértice
g.vs[&amp;quot;symbol&amp;quot;] = returns_all.columns
# o grafo pode estar desconectado. Portanto, extraímos a componente gigante
cl = g.clusters()
g = cl.giant()
n_edges_before = g.ecount()
&lt;/code>&lt;/pre>
&lt;p>Agora iremos aplicar o
&lt;a href="/pt-br/post/edge_graph_filtering/#estatistico" title="Grafos e filtragem de arestas I: conceitos e confusões. Filtro estatístico">filtro de disparidade&lt;/a>
do edgeseraser para remover as arestas que não são significativas&lt;/p>
&lt;pre>&lt;code class="language-python">_ = filter_ig_graph(g, .25, cond=&amp;quot;both&amp;quot;, field=&amp;quot;weight&amp;quot;)
cl = g.clusters()
g = cl.giant()
n_edges_after = g.ecount()
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">print(f&amp;quot;Percentage of edges removed: {(n_edges_before - n_edges_after)/n_edges_before*100:.2f}%&amp;quot;)
print(f&amp;quot;Number of remained stocks: {len(symbols)}&amp;quot;)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>Percentage of edges removed: 95.76%
Number of remained stocks: 492
&lt;/code>&lt;/pre>
&lt;p>A maior parte das arestas foi removida. Será que conseguimos fazer algo com esse grafo compactado?&lt;/p>
&lt;h2 id="nsbm-buscando-hierarquia-e-comunidades">nSBM: buscando hierarquia e comunidades&lt;/h2>
&lt;h3 id="convertendo-o-igraph-em-graph-tool">Convertendo o iGraph em graph-tool&lt;/h3>
&lt;p>O graph-tool é um pacote com excelente desempenho, mas para ganhar essa performance ele
exige um pouquinho mais de trabalho tais como declarar o tipo dos dados. O primeiro
passo para usar o graph-tool é converter nosso grafo iGraph para uma instância dele&lt;/p>
&lt;pre>&lt;code class="language-python">import graph_tool.all as gt
gnsbm = gt.Graph(directed=False)
# iremos adicionar os vértices
for v in g.vs:
gnsbm.add_vertex()
# e as arestas
for e in g.es:
gnsbm.add_edge(e.source, e.target)
&lt;/code>&lt;/pre>
&lt;h3 id="inferência-dos-blocos">Inferência dos blocos&lt;/h3>
&lt;p>Com o grafo construído iremos executar o algoritmo de inferência de blocos.
Esse algoritmo executa uma minimização do que é conhecido como &lt;em>&amp;ldquo;description length&amp;rdquo;&lt;/em> do modelo Bayesiano. Em um post futuro falarei um pouco sobre a matemática se você se já estiver interessado dê uma olhada no artigo original do Tiago Peixoto
&lt;a href="https://dx.doi.org/10.1103/PhysRevX.4.011047" target="_blank" rel="noopener">aqui&lt;/a>.&lt;/p>
&lt;pre>&lt;code class="language-python">state = gt.minimize_nested_blockmodel_dl(gnsbm)
&lt;/code>&lt;/pre>
&lt;p>O código abaixo é só para gerar as cores para nosso plot&lt;/p>
&lt;pre>&lt;code class="language-python">symbols = g.vs[&amp;quot;symbol&amp;quot;]
sectors = [symbol2sector[symbol] for symbol in symbols]
u_sectors = np.sort(np.unique(sectors))
u_colors = [plt.cm.tab10(i/len(u_sectors))
for i in range(len(u_sectors))]
# a primeira cor da lista era muito similar a segunda,
u_colors[0] = [0, 1, 0, 1]
sector2color = {sector: color for sector, color in zip(u_sectors, u_colors)}
rgba = gnsbm.new_vertex_property(&amp;quot;vector&amp;lt;double&amp;gt;&amp;quot;)
gnsbm.vertex_properties['rgba'] = rgba
for i, symbol in enumerate(symbols):
c = sector2color[symbol2sector[symbol]]
rgba[i] = [c[0], c[1], c[2], .5]
&lt;/code>&lt;/pre>
&lt;p>Executaremos o método draw para gerar o plot. O parâmetro que talvez você queira brincar um pouco é o $\beta \in (0, 1)$. Tal parâmetro é responsável pela força do &lt;strong>edge-bundling&lt;/strong>, ou seja, a força com que as arestas serão atraídas uma à outra. Este parâmetro tem finalidades apenas para facilitar a visualização, não existe nenhuma relação com o nSBM.&lt;/p>
&lt;pre>&lt;code class="language-python">options = {
'output': f'nsbm_{start_date}_{end_date}.png',
'beta': .9,
'bg_color': 'w',
#'output_size': (1500, 1500),
'vertex_color': gnsbm.vertex_properties['rgba'],
'vertex_fill_color': gnsbm.vertex_properties['rgba'],
'hedge_pen_width': 2,
'hvertex_fill_color': np.array([0., 0., 0., .5]),
'hedge_color': np.array([0., 0., 0., .5]),
'hedge_marker_size': 20,
'hvertex_size':20
}
state.draw(**options)
&lt;/code>&lt;/pre>
&lt;p>Finalmente, agora é só ver o resultado da nossa filtragem e inferência&lt;/p>
&lt;pre>&lt;code class="language-python">plt.figure(dpi=150)
plt.title(f&amp;quot;Sectors of the S&amp;amp;P 500 from {start_date} to {end_date}&amp;quot;)
legend = plt.legend(
[plt.Line2D([0], [0], color=c, lw=10)
for c in list(sector2color.values())],
list(sector2color.keys()),
bbox_to_anchor=(1.05, 1),
loc=2,
borderaxespad=0.)
plt.imshow(plt.imread(f'nsbm_{start_date}_{end_date}.png'))
plt.xticks([])
plt.yticks([])
plt.axis('off')
plt.savefig(f'nsbm_final_{start_date}_{end_date}.png', bbox_inches='tight',
dpi=150, bbox_extra_artists=(legend,), facecolor='w', edgecolor='w')
plt.show()
&lt;/code>&lt;/pre>
&lt;figure id="figure-resultado-do-modelo-de-blocos-aninhados-para-o-primeiro-semestre-de-2018-de-ativos-do-sp500-artístico">
&lt;a data-fancybox="" href="/pt-br/post/nsbm_sp500_stock_market_disparity_filter/nsbm_final_2018-01-01_2018-06-01_hudcff6362c16284ef400b42c5fe2e1b69_266080_0x500_resize_lanczos_3.png" data-caption="Resultado do modelo de blocos aninhados para o primeiro semestre de 2018 de ativos do s&amp;amp;amp;p500. Artístico?">
&lt;img data-src="/pt-br/post/nsbm_sp500_stock_market_disparity_filter/nsbm_final_2018-01-01_2018-06-01_hudcff6362c16284ef400b42c5fe2e1b69_266080_0x500_resize_lanczos_3.png" class="lazyload" alt="" width="100%" height="500px">
&lt;/a>
&lt;figcaption>
Resultado do modelo de blocos aninhados para o primeiro semestre de 2018 de ativos do s&amp;amp;p500. Artístico?
&lt;/figcaption>
&lt;/figure>
&lt;p>Ok, muito bonito! Conseguimos ver agrupamentos de certos setores, algumas misturas, muitas conexões entre o &lt;em>Financials&lt;/em> e &lt;em>Industrials&lt;/em>, etc. Se você não consegue ver isso agora vou tentar te explicar como interpretar esse gráfico.&lt;/p>
&lt;h3 id="como-analisar">Como analisar?&lt;/h3>
&lt;figure >
&lt;a data-fancybox="" href="/pt-br/post/nsbm_sp500_stock_market_disparity_filter/descripition_nsbm_sp500_hufe6166112f10aa58541f55447fe95bbb_1458779_0x500_resize_lanczos_3.png" >
&lt;img data-src="/pt-br/post/nsbm_sp500_stock_market_disparity_filter/descripition_nsbm_sp500_hufe6166112f10aa58541f55447fe95bbb_1458779_0x500_resize_lanczos_3.png" class="lazyload" alt="" width="100%" height="500px">
&lt;/a>
&lt;/figure>
&lt;ul>
&lt;li>Cada círculo no conjunto que parece a escova de uma vassoura é um ativo, um vértice do grafo original.&lt;/li>
&lt;li>Cada escova é uma comunidade de ativos. Podemos navegar na hierarquia seguindo o caminho reverso apontado pelas setinhas no grafo em preto. Veja que na imagem eu coloquei como exemplo três comunidades que pertencem à mesma comunidade pai.
Uma coisa interessante que podemos observar é que a maior parte dos ativos relacionados a &lt;strong>Consumer staples&lt;/strong> forma uma comunidade com &lt;strong>Real state&lt;/strong> e &lt;strong>Utilities&lt;/strong> no segundo nível.&lt;/li>
&lt;/ul>
&lt;p>E as arestas?&lt;/p>
&lt;ul>
&lt;li>Podemos notar que um grande número de conexões entre &lt;strong>Financials&lt;/strong>, &lt;strong>Industrials&lt;/strong> e &lt;strong>Information technology&lt;/strong> sobreviveram ao filtro de disparidade. Sendo um indicativo que esses ativos têm uma forte relação nos retornos.
Ok, antes eu falei que o $\beta$ controla o efeito de atração entre as arestas, veja o que acontece se eu reduzir o $\beta$ para $0.5$:&lt;/li>
&lt;/ul>
&lt;figure id="figure-horrível-não-é-mesmo">
&lt;a data-fancybox="" href="/pt-br/post/nsbm_sp500_stock_market_disparity_filter/nsbm_2018-01-01_2018-06-01_beta_0.5_hu63a031e361e4cd7a15f51f9db9995b63_1589031_0x400_resize_lanczos_3.png" data-caption="Horrível não é mesmo?">
&lt;img src="/pt-br/post/nsbm_sp500_stock_market_disparity_filter/nsbm_2018-01-01_2018-06-01_beta_0.5_hu63a031e361e4cd7a15f51f9db9995b63_1589031_0x400_resize_lanczos_3.png" alt="" height="400px">
&lt;/a>
&lt;figcaption>
Horrível não é mesmo?
&lt;/figcaption>
&lt;/figure>
&lt;p>Você também pode explorar o resultado do nSBM manualmente. Para obter um sumário da hierarquia das comunidades obtidas pelo nSBM podemos invocar o método &lt;code>print_summary&lt;/code>&lt;/p>
&lt;pre>&lt;code class="language-python">state.print_summary()
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>l: 0, N: 483, B: 25
l: 1, N: 25, B: 6
l: 2, N: 6, B: 2
l: 3, N: 2, B: 1
l: 4, N: 1, B: 1
&lt;/code>&lt;/pre>
&lt;p>No nível de folhas temos os ativos. No primeiro nível temos &lt;strong>21&lt;/strong> comunidades para os &lt;strong>11&lt;/strong> setores.&lt;/p>
&lt;p>Supondo que você queira obter quais comunidades um dado ativo pertence, no caso &amp;ldquo;TSLA&amp;rdquo;,&lt;/p>
&lt;pre>&lt;code class="language-python"># esse é o indice da TSLA no nosso grafo original
symbol = &amp;quot;TSLA&amp;quot;
index_tesla = symbols.index(symbol)
symbol, symbol2sector[symbol], symbol2name[symbol]
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>('TSLA', 'Consumer Discretionary', 'Tesla')
&lt;/code>&lt;/pre>
&lt;p>Para obter as comunidades que o TSLA pertence percorremos a hierarquia de baixo para cima, até a raiz&lt;/p>
&lt;pre>&lt;code class="language-python"># para obter os indices
r0 = state.levels[0].get_blocks()[index_tesla]
r1 = state.levels[1].get_blocks()[r0]
r2 = state.levels[2].get_blocks()[r1]
r3 = state.levels[3].get_blocks()[r2]
(r1, r2, r3)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>(19, 0, 0)
&lt;/code>&lt;/pre>
&lt;p>Você pode explorar as comunidades usando essa abordagem. Contudo, eu recomendo você usar o THREE.js ou D3 para realizar essa exploração. Futuramente disponibilizarei meu código para permitir uma visualização interativa do nsbm usando threejs direto no browser!&lt;/p>
&lt;h3 id="outras-aplicações-de-nsbm">Outras aplicações de nSBM&lt;/h3>
&lt;p>nSBM&amp;rsquo;s e SBM&amp;rsquo;s encontram diversas aplicações como
&lt;a href="https://www.science.org/doi/10.1126/sciadv.aaq1360" target="_blank" rel="noopener">NLP&lt;/a> e em um trabalho recente meu em análise de
&lt;a href="https://arxiv.org/abs/2110.01421" target="_blank" rel="noopener">surveys&lt;/a>.
&lt;figure id="figure-um-jaba-próprio-nsbm-do-censo-escolar-pense">
&lt;a data-fancybox="" href="/pt-br/post/nsbm_sp500_stock_market_disparity_filter/allDummyAUC_alpha=0-05_hu1c20d905c5634e6a84f02ee440a1f7e5_1616945_0x400_resize_lanczos_3.png" data-caption="Um jaba próprio. nSBM do censo escolar PeNSE.">
&lt;img src="/pt-br/post/nsbm_sp500_stock_market_disparity_filter/allDummyAUC_alpha=0-05_hu1c20d905c5634e6a84f02ee440a1f7e5_1616945_0x400_resize_lanczos_3.png" alt="" height="400px">
&lt;/a>
&lt;figcaption>
Um jaba próprio. nSBM do censo escolar PeNSE.
&lt;/figcaption>
&lt;/figure>
&lt;/p>
&lt;h2 id="extras-mst">Extras: MST&lt;/h2>
&lt;p>Eu prometi mostrar como ficaria o mesmo universo de dados usando MST (árvores de expansão mínima). A intuição por trás do MST é que queremos construir um grafo esparso de um grafo original, tal que as somas dos pesos das arestas seja a menor possível sem desconectar os vértices do grafo. Veja mais aprofundado
&lt;a href="https://hudsonthames.org/networks-with-mlfinlab-minimum-spanning-tree-mst/" target="_blank" rel="noopener">aqui&lt;/a>.&lt;/p>
&lt;h3 id="convertendo-correlações-em-distâncias">Convertendo correlações em distâncias&lt;/h3>
&lt;p>A primeira coisa que precisamos fazer é converter a matriz de correlação em uma matriz de distância. Isso pode ser feito usando a seguinte função&lt;/p>
&lt;p>$d(\mathrm{stock}_1, \mathrm{stock}_2) = \sqrt{2(1-\mathrm{corr}(\mathrm{stock_1}, \mathrm{stock}_2))}$&lt;/p>
&lt;pre>&lt;code class="language-python">dist_matrix = np.sqrt(2*(1-correlation_matrix))
dist_matrix = dist_matrix.fillna(0)
np.fill_diagonal(dist_matrix.values, 0)
&lt;/code>&lt;/pre>
&lt;h3 id="extraindo-o-mst">Extraindo o MST&lt;/h3>
&lt;p>O &lt;code>igraph&lt;/code> já implementa um algoritmo para extrair o MST de um grafo de forma eficiente, mesmo que o grafo seja completo. Nossa matriz de correlação é um grafo completo!&lt;/p>
&lt;pre>&lt;code class="language-python">
g = ig.Graph.Weighted_Adjacency(dist_matrix.values, mode='undirected')
g = g.spanning_tree(weights=&amp;quot;weight&amp;quot;, return_tree=True)
g.vs[&amp;quot;symbol&amp;quot;] = returns_all.columns
sectors = [symbol2sector[symbol] for symbol in returns_all.columns]
colors = [
sector2color[sector] for sector in sectors
]
&lt;/code>&lt;/pre>
&lt;h3 id="visualizando-o-mst">Visualizando o MST&lt;/h3>
&lt;p>Agora com nosso MST vamos usar um layout de grafos bem simples para visualizar nosso grafo&lt;/p>
&lt;pre>&lt;code class="language-python">g.vs[&amp;quot;color&amp;quot;] = colors
pos = g.layout_fruchterman_reingold(niter=10000, weights=&amp;quot;weight&amp;quot;)
pos = np.array(pos.coords)
&lt;/code>&lt;/pre>
&lt;p>Finalmente, nosso resultado:&lt;/p>
&lt;pre>&lt;code class="language-python">from matplotlib.collections import LineCollection
lines = []
colors = []
for s, t in g.get_edgelist():
x0, y0 = pos[s]
x1, y1 = pos[t]
lines.append([(x0, y0), (x1, y1)])
colors.append(&amp;quot;black&amp;quot;)
lc = LineCollection(lines, colors=colors, zorder=0, alpha=.5)
fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(1, 1, 1)
ax.scatter(
pos[:, 0],
pos[:, 1],
c=g.vs[&amp;quot;color&amp;quot;],
s=25,
marker=&amp;quot;d&amp;quot;,
alpha=.8,
zorder=1
)
ax.add_collection(lc)
ax.axis('off')
legend = ax.legend(
[plt.Line2D([0], [0], color=c, lw=10)
for c in list(sector2color.values())],
list(sector2color.keys()),
bbox_to_anchor=(1.05, 1),
loc=2,
borderaxespad=0.)
plt.savefig(f'mst_{start_date}_{end_date}.png', bbox_inches='tight',)
plt.show()
&lt;/code>&lt;/pre>
&lt;figure id="figure-mst-para-ativos-do-sp500-no-primeiro-semestre-de-2018">
&lt;a data-fancybox="" href="/pt-br/post/nsbm_sp500_stock_market_disparity_filter/mst_2018-01-01_2018-06-01_huc06590f539d2bf35f6546a367d158c6d_73932_0x500_resize_lanczos_3.png" data-caption="MST para ativos do S&amp;amp;amp;P500 no primeiro semestre de 2018">
&lt;img data-src="/pt-br/post/nsbm_sp500_stock_market_disparity_filter/mst_2018-01-01_2018-06-01_huc06590f539d2bf35f6546a367d158c6d_73932_0x500_resize_lanczos_3.png" class="lazyload" alt="" width="100%" height="500px">
&lt;/a>
&lt;figcaption>
MST para ativos do S&amp;amp;P500 no primeiro semestre de 2018
&lt;/figcaption>
&lt;/figure>
&lt;p>Alguns padrões aparecem, mas o MST é muito menos rico de informações que o nSBM, exploraremos mais essas vantagens em posts futuros.&lt;/p>
&lt;h2 id="agradecimentos">Agradecimentos&lt;/h2>
&lt;p>Agradeço ao
&lt;a href="https://www.linkedin.com/in/maikereis/" target="_blank" rel="noopener">Maike Reis&lt;/a> e
&lt;a href="https://www.linkedin.com/in/felipe-alves-dos-santos/" target="_blank" rel="noopener">Felipe Santos&lt;/a> pelas dicas e correções.&lt;/p></description></item><item><title>Metaprogramação em Python com ASTs: criando um decorador com introspecção</title><link>/pt-br/post/python_ast_metaprogramming_with_introspection_and_decorators/</link><pubDate>Mon, 11 Apr 2022 00:00:00 +0000</pubDate><guid>/pt-br/post/python_ast_metaprogramming_with_introspection_and_decorators/</guid><description>&lt;details
class="toc-inpage d-print-none d-none d-sm-block d-md-none " open>
&lt;summary class="font-weight-bold">Lista de Conteúdos&lt;/summary>
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>
&lt;ul>
&lt;li>&lt;a href="#introdução">Introdução&lt;/a>&lt;/li>
&lt;li>&lt;a href="#asts-o-que-são">ASTs: O que são?&lt;/a>&lt;/li>
&lt;li>&lt;a href="#python-é--interpretado-ou-compilado">Python é interpretado ou compilado?&lt;/a>&lt;/li>
&lt;li>&lt;a href="#extraindo-e-interpretando-asts">Extraindo e interpretando ASTs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#como-metaprogramar-de-forma-eficiente">Como metaprogramar de forma eficiente?&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#6-passos-simples">6 passos simples&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#criando-nossa-meta-função">Criando nossa meta-função&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#primeira-iteração">Primeira iteração&lt;/a>&lt;/li>
&lt;li>&lt;a href="#o-nodetransformer">O NodeTransformer&lt;/a>&lt;/li>
&lt;li>&lt;a href="#a-segunda-iteração">A segunda iteração&lt;/a>&lt;/li>
&lt;li>&lt;a href="#criando-uma-nova-função-em-runtime">Criando uma nova função em runtime&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#integrando-a-manipulação-de-ast-com-um-decorador">Integrando a manipulação de AST com um decorador&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/details>
&lt;h2 id="introdução">Introdução&lt;/h2>
&lt;p>Não se assuste com as palavras no título. Embora possam ser estranhas para você provavelmente em algum momento você utilizou ferramentas que fazem uso de técnicas de metaprogramação ou inspeção de AST. Pytest e Numba são exemplos.&lt;/p>
&lt;p>
&lt;a href="/pt-br/post/python_decorator_that_exposes_locals/" title="An introspective python decorator using stack frames and the inspect module">No post anterior eu falei sobre python frames e inspection.&lt;/a>.
Mostrei como podemos usar &lt;code>inspect.signautre&lt;/code> para criar um decorador que valide argumentos:&lt;/p>
&lt;pre>&lt;code class="language-python">@math_validator()
def simple_method(x: &amp;quot;\in R&amp;quot;, y: &amp;quot;\in R_+&amp;quot;, z: float = 2) -&amp;gt; float:
...
simple_method(1, 0)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>simple_method((1, 2)) -&amp;gt; 1.5
---&amp;gt; 19 simple_method(1, 0)
...
&amp;lt;locals&amp;gt;.decorate.&amp;lt;locals&amp;gt;.decorated(*_args)
11 continue
13 if not MATH_SPACES[annotation][&amp;quot;validator&amp;quot;](_args[i]):
---&amp;gt; 14 raise ValueError(f&amp;quot;{k} doesn't belong to the {MATH_SPACES[annotation]['name']}&amp;quot;)
15 result = func(*_args)
16 print(f&amp;quot;{func.__name__}({_args}) -&amp;gt; {result}&amp;quot;)
ValueError: y doesn't belong to the space of real numbers greater than zero
&lt;/code>&lt;/pre>
&lt;p>No outro exemplo mostrei como podemos combinar o &lt;code>signature&lt;/code> com &lt;code>sys.trace&lt;/code> para criar um decorador que expõe o &lt;code>locals&lt;/code> da função decorada. O que nos permite fazer coisas legais tais como criar um decorador &lt;code>@report&lt;/code>&lt;/p>
&lt;pre>&lt;code class="language-python">@report('{arg.n_bananas} Monkey {gluttonous_monkey} ate too much bananas. Num monkeys {num_monkeys}')
def feed_monkeys(n_bananas):
num_monkeys = 3
monkeys = {
f&amp;quot;monkey_{i}&amp;quot;: {&amp;quot;bananas&amp;quot;: 0}
for i in range(num_monkeys)
}
while n_bananas &amp;gt; 0:
if np.random.uniform() &amp;lt; 0.4:
continue
monkey = monkeys[np.random.choice(list(monkeys.keys()))]
if n_bananas &amp;gt; 0:
monkey[&amp;quot;bananas&amp;quot;] += 1
n_bananas -= 1
gluttonous_monkey = max(monkeys, key=lambda k: monkeys[k][&amp;quot;bananas&amp;quot;])
&lt;/code>&lt;/pre>
&lt;p>Contudo, no final do post passado eu disse que essa solução tem alguns problemas&lt;/p>
&lt;p>&lt;div class="spoiler " >
&lt;p>
&lt;a class="btn btn-primary" data-toggle="collapse" href="#spoiler-2" role="button" aria-expanded="false" aria-controls="spoiler-2">
Click here to see the solution
&lt;/a>
&lt;/p>
&lt;div class="collapse card " id="spoiler-2">
&lt;div class="card-body">
&lt;pre>&lt;code class="language-python">import sys
import inspect
from types import SimpleNamespace
def call_and_extract_frame(func, *args, **kwargs):
frame_var = None
trace = sys.gettrace()
def update_frame_var(stack_frame, event_name, arg_frame):
&amp;quot;&amp;quot;&amp;quot;
Args:
stack_frame: (frame)
The current stack frame.
event_name: (str)
The name of the event that triggered the call.
Can be 'call', 'line', 'return' and 'exception'.
arg_frame:
Depends on the event. Can be a None type
&amp;quot;&amp;quot;&amp;quot;
nonlocal frame_var # nonlocal is a keyword which allows us to modify the outisde scope variable
if event_name != 'call':
return trace
frame_var = stack_frame
sys.settrace(trace)
return trace
sys.settrace(update_frame_var)
try:
func_result = func(*args, **kwargs)
finally:
sys.settrace(trace)
return frame_var, func_result
def report(formater):
def decorate(func):
def decorated(*_args):
sig = inspect.signature(func)
named_args = {}
num_args = len(_args)
for i, (k, v) in enumerate(sig.parameters.items()):
if i &amp;lt; num_args:
named_args[k] = repr(_args[i])
else:
named_args[k] = repr(v.default)
frame_func, _result = call_and_extract_frame(func, *_args)
name = func.__name__
result = repr(_result)
args_dict = {
&amp;quot;args&amp;quot;: SimpleNamespace(**named_args),
&amp;quot;args_repr&amp;quot;: repr(SimpleNamespace(**named_args)),
**locals(),
**frame_func.f_locals,
}
print(formater.format(**args_dict))
# do other stuff here
return _result
return decorated
return decorate
&lt;/code>&lt;/pre>
&lt;/div>
&lt;/div>
&lt;/div>
Quais são os problemas?&lt;/p>
&lt;ul>
&lt;li>
&lt;p>É esperado que o tracing reduza a performance do sistema. Se você usar a solução acima só para casos pontuais ou debug é ok&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Pode criar conflitos com outras ferramentas e bibliotecas que também estão usando a ferramenta de tracing, tais como debuggers.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Parece uma solução feia!&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Você pode se perguntar: &amp;ldquo;&lt;em>Overengineering! Era só esse fazer isso aqui:&lt;/em>&amp;rdquo;&lt;/p>
&lt;pre>&lt;code class="language-python">@report('stuff goes here')
def func(x, y):
random_var = np.random.uniform()
... #more local vars
result = (x+y)**random_var
return result, locals
&lt;/code>&lt;/pre>
&lt;p>&lt;em>&amp;quot;..e dentro do decorador ele mudar para isso&amp;quot;&lt;/em>&lt;/p>
&lt;pre>&lt;code class="language-python">_result, local_vars = func(x, y)
&lt;/code>&lt;/pre>
&lt;p>A razão é:&lt;/p>
&lt;p>O ponto de usar um decorador é para evitar mudanças em qualquer outra parte da nossa codebase. Por exemplo, se em qualquer outra parte da nossa codebase func está sendo chamada, eu teria que fazer mudanças do tipo&lt;/p>
&lt;pre>&lt;code class="language-python">result = func(x, y) # to
result = func(x, y)[0]
&lt;/code>&lt;/pre>
&lt;p>E se futuramente eu quisesse remover o decorador de uma função eu teria que desfazer todas as mudanças acima&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Você irá aumentar o cognitive load de todos os membros do seu time que não precisam saber sobre ou usar o decorador.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Se você está ok com fazer mudanças em outros lugares do seu código por que não criar novas funções ao invés de decoradores que funcionam mais ou menos?&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Ok, você pode estar pensando: &amp;ldquo;Tá , faz sentido não fazer isso que sugeri, mas do que adianta evitar sujar sua codebase se você tá criando problemas de desempenho e debug? Não parece uma boa solução na maioria dos casos.&amp;rdquo; Eu tenho que concordar com você!&lt;/p>
&lt;p>Bom, então o que podemos fazer?? O problema que encontramos é que em python não temos context managers que podem lidar com namespaces
&lt;a href="https://mail.python.org/archives/list/python-ideas@python.org/thread/TAVHEKDZVYKJUGZKWSVZVAOGBPLZVKQG/" target="_blank" rel="noopener">https://mail.python.org/archives/list/python-ideas@python.org/&lt;/a>. Mas não se desanime com essa limitação, a questão agora é:&lt;/p>
&lt;div class="alert alert-note">
&lt;div>
&lt;strong>Se uma linguagem não tem uma feature que eu preciso o que eu posso fazer?&lt;/strong>
&lt;/div>
&lt;/div>
&lt;p>Em pyhton estamos bem com isso pois é fácil manipular o que é conhecido como &lt;strong>A&lt;/strong>bstract &lt;strong>S&lt;/strong>yntax &lt;strong>T&lt;/strong>ree (árvore sintática abstrata) e compilar ela em uma nova função em tempo de execução (runtime). ** Quando programamos desse jeito estamos no reino da metaprogramação! Tentarei esclarecer esses pontos agora**&lt;/p>
&lt;h2 id="asts-o-que-são">ASTs: O que são?&lt;/h2>
&lt;p>Uma linguagem de programação é obviamente, pelo menos uma linguagem&amp;hellip; OK, &lt;strong>mas o que é uma linguagem? Todas as linguagens humanas compartilham uma estrutura em comum? Como podemos comparar sentenças diferentes na mesma linguagem?&lt;/strong> Essas questões talvez pareçam ser mais adequadas para serem respondidas por filósofos. Contudo, também é tema de trabalho de matemáticos e computeiros&lt;/p>
&lt;p>A grande diferença é que matemáticos e computeiros comumente preferem falar sobre coisas usando algum formalismo matemático. Em essência, &lt;strong>AST&lt;/strong> faz parte de um formalismo matemático que permite isso. Uma &lt;strong>AST&lt;/strong> permite representar uma sentença através de um grafo direcionado do tipo árvore. Para isso usamos um conjunto de regras bem definidas em como construir essa árvore.&lt;/p>
&lt;h3>Como saber se uma sentença está gramaticalmente correta?&lt;/h3>
&lt;p>Você provavelmente se lembra quase institivamente de um conjunto de regras que aprendeu durante sua vida ou acabou se acostumando sobre como organizar e compor verbos, substantivos, adjetivos, etc. Este conjunto de regras e guias é a sintaxe da linguagem que você fala/escreve. &lt;em>AST&lt;/em>s permitem checar e compreender uma sentença utilizando essas regras&lt;/p>
&lt;p>Pegue por exemplo a sentença&lt;/p>
&lt;p>&lt;em>&amp;ldquo;I drive a car to my college&amp;rdquo;&lt;/em>, a AST é a seguinte&lt;/p>
&lt;figure id="figure-fonte-geeks-for-geekssyntax-tree--natural-language-processinghttpswwwgeeksforgeeksorgsyntax-tree-natural-language-processing">
&lt;a data-fancybox="" href="/pt-br/post/python_ast_metaprogramming_with_introspection_and_decorators/ast_english_sentence_hue5b8d52ce962721ee6d0acb19268cb10_239788_0x400_resize_lanczos_3.png" data-caption="&amp;lt;strong&amp;gt;Fonte&amp;lt;/strong&amp;gt;:&amp;lt;a href=&amp;#34;https://www.geeksforgeeks.org/syntax-tree-natural-language-processing/&amp;#34;&amp;gt; Geeks for Geeks:Syntax Tree – Natural Language Processing.&amp;lt;/a&amp;gt;">
&lt;img src="/pt-br/post/python_ast_metaprogramming_with_introspection_and_decorators/ast_english_sentence_hue5b8d52ce962721ee6d0acb19268cb10_239788_0x400_resize_lanczos_3.png" alt="" height="400px">
&lt;/a>
&lt;figcaption>
&lt;strong>Fonte&lt;/strong>:&lt;a href="https://www.geeksforgeeks.org/syntax-tree-natural-language-processing/"> Geeks for Geeks:Syntax Tree – Natural Language Processing.&lt;/a>
&lt;/figcaption>
&lt;/figure>
&lt;p>Qual a vantagem de usar ASTs? Note que não precisamos falar de espaços, caligrafia ou estilo pessoal de organizar escrita para compreender uma sentença e saber se ela está válida. Além disso, temos uma estrutura hierárquica que permite entender a sentença por níveis!&lt;/p>
&lt;p>Não é uma surpresa que ASTs são também uma ferramenta comum em processos de analisar a validade de um código ou na construção de um compilador/interpretador. Nesse post iremos manipular a AST! Mas antes disso quero fazer uma pergunta:&lt;/p>
&lt;h2 id="python-é--interpretado-ou-compilado">Python é interpretado ou compilado?&lt;/h2>
&lt;p>Geralmente, quando encontro um hater de python ou mesmo um entusiasta ouço ou leio coisas do tipo:&lt;/p>
&lt;ul>
&lt;li>&amp;ldquo;&lt;em>Python é lento pois é uma linguagem interpretada&lt;/em>&amp;rdquo;&lt;/li>
&lt;li>&amp;ldquo;*Python é legal pois não tem chatice de compilação&amp;rdquo;&lt;/li>
&lt;li>&lt;em>&amp;ldquo;Python é ruim comparado a C pois não tem um compilador&amp;rdquo;&lt;/em>&lt;/li>
&lt;/ul>
&lt;p>Bem, essas asserções não são verdadeiras, pois estão usando conceitos errados! Outra confusão é que geralmente quando se fala em python estamos nos referindo a linguagem (sintaxe, etc) python mais a máquina virtual do CPython. Vamos conversar um pouco mais sobre isso&lt;/p>
&lt;p>Dizer que uma linguagem hoje é &lt;em>puramente compilada ou interpretada&lt;/em> é confuso, pois essa divisão é borrada. Veja o seguinte&lt;/p>
&lt;pre>&lt;code class="language-python">hello_world = &amp;quot;print('Hello, world!')&amp;quot;
hello_world_obj = compile(hello_world, '&amp;lt;string&amp;gt;', 'single')
&lt;/code>&lt;/pre>
&lt;p>Pois é&amp;hellip; se você tentaria defender nos comentários que python é puramente interpretado as coisas estão mais difíceis para você. Por que tem um &lt;code>compile&lt;/code> disponível? O que ele faz?&lt;/p>
&lt;pre>&lt;code class="language-python">exec(hello_world_obj)
Hello, world!
&lt;/code>&lt;/pre>
&lt;p>O que será que tem dentro desse &lt;code>hello_world_obj&lt;/code>?&lt;/p>
&lt;pre>&lt;code class="language-python">print(f&amp;quot;Bad news for you:\n\tContent: {hello_world_obj.co_code}\n\tType: {type(hello_world_obj.co_code)}&amp;quot;)
Bad news for you:
Content: b'e\x00d\x00\x83\x01F\x00d\x01S\x00'
Type: &amp;lt;class 'bytes'&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>Para entender os prints acima você precisa compreender o que acontece por trás dos panos quando um código python é &amp;ldquo;&lt;em>interpretado&lt;/em>&amp;rdquo;.&lt;/p>
&lt;p>Após você escrever um código e chamar o comando python, o python inicia um processo de compilação criando as ASTs, depois gerando bytecodes a partir das ASTs e esses últimos serão encapsulados em &lt;code>code_object&lt;/code>s. Na última etapa os code objects serão interpretados pela máquina virtual do CPython. O diagrama à baixo é uma representação simples (com passos omitidos) do processo&lt;/p>
&lt;div class="mermaid mermaidContainer">
graph LR;
A[Source Code]-->|parsing|B[Parse Tree];
B-->C[AST];
C-->E[Bytecode];
E-->F[Code Object];
F-->|execution by|G[CPython Virtual Machine];
&lt;/div>
&lt;p>A fase de compilação são os primeiros passos do diagrama acima&lt;/p>
&lt;div class="mermaid mermaidContainer">
graph LR;
A[Source Code]-->|parsing|B[Parse Tree];
B-->C[AST];
C-->E[Bytecode];
E-->F[Code Object];
&lt;/div>
Se você não conhece os conceitos dos nomes acima não se preocupe, não precisamos de tanto aprofundamento.
**Bytecodes são apenas uma maneira compacta de dizer ao interpretador o que o código quer que ele faça. Enquanto code objects são coisas que encapsulam esses bytecodes.**
&lt;p>Ok, onde isso entra na minha solução? O que eu proponho fazer é manipular a AST e compilar um novo code object que será interpretado pelo cpython!&lt;/p>
&lt;!-- > Um história engraçada do Luciano Ramalho:
-->
&lt;h2 id="extraindo-e-interpretando-asts">Extraindo e interpretando ASTs&lt;/h2>
&lt;p>Veja o seguinte exemplo:&lt;/p>
&lt;pre>&lt;code class="language-python">import inspect
import ast
import astor # install this for pretty printing
def example(a: float, b:float = 2) -&amp;gt; float:
s = a+b
return s
tree = ast.parse(inspect.getsource(example))
print(astor.dump(tree))
astor.to_source(tree)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>Module(
body=[
FunctionDef(name='example',
args=arguments(posonlyargs=[],
args=[arg(arg='a', annotation=Name(id='float'), type_comment=None),
arg(arg='b', annotation=Name(id='float'), type_comment=None)],
vararg=None,
kwonlyargs=[],
kw_defaults=[],
kwarg=None,
defaults=[Constant(value=2, kind=None)]),
body=[
Assign(targets=[Name(id='s')],
value=BinOp(left=Name(id='a'), op=Add, right=Name(id='b')),
type_comment=None),
Return(value=Name(id='s'))],
decorator_list=[],
returns=Name(id='float'),
type_comment=None)],
type_ignores=[])
&lt;/code>&lt;/pre>
&lt;p>O output acima é a AST da função. Gaste algum tempo olhando essa saída e tente entender/inferir o que cada coisa significa e como ela é organizada. A imagem abaixo é a representação visual da saída acima&lt;/p>
&lt;figure >
&lt;a data-fancybox="" href="/pt-br/post/python_ast_metaprogramming_with_introspection_and_decorators/simple_ast_hudca446749283cbe6d28b67a245474890_120568_0x1000_resize_lanczos_3.png" >
&lt;img src="/pt-br/post/python_ast_metaprogramming_with_introspection_and_decorators/simple_ast_hudca446749283cbe6d28b67a245474890_120568_0x1000_resize_lanczos_3.png" alt="" height="400px">
&lt;/a>
&lt;/figure>
&lt;p>Cada elemento do output que inicia com uma letra maiúscula é um nó, &lt;strong>node&lt;/strong>(Name, BinOp, FunctionDef, etc) derivado da classe &lt;code>ast.Node&lt;/code>. Um dos nós mais importante é o &lt;code>ast.Name&lt;/code>.
Por exemplo em&lt;/p>
&lt;pre>&lt;code>value=BinOp(left=Name(id='a'), op=Add, right=Name(id='b')),
&lt;/code>&lt;/pre>
&lt;p>o &lt;code>ast.Name(...&lt;/code> é usado para referenciar as variáveis &lt;code>a&lt;/code> e &lt;code>b&lt;/code> da nossa função.&lt;/p>
&lt;p>Ok, voltemos ao nosso problema. Lembre-se que uma solução ruim era reescrever cada função que precisa ser decorada, por exemplo&lt;/p>
&lt;pre>&lt;code class="language-python">def func(x, y):
random_var = np.random.uniform()
... #more local vars
result = (x+y)**random_var
return result
&lt;/code>&lt;/pre>
&lt;p>como&lt;/p>
&lt;pre>&lt;code class="language-python">def func_transformed(x, y):
random_var = np.random.uniform()
... #more local vars
result = (x+y)**random_var
return result, locals
&lt;/code>&lt;/pre>
&lt;p>A coisa legal que faremos aqui é &lt;strong>escrever uma função que escrevera essas mudanças para nós! E depois colocaremos a compilação dentro de um decorador para evitar que nossa codebase seja alterada.&lt;/strong>&lt;/p>
&lt;h2 id="como-metaprogramar-de-forma-eficiente">Como metaprogramar de forma eficiente?&lt;/h2>
&lt;p>Fazer um código que faça as alterações desejadas na nossa AST pode ser trabalhoso. Como começar a ter uma ideia do que precisa ser feito? Eu penso em uma sucessão de 6 passos e ir iterando para melhorar&lt;/p>
&lt;h3 id="6-passos-simples">6 passos simples&lt;/h3>
&lt;ol>
&lt;li>Criar uma função exemplo (A)&lt;/li>
&lt;li>Codar uma função transformada do jeito que queremos que ela seja (B)&lt;/li>
&lt;li>Escrever um teste para que possa ser usado posteriormente para&lt;br>
checar se nossa função transformada (B) bate com a função gerada pela meta-programação (C)&lt;/li>
&lt;li>Extrair a AST de A e B&lt;/li>
&lt;li>Comparar as ASTs. O que elas diferem? Anote as diferenças
&lt;ul>
&lt;li>Você pode usar a &lt;code>difflib&lt;/code> do python para fazer isso&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Criar uma nova e mais complexa função exemplo (A) e repetir o processo até termos uma boa ideia das modificações necessárias na AST&lt;/li>
&lt;/ol>
&lt;h2 id="criando-nossa-meta-função">Criando nossa meta-função&lt;/h2>
&lt;h3 id="primeira-iteração">Primeira iteração&lt;/h3>
&lt;p>Começaremos escrevendo uma função incrivelmente simples&lt;/p>
&lt;pre>&lt;code class="language-python">def example_1(x, y):
internal_var = 222
result = (x+y)**internal_var
return result
def example_1_expected(x, y):
internal_var = 222
result = (x+y)**internal_var
return result, locals()
def test_meta_example_1(meta_func, x, y):
expected_result, expected_locals = example_1_expected(x, y)
result, locals_dict = meta_func(x, y)
assert result == expected_result
assert expected_locals == locals_dict
&lt;/code>&lt;/pre>
&lt;p>Agora usaseri a &lt;code>difflib&lt;/code> para entender as diferenças entre as duas ASTs.&lt;/p>
&lt;pre>&lt;code class="language-python">import difflib
from pprint import pprint
example_1_ast_str = astor.dump_tree(ast.parse(inspect.getsource(example_1)))
example_1_expected_str = astor.dump_tree(ast.parse(inspect.getsource(example_1_expected)))
pprint(
list(
difflib.unified_diff(example_1_ast_str.splitlines(), example_1_expected_str.splitlines(), n=0)
)
)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>['--- \n',
'+++ \n',
'@@ -3 +3 @@\n',
&amp;quot;- FunctionDef(name='example_1',&amp;quot;,
&amp;quot;+ FunctionDef(name='example_1_expected',&amp;quot;,
'@@ -19 +19 @@\n',
&amp;quot;- Return(value=Name(id='result'))],&amp;quot;,
&amp;quot;+ Return(value=Tuple(elts=[Name(id='result'), &amp;quot;
&amp;quot;Call(func=Name(id='locals'), args=[], keywords=[])]))],&amp;quot;]
&lt;/code>&lt;/pre>
&lt;p>Com o output acima sabemos aogra que precisaremos mudar o seguinte nó na AST&lt;/p>
&lt;pre>&lt;code>Return(value=Name(id='result'))],
&lt;/code>&lt;/pre>
&lt;p>para isto&lt;/p>
&lt;pre>&lt;code>Return(value=Tuple(elts=[Name(id='result'), Call(func=Name(id='locals'), args=[], keywords=[])]))],
&lt;/code>&lt;/pre>
&lt;p>Como alterar nós na AST? Com a ajuda do &lt;code>NodeTransformer&lt;/code>&lt;/p>
&lt;h3 id="o-nodetransformer">O NodeTransformer&lt;/h3>
&lt;p>O &lt;code>ast.NodeTransformer&lt;/code> nos permite criar objetos com uma interface de caminhante. O caminhante visitará cada Node da AST e durante cada visita ele pode remover, substituir, modificar ou adicionar Nodes. Após fazer essas alterações o caminhante pode continuar sua caminhada nos filhos do Node ou apenas parar.&lt;/p>
&lt;p>Vamos iniciar criando uma classe derivada de &lt;code>ast.NodeTransformer&lt;/code>&lt;/p>
&lt;pre>&lt;code class="language-python">class ASTTransformer(ast.NodeTransformer):
def visit_Return(self, node):
&lt;/code>&lt;/pre>
&lt;p>Se queremos interagir com um nó do tipo &lt;code>AlgumaCoisa&lt;/code> precisamos sobrescrever o método &lt;code>visit_AlgumaCoisa&lt;/code>.
Portanto, como sabemos que precisamos mudar o &lt;code>Return&lt;/code> iremos sobrescrever o &lt;code>visit_Return&lt;/code>. Precisaremos criar também um nó para pegar o &lt;code>locals&lt;/code>. Esse nó é o &lt;code>Call&lt;/code>&lt;/p>
&lt;pre>&lt;code class="language-python">class ASTTransformer(ast.NodeTransformer):
def visit_Return(self, node):
node_locals = ast.Call(
func=ast.Name(id='locals', ctx=ast.Load()),
args=[], keywords=[]
)
self.generic_visit(node)
return node
&lt;/code>&lt;/pre>
&lt;p>Veja que usamos o nó &lt;code>Name&lt;/code> para identificar a função &lt;code>locals&lt;/code>. Agora, de acordo com o resultado do nosso diff o resultado do &lt;code>Return&lt;/code> precisa ser uma nó do tipo &lt;code>Tuple&lt;/code>&lt;/p>
&lt;pre>&lt;code class="language-python">class ASTTransformer(ast.NodeTransformer):
def visit_Return(self, node):
node_locals = ast.Call(
func=ast.Name(id='locals', ctx=ast.Load()),
args=[], keywords=[]
)
new_node.value = ast.Tuple(
elts=[
node.value,
node_locals
],
ctx=ast.Load()
)
self.generic_visit(new_node)
return new_node
&lt;/code>&lt;/pre>
&lt;p>Uma nova coisa apareceu. O argumento &lt;code>elts&lt;/code>. Não se preoucupe em entender tudo. Mas o &lt;code>elts&lt;/code> é um arg que diz qual é a lista de nós que a Tupla deve conter. Toda vez que você quiser entender um pouco mais sobre ASTs e a gramática do python você pode consultar a documentação oficial
&lt;a href="https://docs.python.org/3/library/ast.html" target="_blank" rel="noopener">aqui&lt;/a>.&lt;/p>
&lt;p>Quase tudo pronto. A última coisa que precisamos fazer é corrigir nossa AST. Pois ao alterar o Node precisamos preencher/corrigir as informações de line_number e column_offest. O python torna isso fácil com o método &lt;code>fix_missing_locations&lt;/code>&lt;/p>
&lt;pre>&lt;code class="language-python">
class ASTTransformer(ast.NodeTransformer):
def visit_Return(self, node):
new_node = node
node_locals = ast.Call(
func=ast.Name(id='locals', ctx=ast.Load()),
args=[], keywords=[]
)
new_node.value = ast.Tuple(
elts=[
node.value,
node_locals
],
ctx=ast.Load()
)
ast.copy_location(new_node, node)
ast.fix_missing_locations(new_node)
self.generic_visit(new_node)
return new_node
&lt;/code>&lt;/pre>
&lt;p>Ok, vamos ver se funcionou. Para isso, precisamos instanciar nosso transformer e chamar o método &lt;code>visit&lt;/code> que diz para o caminhante iniciar a caminhada e fazer as modificações pedidas&lt;/p>
&lt;pre>&lt;code class="language-python">tree_meta = ast.parse(inspect.getsource(example_1))
transformer = ASTTransformer()
transformer.visit(tree_meta)
example_1_meta_ast_str = astor.dump_tree(tree_meta)
example_1_expected_str = astor.dump_tree(ast.parse(inspect.getsource(example_1_expected)))
pprint(
list(
difflib.unified_diff(example_1_meta_ast_str.splitlines(), example_1_expected_str.splitlines(), n=0)
)
)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>['--- \n',
'+++ \n',
'@@ -3 +3 @@\n',
&amp;quot;- FunctionDef(name='example_1',&amp;quot;,
&amp;quot;+ FunctionDef(name='example_1_expected',&amp;quot;]
&lt;/code>&lt;/pre>
&lt;p>Funcionou! Vamos adicionar um pouco mais de complicação para ver se o NodeTransformer continuará funcionando.&lt;/p>
&lt;h3 id="a-segunda-iteração">A segunda iteração&lt;/h3>
&lt;p>Seja criativo na hora de complicar, eu fiz isso aqui é feio mais adiciona muita confusão para estressar o NodeTransformer.&lt;/p>
&lt;pre>&lt;code class="language-python">def example_2(x, y):
internal_var = 222
def sub(x, y):
ommit_this_var = 1
return x - y
result = sub(x,y)**internal_var
return (result, False)
def example_2_expected(x, y):
internal_var = 222
def sub(x, y):
ommit_this_var = 1
return x - y
result = sub(x,y)**internal_var
return ((result, False), locals())
def test_meta_example_2(meta_func, x, y):
expected_result, expected_locals = example_2_expected(x, y)
result, locals_dict = meta_func(x, y)
del locals_dict[&amp;quot;sub&amp;quot;]
del expected_locals[&amp;quot;sub&amp;quot;]
assert result == expected_result
assert expected_locals == locals_dict
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">example_2_ast_str = astor.dump_tree(ast.parse(inspect.getsource(example_2)))
example_2_expected_str = astor.dump_tree(ast.parse(inspect.getsource(example_2_expected)))
pprint(
list(
difflib.unified_diff(example_2_ast_str.splitlines(), example_2_expected_str.splitlines(), n=0)
)
)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>['--- \n',
'+++ \n',
'@@ -3 +3 @@\n',
&amp;quot;- FunctionDef(name='example_2',&amp;quot;,
&amp;quot;+ FunctionDef(name='example_2_expected',&amp;quot;,
'@@ -37 +37,4 @@\n',
&amp;quot;- Return(value=Tuple(elts=[Name(id='result'), &amp;quot;
'Constant(value=False, kind=None)]))],',
'+ Return(',
'+ value=Tuple(',
&amp;quot;+ elts=[Tuple(elts=[Name(id='result'), &amp;quot;
'Constant(value=False, kind=None)]),',
&amp;quot;+ Call(func=Name(id='locals'), args=[], &amp;quot;
'keywords=[])]))],']
&lt;/code>&lt;/pre>
&lt;p>Agora é hora de cruzar os dedos e esperar que continue funcionando&lt;/p>
&lt;pre>&lt;code class="language-python">tree_meta = ast.parse(inspect.getsource(example_2))
transformer = ASTTransformer()
transformer.visit(tree_meta)
example_2_meta_ast_str = astor.dump_tree(tree_meta)
example_2_expected_str = astor.dump_tree(ast.parse(inspect.getsource(example_2_expected)))
pprint(
list(
difflib.unified_diff(example_2_meta_ast_str.splitlines(), example_2_expected_str.splitlines(), n=0)
)
)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>['--- \n',
'+++ \n',
'@@ -3 +3 @@\n',
&amp;quot;- FunctionDef(name='example_2',&amp;quot;,
&amp;quot;+ FunctionDef(name='example_2_expected',&amp;quot;,
'@@ -27,4 +27 @@\n',
'- Return(',
'- value=Tuple(',
&amp;quot;- elts=[BinOp(left=Name(id='x'), op=Sub, &amp;quot;
&amp;quot;right=Name(id='y')),&amp;quot;,
&amp;quot;- Call(func=Name(id='locals'), args=[], &amp;quot;
'keywords=[])]))],',
&amp;quot;+ Return(value=BinOp(left=Name(id='x'), op=Sub, &amp;quot;
&amp;quot;right=Name(id='y')))],&amp;quot;]
&lt;/code>&lt;/pre>
&lt;p>Falhou miseravelmente. Qual é o problema? Se você olhar o &lt;code>diff&lt;/code> com cuidado verá que o &lt;code>NodeTransformer&lt;/code> alterou a função interna. Não queremos isso. Portanto, diremos para o caminhante evitar modificar se estiver em uma função interna. Para isso, precisamos sobrescrever o método &lt;code>visit_FunctionDef&lt;/code> e criar uma flag para marcar em que nível o caminhante está&lt;/p>
&lt;pre>&lt;code class="language-python">class ASTTransformer(ast.NodeTransformer):
def visit_FunctionDef(self, node):
if self._sub:
return node
self._sub = True
self.generic_visit(node)
return node
def visit_Module(self, node):
self._sub = 0
self.generic_visit(node)
def visit_Return(self, node):
new_node = node
node_locals = ast.Call(
func=ast.Name(id='locals', ctx=ast.Load()),
args=[], keywords=[]
)
new_node.value = ast.Tuple(
elts=[
node.value,
node_locals
],
ctx=ast.Load()
)
ast.copy_location(new_node, node)
ast.fix_missing_locations(new_node)
self.generic_visit(new_node)
return new_node
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">tree_meta = ast.parse(inspect.getsource(example_2))
transformer = ASTTransformer()
transformer.visit(tree_meta)
example_2_meta_ast_str = astor.dump_tree(tree_meta)
example_2_expected_str = astor.dump_tree(ast.parse(inspect.getsource(example_2_expected)))
pprint(
list(
difflib.unified_diff(example_2_meta_ast_str.splitlines(), example_2_expected_str.splitlines(), n=0)
)
)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>['--- \n',
'+++ \n',
'@@ -3 +3 @@\n',
&amp;quot;- FunctionDef(name='example_2',&amp;quot;,
&amp;quot;+ FunctionDef(name='example_2_expected',&amp;quot;]
&lt;/code>&lt;/pre>
&lt;p>Tudo ok! Próximo passo: compilar nossa ast.&lt;/p>
&lt;h3 id="criando-uma-nova-função-em-runtime">Criando uma nova função em runtime&lt;/h3>
&lt;p>O que faremos agora é compilar a AST transformada e associa-la com uma nova função. Em python podemos fazer isso em tempo de execução com &lt;code>type.FunctionType&lt;/code>&lt;/p>
&lt;pre>&lt;code class="language-python">from types import FunctionType, CodeType
def transform_and_compile(func: FunctionType)-&amp;gt;FunctionType:
source = inspect.getsource(func)
# we put this to remove the line from source code with the decorator
source = &amp;quot;\n&amp;quot;.join([l for l in source.splitlines() if not l.startswith(&amp;quot;@&amp;quot;)])
tree = ast.parse(source)
transformer = ASTTransformer()
transformer.visit(tree)
code_obj = compile(tree, func.__code__.co_filename, 'exec')
function_code = [c for c in code_obj.co_consts if isinstance(c, CodeType)][0]
# we must to pass the globals context to the function
transformed_func = FunctionType(function_code, func.__globals__)
return transformed_func
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">test_meta_example_1(transform_and_compile(example_1), 4, 2)
test_meta_example_2(transform_and_compile(example_2), 1, 2)
&lt;/code>&lt;/pre>
&lt;p>Veja que &lt;code>transform_and_compile&lt;/code> foi capaz de criar novas funções que passaram nos testes que escrevemos nas iterações anteriores! Agora é o passo final e mais fácil desse post. Integrar com o decorador.&lt;/p>
&lt;h2 id="integrando-a-manipulação-de-ast-com-um-decorador">Integrando a manipulação de AST com um decorador&lt;/h2>
&lt;p>O que faremos é chamar &lt;code>transform_and_compile&lt;/code> logo após o &lt;code>def decorate&lt;/code> para evitar compilações desnecessárias toda vez que chamarmos a função decorada&lt;/p>
&lt;pre>&lt;code class="language-python">def report(fmt):
def decorate(func):
meta_func = transform_and_compile(func)
....
&lt;/code>&lt;/pre>
&lt;p>Agora, dentro de &lt;code>def decorated&lt;/code> podemos chamar a &lt;code>meta_func&lt;/code> e retornar só o resultado pois não queremos mudar nossa codebase&lt;/p>
&lt;pre>&lt;code class="language-python">def report(fmt):
def decorate(func):
meta_func = transform_and_compile(func)
...
def decorated(*_args):
_result, internal_locals = meta_func(*_args)
....
return _result
&lt;/code>&lt;/pre>
&lt;p>Com todas as coisas que fizemos no post nosso decorador &lt;code>report&lt;/code> está pronto para ser usado&lt;/p>
&lt;pre>&lt;code class="language-python">
def report(fmt):
def decorate(func):
meta_func = transform_and_compile(func)
sig = inspect.signature(func)
def decorated(*_args):
_result, internal_locals = meta_func(*_args)
named_args = {}
num_args = len(_args)
for i, (k, v) in enumerate(sig.parameters.items()):
if i &amp;lt; num_args:
named_args[k] = repr(_args[i])
else:
named_args[k] = repr(v.default)
name = func.__name__
result = repr(_result)
args_dict = {
**internal_locals,
**locals(),
**named_args
}
print(fmt.format(**args_dict))
# store the information in some place
return result
return decorated
return decorate
&lt;/code>&lt;/pre>
&lt;p>Veja o resultado em uma função bem simples&lt;/p>
&lt;pre>&lt;code class="language-python">@report(fmt='{name}(a={a}, b={b}, c={c}); sum_ab {sum_ab}, diff_ab {dif_ab}; r={result}')
def dummy_example(a, b, c=2):
sum_ab = a + b
dif_ab = a - b
r = sum_ab**c + dif_ab**c
return r
r = dummy_example(2, 3, 1)
print(&amp;quot;r:&amp;quot;, r)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code> dummy_example(a=2, b=3, c=1); sum_ab 5, diff_ab -1; r=4
r: 4
&lt;/code>&lt;/pre>
&lt;p>Eu sei que esse post pode ter sido difícil, se tiver dúvida você pode entrar em contato comigo pelos comentários abaixo, twitter ou linkedin. Compartilhe se você gostou.&lt;/p></description></item><item><title>An introspective python decorator using stack frames and the inspect module</title><link>/pt-br/post/python_decorator_that_exposes_locals/</link><pubDate>Mon, 04 Apr 2022 00:00:00 +0000</pubDate><guid>/pt-br/post/python_decorator_that_exposes_locals/</guid><description>&lt;details
class="toc-inpage d-print-none d-none d-sm-block d-md-none " open>
&lt;summary class="font-weight-bold">Lista de Conteúdos&lt;/summary>
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>
&lt;ul>
&lt;li>&lt;a href="#gaining-a-deeper-understanding-about-the-execution-context-of-a-function">Gaining a deeper understanding about the execution context of a function&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#the-fluent-python-book-example">The Fluent Python Book example&lt;/a>&lt;/li>
&lt;li>&lt;a href="#current-issues-and-limitations">Current issues and limitations&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#creating-an-introspective-code-with-the-inspect-module">Creating an introspective code with the inspect module&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#a-decorator-that-validates-arguments-using-mathematical-notation">A decorator that validates arguments using mathematical notation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#going-back-to-the-fluent-python-example">Going back to the Fluent python example&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#how-to-expose-the-locals-inside-of-a-decorator">How to expose the locals() inside of a decorator?&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#call-stack-and-frames-in-python">Call stack and frames in python&lt;/a>&lt;/li>
&lt;li>&lt;a href="#using-systrace-to-track-our-frames">Using sys.trace to track our frames&lt;/a>&lt;/li>
&lt;li>&lt;a href="#lets-solve-our-problem">Let&amp;rsquo;s solve our problem&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#conclusion-and-next-steps">Conclusion and next steps&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#it-depends">&amp;ldquo;&amp;hellip;it depends&amp;rdquo;&lt;/a>&lt;/li>
&lt;li>&lt;a href="#the-next-step-we-dont-need-a-trace-we-can-do-better-using-ast-manipulation">The next step: we don&amp;rsquo;t need a trace! We can do better using AST manipulation&lt;/a>&lt;/li>
&lt;li>&lt;a href="#simplenamespace-for-dictkey-instead-of-dictkey">SimpleNamespace for dict.key instead of dict[&amp;ldquo;key]&lt;/a>&lt;/li>
&lt;li>&lt;a href="#want-to-know-more-about-call-stack--inspect-and-trace">Want to know more about call stack , inspect and trace?&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/details>
&lt;p>
&lt;a href="https://www.amazon.com.br/Fluent-Python-Luciano-Ramalho/dp/1491946008" target="_blank" rel="noopener">Fluent Python&lt;/a> is the best resource to learn to use and love python. Some days ago I was reading a section of the chapter 7: &lt;em>&amp;ldquo;Function Decorators and Closures&lt;/em>&amp;rdquo;. This chapter has a lot of interesting and cool examples. Here I&amp;rsquo;ll discuss one of them and how I tried to put more shiny stuff in it.&lt;/p>
&lt;figure id="figure-a-book-that-every-python-programmer-should-read">
&lt;a data-fancybox="" href="/pt-br/post/python_decorator_that_exposes_locals/fluent_python_huae514437a1dc47e163345635da95e061_41082_0x200_resize_lanczos_3.png" data-caption="A book that every python programmer should read.">
&lt;img src="/pt-br/post/python_decorator_that_exposes_locals/fluent_python_huae514437a1dc47e163345635da95e061_41082_0x200_resize_lanczos_3.png" alt="" height="200px">
&lt;/a>
&lt;figcaption>
A book that every python programmer should read.
&lt;/figcaption>
&lt;/figure>
&lt;h2 id="gaining-a-deeper-understanding-about-the-execution-context-of-a-function">Gaining a deeper understanding about the execution context of a function&lt;/h2>
&lt;h3 id="the-fluent-python-book-example">The Fluent Python Book example&lt;/h3>
&lt;p>Ramalho’s book presents us with a &lt;code>@clock&lt;/code> decorator that can be used to decorate a method, measure the time it takes to execute, and print in a human-readable format the arguments and name of the method. The example is shown below:&lt;/p>
&lt;pre>&lt;code class="language-python">import time
DEFAULT_FMT = '[{elapsed:0.8f}s] {name}({args}) -&amp;gt; {result}'
def clock(fmt=DEFAULT_FMT):
def decorate(func):
def clocked(*_args):
t0 = time.time()
_result = func(*_args)
elapsed = time.time() - t0
name = func.__name__
args = ', '.join(repr(arg) for arg in _args)
result = repr(_result)
log_string = fmt.format(**locals())
# send to somewhere
# csv, ELK, etc
print(log_string)
return result
return clocked
return decorate
@clock('[{elapsed:0.8f}s] {name}({args})')
def snooze(seconds):
time.sleep(seconds)
return time.time()
for _ in range(3):
snooze(.123)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>[0.12315798s] snooze(0.123)
[0.12315822s] snooze(0.123)
[0.12317085s] snooze(0.123)
&lt;/code>&lt;/pre>
&lt;p>If you don&amp;rsquo;t understand something in the above code I recommend that you take some time searching and reading about each aspect. There are many cool things being used there, for example:&lt;/p>
&lt;ul>
&lt;li>&lt;code>repr&lt;/code> which is a function that returns a string representation of an object.
&lt;ul>
&lt;li>This is essential because the &lt;code>DEFAULT_FMT&lt;/code> is a string, not a &lt;code>f-string&lt;/code>, we can&amp;rsquo;t just put a generic object to be printed in &lt;code>DEFAULT_FMT&lt;/code>.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>log_string = fmt.format(**locals())&lt;/code>: instead of creating a repetitive code like &lt;code>fmt.format(**{&amp;quot;result&amp;quot;:result, &amp;quot;args&amp;quot;:args, ...})&lt;/code> we can just use the &lt;code>locals()&lt;/code> which is a dictionary that contains all the local variables of the current scope.&lt;/li>
&lt;/ul>
&lt;p>When I study something I always like to create a fresh problem with the stuff that I&amp;rsquo;ve learned and try to solve it. Sometimes there is no solution. But even if there is no solution, we still learn other stuff.&lt;/p>
&lt;p>I&amp;rsquo;ve started by creating the following example:&lt;/p>
&lt;pre>&lt;code class="language-python">import numpy as np
@clock('[{elapsed:0.8f}s] {name}({args})')
def snooze_and_snore(seconds, snore_loud, min_prob_to_snore=0.4):
time.sleep(seconds)
to_snore = np.random.uniform() &amp;gt; min_prob_to_snore
if to_snore:
if snore_loud:
pass
# r.requets(wake_up_everyone)
pass
return time.time()
for _ in range(3):
snooze_and_snore(.4, True, .1)
snooze_and_snore(.4, False, .1)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>[0.40229130s] snooze_and_snore(0.4, True, 0.1)
[0.40049720s] snooze_and_snore(0.4, False, 0.1)
[0.40058565s] snooze_and_snore(0.4, True, 0.1)
[0.40013075s] snooze_and_snore(0.4, False, 0.1)
[0.40052223s] snooze_and_snore(0.4, True, 0.1)
[0.40057564s] snooze_and_snore(0.4, False, 0.1)
&lt;/code>&lt;/pre>
&lt;p>Ok, what are the problems/issues/limitations that the above code showed me?&lt;/p>
&lt;h3 id="current-issues-and-limitations">Current issues and limitations&lt;/h3>
&lt;ol>
&lt;li>We don&amp;rsquo;t have information about the names of the arguments passed to the method.
&lt;ul>
&lt;li>If the list of arguments is long, trying to understand what is happening becomes a hard task. Because we are increasing the amount of stuff that we must keep in our mind. We are increasing the &lt;strong>cognitive load&lt;/strong> in the terms presented in the excelsior book:
&lt;a href="https://linghao.io/notes/a-philosophy-of-software-design" target="_blank" rel="noopener">A Philosophy of Software Design&lt;/a>.&lt;/li>
&lt;li>A person who is not familiar with the codebase cannot understand what is happening by analyzing the outputs of the decorator. If these outputs are being stored in the ELK stack, this will be unproductive.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>We have the &lt;code>locals()&lt;/code> information from the decorator which is fed by the result of the decorated method. However, we can&amp;rsquo;t get any information about the &lt;code>locals()&lt;/code> of the decorated method. Why is this bad?
&lt;ul>
&lt;li>The final internal state of the method is commonly used to understand the execution of a method.&lt;/li>
&lt;li>Sometimes a method depends on random variables defined in the local context. Thus, the same set of arguments can give different executions. Until now, we don&amp;rsquo;t have a way to get the &lt;code>locals()&lt;/code> of the decorated method. For example, in the &lt;code>snooze_and_snore&lt;/code> we can&amp;rsquo;t know if the person snored or not.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>We will attack the first issue using the inspect module. As I&amp;rsquo;ll show you, we can do cool things with this module.&lt;/p>
&lt;div class="alert alert-note">
&lt;div>
If you know about &lt;code>sys.trace&lt;/code>, &lt;code>call stack&lt;/code> and &lt;code>inspect.signatures&lt;/code> I recommend
you go directly to the section &lt;a href="#lets_solve_our_problem">Let&amp;rsquo;s solve our problem&lt;/a>
&lt;/div>
&lt;/div>
&lt;h2 id="creating-an-introspective-code-with-the-inspect-module">Creating an introspective code with the inspect module&lt;/h2>
&lt;p>The
&lt;a href="https://docs.python.org/3/library/inspect.html" target="_blank" rel="noopener">inspect&lt;/a> module is a Python standard library that provides several tools to help you to introspect and consequently learn about live objects like functions, modules, classes, instances, frame objects (I&amp;rsquo;ll talk about frames later in this post), etc. Well, what can you do with this? Really, a lot of things. You can use it to automatically create documentation, parse the docstrings, manipulate the AST, etc.&lt;/p>
&lt;h3 id="a-decorator-that-validates-arguments-using-mathematical-notation">A decorator that validates arguments using mathematical notation&lt;/h3>
&lt;p>In the last years, we have seen the development of the &lt;code>typing&lt;/code> module and the &lt;code>mypy&lt;/code> static analysis tool for python. This module and tool can be very useful sometimes. However, it doesn&amp;rsquo;t provide some features that are essential for proper validation. But at least in my experience creating code for my Ph.D., I usually don&amp;rsquo;t need so much sophisticated type theory and validation to be able to write a good code for a mathematical modeling tool. Most of the mathematical validation that I need is just checking if an argument still satisfies some constraints or lives in a proper subspace. If not, I need to raise an exception or perform some kind of regularization.&lt;/p>
&lt;p>Let&amp;rsquo;s create a decorator that will validate arguments using simple mathematical notation.&lt;/p>
&lt;p>We will create a dictionary that will contain the annotation as a key and the value will be a human-readable
description of the annotation and a method responsible for check if everything is right.&lt;/p>
&lt;pre>&lt;code class="language-python">import inspect
MATH_SPACES = {
&amp;quot;\in R&amp;quot;: {&amp;quot;name&amp;quot; : &amp;quot;real space&amp;quot;, &amp;quot;validator&amp;quot;: lambda x: isinstance(x, (int, float))},
&amp;quot;\in R_+&amp;quot;: {&amp;quot;name&amp;quot;: &amp;quot;space of real numbers greater than zero&amp;quot;, &amp;quot;validator&amp;quot;: lambda x: isinstance(x, (int, float)) and x &amp;gt; 0},
}
&lt;/code>&lt;/pre>
&lt;p>We will use the &lt;code>inspect.signature&lt;/code> to get the annotations of each argument of the decorated method.
For example, if the decorated method is &lt;code>def foo(a: '\in R', b)&lt;/code> the &lt;code>inspect.signature(foo)&lt;/code> will return an object which we can use to extract an ordered dictionary with the arguments and the annotations. Like this&lt;/p>
&lt;pre>&lt;code class="language-python">def foo(a: &amp;quot;\in R&amp;quot;, b, c:int, d= 2):
pass
for k, v in inspect.signature(foo).parameters.items():
print(k, v, type(v._annotation), v.default)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>a a: '\\in R' &amp;lt;class 'str'&amp;gt; &amp;lt;class 'inspect._empty'&amp;gt;
b b &amp;lt;class 'type'&amp;gt; &amp;lt;class 'inspect._empty'&amp;gt;
c c: int &amp;lt;class 'type'&amp;gt; &amp;lt;class 'inspect._empty'&amp;gt;
d d=2 &amp;lt;class 'type'&amp;gt; 2
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s create our decorator. It should be really simple. Just check if we should verify the argument and if so, check if the value respects the annotated mathematical space.&lt;/p>
&lt;pre>&lt;code class="language-python">def math_validator():
def decorate(func):
def decorated(*_args):
sig = inspect.signature(func)
# sig parameters is an ordered dict
for i, (k, v) in enumerate(sig.parameters.items()):
annotation = v._annotation
if not isinstance(annotation, str):
continue
if not annotation in MATH_SPACES:
print(f&amp;quot;{annotation} is not implemented in Math Spaces&amp;quot;)
continue # skip if we didn't implement this space validation
if not MATH_SPACES[annotation][&amp;quot;validator&amp;quot;](_args[i]):
raise ValueError(f&amp;quot;{k} doesn't belong to the {MATH_SPACES[annotation]['name']}&amp;quot;)
result = func(*_args)
print(f&amp;quot;{func.__name__}({_args}) -&amp;gt; {result}&amp;quot;)
return result
return decorated
return decorate
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">@math_validator()
def simple_method(x: &amp;quot;\in R&amp;quot;, y: &amp;quot;\in R_+&amp;quot;, z: float = 2) -&amp;gt; float:
&amp;quot;&amp;quot;&amp;quot;Simple method to add two numbers together and
divide by the last number
Args:
x: The first number to add.
y: The second number to add.
z: it is a float number that will be the power of the result.
This will not be checked for math spaces.
Returns:
float: result
&amp;quot;&amp;quot;&amp;quot;
result = (x+y)/y
return result**z
simple_method(1, 2)
simple_method(1, 0)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>simple_method((1, 2)) -&amp;gt; 1.5
---&amp;gt; 19 simple_method(1, 0)
...
&amp;lt;locals&amp;gt;.decorate.&amp;lt;locals&amp;gt;.decorated(*_args)
11 continue
13 if not MATH_SPACES[annotation][&amp;quot;validator&amp;quot;](_args[i]):
---&amp;gt; 14 raise ValueError(f&amp;quot;{k} doesn't belong to the {MATH_SPACES[annotation]['name']}&amp;quot;)
15 result = func(*_args)
16 print(f&amp;quot;{func.__name__}({_args}) -&amp;gt; {result}&amp;quot;)
ValueError: y doesn't belong to the space of real numbers greater than zero
&lt;/code>&lt;/pre>
&lt;p>Our decorator is quite simple but does the job. You can go deeper into this and use a more sophisticated mathematical notation, printing using latex, etc. But now, let&amp;rsquo;s go back to the Python Fluent example because the &lt;code>inspect.signature&lt;/code> already provides us with a way to solve the first limitation!&lt;/p>
&lt;h3 id="going-back-to-the-fluent-python-example">Going back to the Fluent python example&lt;/h3>
&lt;p>Let&amp;rsquo;s remember one thing that I&amp;rsquo;ve pointed out:&lt;/p>
&lt;blockquote>
&lt;p>A person who is not familiar with the code base will not be able to understand what is happening just by analyzing the outputs of the decorator.&lt;/p>
&lt;/blockquote>
&lt;p>It&amp;rsquo;s obvious that we can overcome this issue by using the &lt;code>inspect&lt;/code> module. Let&amp;rsquo;s create a more elaborated example using monkeys and a zookeeper that must record and report the information about how the life of the monkeys are going.&lt;/p>
&lt;pre>&lt;code class="language-python">NUM_MONKEYS = 20
def feed_monkeys(n_bananas, n_apples=0):
monkeys = {
f&amp;quot;monkey_{i}&amp;quot;: {&amp;quot;bananas&amp;quot;: 0, &amp;quot;apples&amp;quot;: 0}
for i in range(NUM_MONKEYS)
}
while n_bananas &amp;gt; 0 and n_apples &amp;gt; 0:
if np.random.uniform() &amp;lt; 0.4:
continue
monkey = monkey[np.random.choice(list(monkeys.keys()))]
if n_bananas &amp;gt; 0:
monkey[&amp;quot;bananas&amp;quot;] += 1
n_bananas -= 1
if n_apples &amp;gt; 0:
monkey[&amp;quot;apples&amp;quot;] += 1
n_apples -= 1
if n_apples == 0 and n_bananas == 0:
break
&lt;/code>&lt;/pre>
&lt;p>My solution is the &lt;code>@report&lt;/code> decorator presented below.&lt;/p>
&lt;pre>&lt;code class="language-python">def report(fmt=DEFAULT_FMT):
def decorate(func):
def decorated(*_args):
sig = inspect.signature(func)
named_args = {}
num_args = len(_args)
for i, (k, v) in enumerate(sig.parameters.items()):
if i &amp;lt; num_args:
named_args[k] = repr(_args[i])
else:
named_args[k] = repr(v.default)
t0 = time.time()
_result = func(*_args)
elapsed = time.time() - t0
name = func.__name__
result = repr(_result)
args_dict = {
**locals(),
**named_args}
del args_dict['_args']
print(fmt.format(**args_dict))
# store the information in some place
return result
return decorated
return decorate
&lt;/code>&lt;/pre>
&lt;p>What is important here are the following statements:&lt;/p>
&lt;pre>&lt;code class="language-python">sig = inspect.signature(func)
named_args = {}
num_args = len(_args)
for i, (k, v) in enumerate(sig.parameters.items()):
if i &amp;lt; num_args:
named_args[k] = repr(_args[i])
else:
named_args[k] = repr(v.default)
&lt;/code>&lt;/pre>
&lt;p>We are iterating over the signature parameters and checking if it passed the value to &lt;code>func&lt;/code>. If not, we extract the default value from the signature.&lt;/p>
&lt;p>Using the &lt;code>@report&lt;/code> decorator in the &lt;code>feed_monkeys&lt;/code> we have this output:&lt;/p>
&lt;pre>&lt;code class="language-python">NUM_MONKEYS = 20
@report('The zookeeper feeds the monkeys with {n_bananas} bananas and {n_apples} apples. Time to feed: {elapsed:0.4f}s')
def feed_monkeys(n_bananas, n_apples=0):
monkeys = {
f&amp;quot;monkey_{i}&amp;quot;: {&amp;quot;bananas&amp;quot;: 0, &amp;quot;apples&amp;quot;: 0}
for i in range(NUM_MONKEYS)
}
while n_bananas &amp;gt; 0 and n_apples &amp;gt; 0:
if np.random.uniform() &amp;lt; 0.4:
continue
monkey = monkeys[np.random.choice(list(monkeys.keys()))]
if n_bananas &amp;gt; 0:
monkey[&amp;quot;bananas&amp;quot;] += 1
n_bananas -= 1
if n_apples &amp;gt; 0:
monkey[&amp;quot;apples&amp;quot;] += 1
n_apples -= 1
if n_apples == 0 and n_bananas == 0:
break
for _ in range(3):
feed_monkeys(np.random.randint(10, 100))
feed_monkeys(np.random.randint(10, 100), 10)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>The zookeeper feeds the monkeys with 69 bananas and 0 apples. Time to feed: 0.0000s
The zookeeper feeds the monkeys with 92 bananas and 10 apples. Time to feed: 0.0011s
The zookeeper feeds the monkeys with 58 bananas and 0 apples. Time to feed: 0.0000s
The zookeeper feeds the monkeys with 53 bananas and 10 apples. Time to feed: 0.0048s
The zookeeper feeds the monkeys with 42 bananas and 0 apples. Time to feed: 0.0000s
The zookeeper feeds the monkeys with 51 bananas and 10 apples. Time to feed: 0.0025s
&lt;/code>&lt;/pre>
&lt;p>First issue solved! But our decorator is still not useful to the zookeeper and managers. We can’t know how good any monkey is doing or if there is any monkey that eats too much. You could already know that somehow we must have a way to access the monkeys' dictionary inside our &lt;code>def decorated&lt;/code> method. Unfortunately, this is not a trivial task in python because it lacks namespaces decorators. But we also can overcome this with a little trick using a trace tool.&lt;/p>
&lt;h2 id="how-to-expose-the-locals-inside-of-a-decorator">How to expose the locals() inside of a decorator?&lt;/h2>
&lt;p>Now we just need to access the local variables of the decorated method. Let&amp;rsquo;s think more deeply about this:&lt;/p>
&lt;ul>
&lt;li>After the execution of the decorated method, all the information about the local variables is lost. Fortunately, we don&amp;rsquo;t want irrelevant information occupying our system memory.&lt;/li>
&lt;li>The decorator will call the decorated method and will receive the return value. Thus, &lt;strong>there is no way to extract the local variables because now there are no more local variables!&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>How to solve it? Well, think first about where the local variables have been stored before being erased.&lt;/p>
&lt;h3 id="call-stack-and-frames-in-python">Call stack and frames in python&lt;/h3>
&lt;p>If you came from a non-CS background, maybe you don&amp;rsquo;t know about an important concept called the
&lt;a href="https://en.wikipedia.org/wiki/Call_stack" target="_blank" rel="noopener">&lt;strong>call stack&lt;/strong>&lt;/a>. A call stack is a data structure that stores information related to living things in our program.&lt;/p>
&lt;p>If you call a function in python, a new block of information (&lt;strong>frame&lt;/strong>) is pushed to the top of the call stack. After the function returns the value, this block of information is popped off the call stack. This comprehension can give insights into how to do things in python and how to create good or strange behaviors.&lt;/p>
&lt;p>Well, you can think. If the elements of the call stack are always added on the top if a function (inner) is called by another function (outer) &lt;strong>can I access the values of the local variables from the outer function inside of the inner? Yes, you can!&lt;/strong> Obviously, this is not always a good idea, but it&amp;rsquo;s good to understand this concept. Because this approach can be useful to deal with rigid frameworks like Django.&lt;/p>
&lt;pre>&lt;code class="language-python">%%writefile test_stack.py
import inspect
N_BANANAS = 12
def outer_call(n_bananas):
var_inside_outer_call = 2
n_bananas += 1
inner_call(n_bananas)
def inner_call(n_bananas):
var_inside_inner_call = {&amp;quot;monkey&amp;quot;: 0}
frame_infos = inspect.stack()
n_frames = len(frame_infos)
frames_var_values = {
f.function: [(k, v) for k, v in f.frame.f_locals.items()] for f in frame_infos
}
for i, (function, frame_local) in enumerate(frames_var_values.items()):
print(f'\n\t {function} stack position: {n_frames - i}')
for var_name, value in frame_local:
print(f'\t\t Name: {var_name:25s}Type: {type(value)}')
if var_name in ('n_bananas', 'N_BANANAS', 'var_inside_outer_call'):
print(f'\t\t\t Value: {value}')
print(&amp;quot;\n Before outer_call() call&amp;quot;)
outer_call(N_BANANAS)
print(&amp;quot;\n After outer_call() call&amp;quot;)
frames = [
[(k, v) for k, v in f.frame.f_locals.items()]
for f in inspect.stack()
]
for frame_local in frames:
for var_name, value in frame_local:
print(f'\t\t Name: {var_name:25s}Type: {type(value)}')
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>Overwriting test_stack.py
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">!python test_stack.py
&lt;/code>&lt;/pre>
&lt;pre>&lt;code> Before outer_call() call
inner_call stack position: 3
Name: n_bananas Type: &amp;lt;class 'int'&amp;gt;
Value: 13
Name: var_inside_inner_call Type: &amp;lt;class 'dict'&amp;gt;
Name: frame_infos Type: &amp;lt;class 'list'&amp;gt;
Name: n_frames Type: &amp;lt;class 'int'&amp;gt;
outer_call stack position: 2
Name: n_bananas Type: &amp;lt;class 'int'&amp;gt;
Value: 13
Name: var_inside_outer_call Type: &amp;lt;class 'int'&amp;gt;
Value: 2
&amp;lt;module&amp;gt; stack position: 1
Name: __name__ Type: &amp;lt;class 'str'&amp;gt;
Name: __doc__ Type: &amp;lt;class 'NoneType'&amp;gt;
Name: __package__ Type: &amp;lt;class 'NoneType'&amp;gt;
Name: __loader__ Type: &amp;lt;class '_frozen_importlib_external.SourceFileLoader'&amp;gt;
Name: __spec__ Type: &amp;lt;class 'NoneType'&amp;gt;
Name: __annotations__ Type: &amp;lt;class 'dict'&amp;gt;
Name: __builtins__ Type: &amp;lt;class 'module'&amp;gt;
Name: __file__ Type: &amp;lt;class 'str'&amp;gt;
Name: __cached__ Type: &amp;lt;class 'NoneType'&amp;gt;
Name: inspect Type: &amp;lt;class 'module'&amp;gt;
Name: N_BANANAS Type: &amp;lt;class 'int'&amp;gt;
Value: 12
Name: outer_call Type: &amp;lt;class 'function'&amp;gt;
Name: inner_call Type: &amp;lt;class 'function'&amp;gt;
After outer_call() call
Name: __name__ Type: &amp;lt;class 'str'&amp;gt;
Name: __doc__ Type: &amp;lt;class 'NoneType'&amp;gt;
Name: __package__ Type: &amp;lt;class 'NoneType'&amp;gt;
Name: __loader__ Type: &amp;lt;class '_frozen_importlib_external.SourceFileLoader'&amp;gt;
Name: __spec__ Type: &amp;lt;class 'NoneType'&amp;gt;
Name: __annotations__ Type: &amp;lt;class 'dict'&amp;gt;
Name: __builtins__ Type: &amp;lt;class 'module'&amp;gt;
Name: __file__ Type: &amp;lt;class 'str'&amp;gt;
Name: __cached__ Type: &amp;lt;class 'NoneType'&amp;gt;
Name: inspect Type: &amp;lt;class 'module'&amp;gt;
Name: N_BANANAS Type: &amp;lt;class 'int'&amp;gt;
Name: outer_call Type: &amp;lt;class 'function'&amp;gt;
Name: inner_call Type: &amp;lt;class 'function'&amp;gt;
&lt;/code>&lt;/pre>
&lt;p>First, draw your attention here&lt;/p>
&lt;pre>&lt;code>outer_call stack position: 2
Name: n_bananas Type: &amp;lt;class 'int'&amp;gt;
Value: 13
Name: var_inside_outer_call Type: &amp;lt;class 'int'&amp;gt;
Value: 2
&lt;/code>&lt;/pre>
&lt;p>&lt;strong>Even if we don&amp;rsquo;t pass a variable as an argument to the &lt;code>inner_call&lt;/code> function, this variable can be accessed because still lives in the call stack!&lt;/strong> As I’ve told you, after the execution of &lt;code>outer_call&lt;/code> the call stack doesn&amp;rsquo;t have any information about what happened inside our functions. This discussion will help us to understand the limitations of our solution. Because &lt;strong>our solution is just to watch the call stack and keep the frame before being popped off!&lt;/strong>&lt;/p>
&lt;h3 id="using-systrace-to-track-our-frames">Using sys.trace to track our frames&lt;/h3>
&lt;p>Some time ago I&amp;rsquo;ve talked about how to dissect a process using &lt;code>lsof&lt;/code> and &lt;code>strace&lt;/code>:
&lt;a href="https://medium.com/@devmessias/dissecting-process-and-failures-in-linux-with-lsof-and-strace-cases-for-mlops-d7755b2ce6ca" target="_blank" rel="noopener">Dissecting processes and failures in Linux with lsof and strace&lt;/a>. The &lt;code>strace&lt;/code> is a tracing tool that intercepts and records in someplace any system call made by a process. Python has a built-in tool to do this kind of stuff. Thus, let&amp;rsquo;s use it to track our frames.&lt;/p>
&lt;h3 id="lets-solve-our-problem">Let&amp;rsquo;s solve our problem&lt;/h3>
&lt;p>We will ask our code to monitor any call made with the decorated function. To do so, we will create a new function that will do this and release the trace after the execution of the decorated function.&lt;/p>
&lt;pre>&lt;code class="language-python">import sys
def call_and_extract_frame(func, *args, **kwargs):
frame_var = None
trace = sys.gettrace()
def update_frame_var(stack_frame, event_name, arg_frame):
&amp;quot;&amp;quot;&amp;quot;
Args:
stack_frame: (frame)
The current stack frame.
event_name: (str)
The name of the event that triggered the call.
Can be 'call', 'line', 'return' and 'exception'.
arg_frame:
Depends on the event. Can be a None type
&amp;quot;&amp;quot;&amp;quot;
nonlocal frame_var # nonlocal is a keyword which allows us to change the variable in the outer scope
if event_name != 'call':
return trace
frame_var = stack_frame
sys.settrace(trace)
return trace
sys.settrace(update_frame_var)
try:
func_result = func(*args, **kwargs)
finally:
sys.settrace(trace)
return frame_var, func_result
&lt;/code>&lt;/pre>
&lt;p>Now to use this trick, we just need to call the above function in our &lt;code>@report&lt;/code> decorator. Like this:&lt;/p>
&lt;pre>&lt;code class="language-python">def report(formater):
def decorate(func):
def decorated(*_args):
sig = inspect.signature(func)
named_args = {}
num_args = len(_args)
for i, (k, v) in enumerate(sig.parameters.items()):
if i &amp;lt; num_args:
named_args[k] = repr(_args[i])
else:
named_args[k] = repr(v.default)
### Our modifications
frame_func, _result = call_and_extract_frame(func, *_args)
name = func.__name__
result = repr(_result)
args_dict = {
**named_args,
**locals(),
**frame_func.f_locals,
}
###
print(formater.format(**args_dict))
# do other stuff here
return _result
return decorated
return decorate
&lt;/code>&lt;/pre>
&lt;p>Let&amp;rsquo;s see the results:&lt;/p>
&lt;pre>&lt;code class="language-python">@report(' Monkey {gluttonous_monkey} ate too much bananas. Num monkeys {num_monkeys}')
def feed_monkeys(n_bananas):
num_monkeys = 3
monkeys = {
f&amp;quot;monkey_{i}&amp;quot;: {&amp;quot;bananas&amp;quot;: 0}
for i in range(num_monkeys)
}
while n_bananas &amp;gt; 0:
if np.random.uniform() &amp;lt; 0.4:
continue
monkey = monkeys[np.random.choice(list(monkeys.keys()))]
if n_bananas &amp;gt; 0:
monkey[&amp;quot;bananas&amp;quot;] += 1
n_bananas -= 1
gluttonous_monkey = max(monkeys, key=lambda k: monkeys[k][&amp;quot;bananas&amp;quot;])
for _ in range(3):
feed_monkeys(np.random.randint(10, 100))
&lt;/code>&lt;/pre>
&lt;pre>&lt;code> The monkey monkey_0 eat too much bananas. Num monkeys 3
The monkey monkey_1 eat too much bananas. Num monkeys 3
The monkey monkey_2 eat too much bananas. Num monkeys 3
&lt;/code>&lt;/pre>
&lt;h2 id="conclusion-and-next-steps">Conclusion and next steps&lt;/h2>
&lt;h3 id="it-depends">&amp;ldquo;&amp;hellip;it depends&amp;rdquo;&lt;/h3>
&lt;p>Nice! It worked. But should you use it?&lt;/p>
&lt;figure >
&lt;a data-fancybox="" href="/pt-br/post/python_decorator_that_exposes_locals/depends_hue4832d1f9bd8c3212ee44b9859787ce4_80720_0x400_resize_q90_lanczos.jpg" >
&lt;img src="/pt-br/post/python_decorator_that_exposes_locals/depends_hue4832d1f9bd8c3212ee44b9859787ce4_80720_0x400_resize_q90_lanczos.jpg" alt="" height="400px">
&lt;/a>
&lt;/figure>
&lt;ul>
&lt;li>We have drawbacks in our approach:
&lt;ul>
&lt;li>a tracing always creates a cost. Thus, is expected that we will reduce the performance of our system. If you use this just for debugging purposes, it&amp;rsquo;s ok.&lt;/li>
&lt;li>can have conflicts with other tools and libs that also trying to use the trace tool&lt;/li>
&lt;li>it seems dirty!&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="the-next-step-we-dont-need-a-trace-we-can-do-better-using-ast-manipulation">The next step: we don&amp;rsquo;t need a trace! We can do better using AST manipulation&lt;/h3>
&lt;ul>
&lt;li>Using the inspect module to get the argument names it&amp;rsquo;s ok but I&amp;rsquo;ve told you the trace tool can be problematic. But we can replace the trace with another approach. Although, it&amp;rsquo;s more conceptually complex don&amp;rsquo;t require dirty tricks and I believe it&amp;rsquo;s far more beautiful. &lt;strong>The next post it&amp;rsquo;s about this!&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h3 id="simplenamespace-for-dictkey-instead-of-dictkey">SimpleNamespace for dict.key instead of dict[&amp;ldquo;key]&lt;/h3>
&lt;p>We have a minor issue and point of improvement. If you&amp;rsquo;re an cautious developer, probably you notice a flaw here&lt;/p>
&lt;pre>&lt;code class="language-python">args_dict = {
**named_args,
**locals(),
**frame_func.f_locals,
}
&lt;/code>&lt;/pre>
&lt;p>if any of the dicts have common keys, one of them will overwrite the other. This is not what we want. You can use a simple solution like this:&lt;/p>
&lt;pre>&lt;code class="language-python">args_dict = {
&amp;quot;args&amp;quot;: **named_args,
**locals(),
&amp;quot;func_locals&amp;quot;: **frame_func.f_locals,
}
&lt;/code>&lt;/pre>
&lt;p>But this is still annoying because we can do this with a format string:&lt;/p>
&lt;pre>&lt;code>@report(fmt=&amp;quot;{args['n_bananas']} ...&amp;quot;)
&lt;/code>&lt;/pre>
&lt;p>Well, how to solve it? Just use a SimpleNamespace to construct an object!&lt;/p>
&lt;pre>&lt;code class="language-python">from types import SimpleNamespace
def report(formater):
def decorate(func):
def decorated(*_args):
sig = inspect.signature(func)
named_args = {}
num_args = len(_args)
for i, (k, v) in enumerate(sig.parameters.items()):
if i &amp;lt; num_args:
named_args[k] = repr(_args[i])
else:
named_args[k] = repr(v.default)
### Our modifications
frame_func, _result = call_and_extract_frame(func, *_args)
name = func.__name__
result = repr(_result)
args_dict = {
&amp;quot;args&amp;quot;: SimpleNamespace(**named_args),
&amp;quot;args_repr&amp;quot;: repr(SimpleNamespace(**named_args)),
**locals(),
**frame_func.f_locals,
}
###
print(formater.format(**args_dict))
# do other stuff here
return _result
return decorated
return decorate
@report(
&amp;quot;&amp;quot;.join((
'The zookeeper feeds the monkeys with {args.n_bananas},',
'bananas. We loost {n_bananas} bananas. Args {args_repr}'
))
)
def feed_monkeys(n_bananas):
num_monkeys = 3
monkeys = {
f&amp;quot;monkey_{i}&amp;quot;: {&amp;quot;bananas&amp;quot;: 0}
for i in range(num_monkeys)
}
while n_bananas &amp;gt; 0:
if np.random.uniform() &amp;gt; .8:
# &amp;quot;bananas rotted . Monkeys will not eat any banana any more&amp;quot;)
break
if np.random.uniform() &amp;lt; 0.4:
continue
monkey = monkeys[np.random.choice(list(monkeys.keys()))]
if n_bananas &amp;gt; 0:
monkey[&amp;quot;bananas&amp;quot;] += 1
n_bananas -= 1
gluttonous_monkey = max(monkeys, key=lambda k: monkeys[k][&amp;quot;bananas&amp;quot;])
for _ in range(3):
feed_monkeys(np.random.randint(10, 100))
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>The zookeeper feeds the monkeys with 15,bananas. We loost 15 bananas. Args namespace(n_bananas='15')
The zookeeper feeds the monkeys with 80,bananas. We loost 77 bananas. Args namespace(n_bananas='80')
The zookeeper feeds the monkeys with 95,bananas. We loost 92 bananas. Args namespace(n_bananas='95')
&lt;/code>&lt;/pre>
&lt;h3 id="want-to-know-more-about-call-stack--inspect-and-trace">Want to know more about call stack , inspect and trace?&lt;/h3>
&lt;ul>
&lt;li>Call stack and frames:
&lt;a href="https://www.linkedin.com/in/reza-bagheri-71882a76/" target="_blank" rel="noopener">Reza Bagheri&lt;/a> explained
&lt;a href="https://reza-bagheri79.medium.com/python-stack-frames-and-tail-call-optimization-4d0ea55b0542" target="_blank" rel="noopener">here&lt;/a> how to add a tail-call optimization in python using python stack frames.&lt;/li>
&lt;li>Fluent Python book by Luciano Ramalho&lt;/li>
&lt;li>Python documentation:
&lt;a href="https://docs.python.org/3/library/traceback.html" target="_blank" rel="noopener">tracebak&lt;/a>,
&lt;a href="https://docs.python.org/3/library/inspect.html" target="_blank" rel="noopener">inspect and stack&lt;/a>.&lt;/li>
&lt;li>
&lt;a href="https://stackoverflow.com/questions/4214936/how-can-i-get-the-values-of-the-locals-of-a-function-after-it-has-been-executed/4249347#4249347" target="_blank" rel="noopener">Stackoverflow discussion&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Variações do teorema central do limite para matrizes aleatórias.</title><link>/pt-br/post/random_matrix_portfolio/</link><pubDate>Mon, 06 Dec 2021 00:00:00 +0000</pubDate><guid>/pt-br/post/random_matrix_portfolio/</guid><description>&lt;blockquote>
&lt;p>Disponível em
&lt;a href="https://opencodecom.net/post/2021-12-14-variacoes-do-teorema-central-do-limite-para-matrizes-aleatorias-de-nucleos-atomicos-a-filtragem-de-matrizes-de-correlaca/" target="_blank" rel="noopener">https://opencodecom.net/&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;p>No célebre trabalho “&lt;em>Can One Hear the Shape of a Drum?&lt;/em>”[1] Kack questiona se conhecendo o espectro (&lt;em>som&lt;/em>) de um certo operador que define as oscilações de uma membrana (&lt;em>tambor&lt;/em>) seria possível identificar o formato de tal membrana de maneira unívoca. Discutiremos aqui como é possível ouvir matrizes de correlação usando seu espectro e como podemos remover o ruído desse som usando resultados da teoria de matrizes aleatórias. Veremos como essa filtragem pode aprimorar algoritmos de construção de carteiras de investimentos.&lt;/p>
&lt;blockquote>
&lt;p>Minhas motivações para escrever esse texto foram o movimento
&lt;a href="https://twitter.com/sseraphini/status/1458169250326142978" target="_blank" rel="noopener">Learn In Public-Sibelius Seraphini&lt;/a> e o Nobel de Física de 2021. Um dos temas de Giorgio Parisi é o estudo de matrizes aleatórias
&lt;a href="https://www.nobelprize.org/uploads/2021/10/sciback_fy_en_21.pdf" target="_blank" rel="noopener">www.nobelprize.org 2021&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;p>..&lt;/p>
&lt;blockquote>
&lt;p>Jupyter notebook disponível
&lt;a href="https://github.com/devmessias/devmessias.github.io/blob/master/content/post/random_matrix_portfolio/index.ipynb" target="_blank" rel="noopener">aqui&lt;/a>&lt;/p>
&lt;/blockquote>
&lt;h1 id="1-introdução-teorema-central-do-limite">1-Introdução: teorema central do limite&lt;/h1>
&lt;p>O teorema central do limite está no coração da análise estatística. Em poucas palavras o mesmo estabelece o seguinte.&lt;/p>
&lt;blockquote>
&lt;p>Suponha uma amostra $A = (x_1, x_2, \dots, x_n)$ de uma variável aleatória com média $\mu$ e variância $\sigma^2$ finita. Se a amostragem é $i.i.d.$ o teorema central do limite estabelece que a
distribuição de probababilidade da média amostral converge
para uma distribuição normal com variância $\sigma^2/n$ e média $\mu$ a medida que $n$ aumenta.&lt;/p>
&lt;/blockquote>
&lt;p>Note que eu não disse nada a respeito de como tal amostra foi gerada; em nenhum momento citei distribuição de Bernoulli, Gauss, Poisson, etc. Desta maneira podemos dizer que tal convergência é uma propriedade &lt;strong>universal&lt;/strong> de amostras aleatórias $i.i.d.$. Essa universalidade é poderosa, pois garante que é possível estimar a média e variância de uma população através de um conjunto de amostragens.&lt;/p>
&lt;p>Não é difícil fazer um experimento computacional onde a implicação desse teorema apareça&lt;/p>
&lt;pre>&lt;code class="language-python">import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import warnings
from matplotlib import style
warnings.filterwarnings('ignore')
style.use('seaborn-white')
np.random.seed(22)
&lt;/code>&lt;/pre>
&lt;p>Usaremos uma amostragem de uma distribuição exponencial com média $\mu = 4$. Tal distribuição tem uma variância dada por $1/\mu^2$. Faremos $10000$ experimentos com amostras de tamanho $500$. Posteriormente calcularemos a media de cada experimento, &lt;code>mean_by_exp&lt;/code>&lt;/p>
&lt;pre>&lt;code class="language-python">rate = 0.25
mu = 1/rate
sample_size=500
exponential_sample = np.random.exponential(mu, size=(sample_size, 30000))
mean_by_exp = exponential_sample.mean(axis=0)
&lt;/code>&lt;/pre>
&lt;p>Agora basta plotar o histograma em comparação com a distribuição normal dada pelo teorema central do limite&lt;/p>
&lt;pre>&lt;code class="language-python">sns.distplot(mean_by_exp, norm_hist=True, label='sample')
x = np.linspace(2.5, 5.5, 100)
var = mu**2/(sample_size)
y = np.exp(-(x-mu)**2/(2*var))/np.sqrt(2*np.pi*var)
plt.plot(x, y, label=r'$N(\mu, \sigma)$', c='tomato')
plt.legend()
plt.xlim(3., 5)
plt.savefig('exponential_distribution.png', facecolor='w')
plt.close()
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="exponential_distribution.png" alt="&amp;ldquo;exponential_distribution.png&amp;rdquo;">&lt;/p>
&lt;p>Note na figura acima que o plot para a função $\frac{e^{-\frac{(x-\mu)^2}{2\sigma^2}}}{\sqrt(2\pi\sigma^2)}$ e o histograma coincidem. Você pode testar essa coincidência com outras distribuições, o mesmo comportamento se repetira. É isso que quero dizer com &lt;strong>universalidade&lt;/strong>.&lt;/p>
&lt;p>Um questionamento válido é que estamos tratando apenas de uma variável aleatória e sua amostragem. Mas no mundo real existem outras estruturas mais intricadas. Por exemplo
pegue um conjunto de variáveis aleatórias
$\mathcal C=(X_{1 1}, X_{1 2}, \cdots, X_{N N})$, suponha que exista uma certa &lt;strong>simetria&lt;/strong> nesse conjunto, uma possibilidade é $X_{i j} = X_{j i}$.
Não é difícil imaginar situações onde tal conjunto apareça.&lt;/p>
&lt;p>Podemos armazenar uma realização de $\mathcal C$ em uma matriz que nada mais é que um grafo completo com pesos. Ao estudar essas matrizes oriundas desse tipo de amostragem entramos em um novo campo da matemática, o campo das matrizes aleatórias.
Nesse campo de estudos uma amostragem não retorna um número, mas sim uma matriz.&lt;/p>
&lt;p>A função &lt;code>normalRMT&lt;/code> apresentada abaixo é um gerador de matrizes aleatórias conhecidas como Gaussianas ortogonais.&lt;/p>
&lt;pre>&lt;code class="language-python">def normalRMT(n=100):
&amp;quot;&amp;quot;&amp;quot;Generate a random matrix with normal distribution entries
Args:
n : (int) number of rows and columns
Returns:
m : (numpy.ndarray) random matrix
&amp;quot;&amp;quot;&amp;quot;
std = 1/np.sqrt(2)
m = np.random.normal(size=(n,n), scale=std)
m = (m+m.T)
m /= np.sqrt(n)
return m
np.set_printoptions(precision=3)
print(f'{normalRMT(3)},\n\n{normalRMT(3)}')
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>[[-1.441e+00 -2.585e-01 -1.349e-01]
[-2.585e-01 -2.304e-01 1.166e-03]
[-1.349e-01 1.166e-03 -1.272e+00]],
[[-0.742 0.607 -0.34 ]
[ 0.607 0.678 0.277]
[-0.34 0.277 -0.127]]
&lt;/code>&lt;/pre>
&lt;p>Sabemos que quando estamos trantando de variáveis aleatórias o teorema central do limite é importantíssimo. O que você pode se perguntar agora é: &lt;strong>Existe um análogo para o teorema central do limite para matrizes aleatórias?&lt;/strong>&lt;/p>
&lt;h1 id="2-núcleos-atômicos-gás-de-números-primos-e-universalidade">2-Núcleos atômicos, gás de números primos e universalidade&lt;/h1>
&lt;p>Para o bem e para o mal o conhecimento da física atômica foi um dos temas mais importantes desenvolvidos pela humanidade. Portanto, não é de se estranhar que após o ano de 1930 iniciou-se uma grande corrida para compreender núcleos atômicos pesados e a física de nêutrons [13].&lt;/p>
&lt;p>Para compreender essa nova física de nêutrons era necessário conhecer a organização do espectro de ressonância dos núcleos pesados (esse espectro nada mais é que os autovalores de um operador muito especial). Uma maneira de se fazer isso é do jeito que muitas das coisas são estudadas na física: pegando se uma coisa e jogando na direção da coisa a ser estudada. Essa metodologia experimental torna possível amostrar alguns valores possíveis para o espectro. Contudo, acredito que não preciso argumentar que fazer isso naquela época era extremamente difícil e caro. Poucos centros conseguiam realizar alguns experimentos e ainda com uma resolução muito baixa para obter resultados suficientes para uma compreensão adequada dos núcleos. Era preciso uma saída mais barata e ela foi encontrada. Tal saída dependeu apenas de física-matemática e maços de papel.&lt;/p>
&lt;p>&lt;img src="frog.png" alt="">&lt;/p>
&lt;p>Dentre os pioneiros que decidiram atacar o problema de núcleos pesados usando matemática temos Eugene Paul Wigner (Nobel de 1963). A grande sacada de Wigner foi perceber que o fato das interações nucleares serem tão complicadas e a infinitude de graus de liberdade seria possível tentar compreender essas interações como uma amostragem sujeita a certas condições de simetria.[10 , 11]&lt;/p>
&lt;p>&lt;img src="wigner.png" alt="wigner.png">&lt;/p>
&lt;p>Aqui com simetria queremos dizer que as matrizes envolvidas possuem certas restrições tais como&lt;/p>
&lt;pre>&lt;code class="language-python">np.assert_equal(A, A.T)
&lt;/code>&lt;/pre>
&lt;p>Na próxima seção veremos qual o impacto dessas restrições na distribuição de autovalores das matrizes envolvidas.&lt;/p>
&lt;h2 id="2-a-universalidade-e-lei-do---semicírculo">2-a) Universalidade e lei do semicírculo&lt;/h2>
&lt;p>A função &lt;code>normalRMT&lt;/code> gera uma matriz simétrica onde as entradas são extraídas de uma distribuição normal. A função &lt;code>laplaceRMT&lt;/code> gera também uma matriz simétrica, contudo as entradas são amostras de uma distribuição de Laplace.&lt;/p>
&lt;pre>&lt;code class="language-python">
def laplaceRMT(n=100):
&amp;quot;&amp;quot;&amp;quot;Generate a random matrix with Laplace distribution
Args:
n : (int) size of the matrix
Returns:
m : (numpy.ndarray) random matrix with Laplace distribution
&amp;quot;&amp;quot;&amp;quot;
# we know that the variance of the laplace distribution is 2*scale**2
scale = 1/np.sqrt(2)
m = np.zeros((n,n))
values = np.random.laplace(size=n*(n-1)//2, scale=scale)
m[np.triu_indices_from(m, k=1)] = values
# copy the upper diagonal to the lower diagonal
m[np.tril_indices_from(m, k=-1)] = values
np.fill_diagonal(m, np.random.laplace(size=n, scale=scale))
m = m/np.sqrt(n)
return m
&lt;/code>&lt;/pre>
&lt;p>As propriedades &lt;strong>universais&lt;/strong> que iremos explorar aqui estão ligadas aos autovalores das matrizes que foram amostradas. Como nossas matrizes são simétricas esses autovalores são todos reais.&lt;/p>
&lt;p>Como cada matriz é diferente os autovalores também serão, eles também são variáveis aleatórias.&lt;/p>
&lt;pre>&lt;code class="language-python">vals_laplace = np.array([
np.linalg.eigh(laplaceRMT(n=100))[0]
for i in range(100)
])
vals_normal = np.array([
np.linalg.eigh(normalRMT(n=100))[0]
for i in range(100)
])
&lt;/code>&lt;/pre>
&lt;p>Na decáda de 50 não havia poder computacional
suficiente para realizar investigações númericas, mas você pode facilmente investigar como os autovalores se distribuem usando seu computador e gerando os histogramas&lt;/p>
&lt;pre>&lt;code class="language-python">t = 1
x = np.linspace(-2*t, 2*t, 100)
y = np.zeros_like(x)
x0 = x[4*t-x*2&amp;gt;0]
y[4*t-x*2&amp;gt;0] = np.sqrt(4*t-x0**2)/(2*np.pi*t)
plt.figure(facecolor='white')
plt.hist(vals_laplace.flatten(), bins=50,
hatch ='|',
density=True, label='laplace', alpha=.2)
plt.hist(vals_normal.flatten(), bins=50,
hatch ='o',
density=True, label='normal', alpha=.2)
#sns.distplot(vals_laplace, norm_hist=True, label='Laplace')
#sns.distplot(vals_normal, norm_hist=True, label='Normal')
#sns.distplot(vals2, norm_hist=True, label='sample2')
plt.plot(x, y, label='analytical')
plt.xlabel(r'$\lambda$')
plt.ylabel(r'$\rho(\lambda)$')
plt.legend()
plt.savefig('RMT_distribution.png', facecolor='w')
plt.close()
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="RMT_distribution.png" alt="">&lt;/p>
&lt;p>Veja na figura acima que a distribuição de autovalores de matrizes simétricas relacionadas com a distribuição normal e de Laplace coincidem. O que estamos vendo aqui é uma propriedade &lt;strong>universal&lt;/strong>! Espero que você acredite em mim, mas dado que você tenha uma matriz aleatória simétrica, quadrada e se as entradas são $i.i.d.$ a distribuição de autovalores seguem o que é conhecido como lei de semicírculo de Wigner. Se a média e variância das entradas da matriz são $0$ e $1$ respectivamente, então tal lei tem a seguinte expressão para a distribuição de probabilidade dos autovalores
$$
\rho(\lambda) = \begin{cases}
\frac{\sqrt{4-\lambda^2}}{(2\pi)} \textrm{ se } 4-\lambda^2 \leq 0\newline
0 \textrm{ caso contrário.}
\end{cases}
$$&lt;/p>
&lt;p>Se trocarmos as simetrias, restrições ou formato (&lt;code>array.shape[0]!=array.shape[1]&lt;/code>) das matrizes podemos encontrar variações da distribuição apresentada acima. Exemplo se a matriz é complexa mas Hermitiana, ou se é &amp;ldquo;retangular&amp;rdquo; e real tal como algums matrizes que são usadas para otimizar carteiras de investimento. A próxima seção mostrará um caso com outro formato para universalidade.&lt;/p>
&lt;h2 id="2-b-repulsão-entre-números-primos">2-b) Repulsão entre números primos&lt;/h2>
&lt;p>Inciamos nosso texto falando sobre como a teoria de matrizes aleatórias floreceu com os estudos estatísticos de núcleos atômicos pesados, especificamente nos trabalhos de Wigner. Embora tenha essa origem, muitas vezes ferramentas matemáticas desenvolvidas apenas por motivações práticas alcançam outros ramos da matemática. Brevemente discutirei aqui alguns pontos e relações com uma das conjecturas mais famosas da matemática: a hipótese de Riemann.&lt;/p>
&lt;p>Qualquer pessoa com alguma curiosidade sobre matemática já ouviu falar sobre a hipótese de Riemann. Essa hipótese estabele uma relação entre os zeros da função zeta de Riemann e a distribuição de números primos. Dada sua importância os maiores ciêntistas do século XX se debruçaram sobre ela almejando a imortalidade. Um desses ciêntistas foi Hugh Montgomery[4].&lt;/p>
&lt;p>Por volta de 1970 Montgomery notou que os zeros da função zeta tinham uma certa propriedade cuirosa, pareciam repelir uns aos outros. Uma expressão foi obtidada, que é a seguinte&lt;/p>
&lt;p>$$
1 - \left( \frac{\sin (\pi u)}{\pi u}\right)^2 + \delta(u)
$$&lt;/p>
&lt;p>Não se preocupe em entender a expressão acima, ela está aqui apenas for motivos estéticos.
O que importa é que ela é simples, tão simples que quando Freeman Dyson - um dos gigantes da física-matemática - colocou os olhos sobre tal equação ele notou imediatamente que tal equação era idêntica a obtida no contexto de matrizes aleatórias Hermitianas (uma matriz é hermitiana se ela é igual a sua transporta conjugada) utilizadas para compreender o comportamento de núcleos de átomos pesados, tais como urânio. A imagem abaixo é uma carta escrita por Dyson.&lt;/p>
&lt;p>&lt;img src="carta.png" alt="">&lt;/p>
&lt;p>As conexão entre um ferramental desenvolvido para estudar núcleos atômicos e números primos era realmente inesperada e talvez seja um dos caminhos para a prova da hipotese de Riemann[5, 2]. Contudo deixemos a história de lado, e voltemos ao ponto principal que é te dar outro exemplo de universalidade.&lt;/p>
&lt;p>Lembra que Montgomery disse que parecia haver uma repulsão entre os zeros da função Zeta? O que seria esse conceito de repulsão em matrizes aleatórias? Vamos checar numericamente&lt;/p>
&lt;p>Voltaremos a usar nossas matrizes aleatórias geradas por distribuições Gaussianas e Laplacianas. Usando o mesmo conjunto de autovalores que obtivemos anteriormente iremos calular o espaçamento entre cada par de autovalores para cada realização de uma matriz aleatória. É bem fácil, basta chamar a função &lt;code>diff&lt;/code> do numpy&lt;/p>
&lt;pre>&lt;code class="language-python">diff_laplace = np.diff(vals_laplace, axis=1)
diff_normal = np.diff(vals_normal, axis=1)
&lt;/code>&lt;/pre>
&lt;p>Agora o que faremos é estimar a densidade de probabilidade usnado KDE. Mas antes disso aqui vai uma dica:&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Evite o KDE do sklearn no seu dia a dia, a implementação é lenta e não flexivél. Difícilmente você conseguirá bons resultados com milhões de pontos. Aqui vou usar uma implementação de KDE mais eficiente você pode instalar ela execuntando o comando abaixo&lt;/strong>&lt;/p>
&lt;/blockquote>
&lt;pre>&lt;code class="language-python">!pip install KDEpy
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">from KDEpy import FFTKDE
estimator_normal = FFTKDE( bw='silverman').fit(diff_normal.flatten())
x_normal, probs_normal = estimator_normal.evaluate(100)
mu_normal = np.mean(diff_normal, axis=1).mean()
estimator_laplace = FFTKDE( bw='silverman').fit(diff_laplace.flatten())
x_laplace, probs_laplace = estimator_laplace.evaluate(100)
mu_laplace = np.mean(diff_laplace, axis=1).mean()
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">goe_law = lambda x: np.pi*x*np.exp(-np.pi*x**2/4)/2
spacings = np.linspace(0, 4, 100)
p_s = goe_law(spacings)
plt.plot(spacings, p_s, label=r'GOE analítico', c='orange', linestyle='--')
plt.plot(
x_normal/mu_normal,
probs_normal*mu_normal,
linestyle=':',
linewidth=2,
zorder=1,
label='normal', c='black')
plt.plot(x_laplace/mu_laplace, probs_laplace*mu_laplace, zorder=2,
linestyle='--', label='laplace', c='tomato')
plt.legend()
plt.savefig('RMT_diff_distribution.png', facecolor='w')
plt.close()
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="RMT_diff_distribution.png" alt="">&lt;/p>
&lt;p>O que as distribuições acima dizem é que dado sua matriz ser $i.i.d.$ quadrada e simétrica então a probabilidade que você encontre dois autovalores iguais é $0$ (zero). Além do mais, existe um ponto de máximo global em relação a distribuição de espaçamentos. Esse comportamento que balanceia repulsão e atração dos autovalores lembra o comportamento de partículas em um fluído. Não é de espantar que o método matemático desenvolvido por Wigner para compreender tais matrizes foi denominado Gás de Coloumb[2].&lt;/p>
&lt;p>Agora que você tem pelo menos uma ideia do que seria essa repulsão para o caso que já abordamos (matrizes simétricas quadradas) voltemos ao problema dos números primos.&lt;/p>
&lt;p>O comando a seguir baixa os primeiros 100k zeros da função zeta&lt;/p>
&lt;pre>&lt;code class="language-python">!wget http://www.dtc.umn.edu/~odlyzko/zeta_tables/zeros1
&lt;/code>&lt;/pre>
&lt;p>Um pequeno preprocessamento dos dados:&lt;/p>
&lt;pre>&lt;code class="language-python">zeros = []
with open('zeros1', 'r') as f:
for line in f.readlines():
# remove all spaces in the line and convert it to a float
zeros.append(float(line.replace(' ', '')))
zeta_zeros = np.array(zeros)
&lt;/code>&lt;/pre>
&lt;p>Iremos calcular os espaçamentos entre os zeros, a média de tais espaçamento e executar um KDE&lt;/p>
&lt;pre>&lt;code class="language-python">from KDEpy import FFTKDE
diff_zeta = np.diff(zeta_zeros[10000:])
m = np.mean(diff_zeta)
estimator = FFTKDE( bw='silverman').fit(diff_zeta)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">x, probs = estimator.evaluate(100)
p = np.pi
goe_law = lambda x: p*x*np.exp(-p*x**2/4)/2
def gue(xs):
arg = -4/np.pi*np.power(xs,2)
vals = 32/np.pi**2*xs**2*np.exp(arg)
return vals
spacings = np.linspace(0, 4, 100)
p_s = gue(spacings)
p_s2 = goe_law(spacings)
plt.plot(x/m, probs*m, label='zeros zeta', linestyle='--')
plt.plot(spacings, p_s, label=r'GUE analítico', c='blue', linestyle='-.')
plt.plot(spacings, p_s2, label=r'GOE analitico', c='orange', linestyle='-.')
plt.xlim(-0.1, 4)
plt.legend()
plt.savefig('zeta.png', facecolor='w')
plt.close()
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="zeta.png" alt="">&lt;/p>
&lt;p>Veja que a propriedade de repulsão apareceu novamente. Note que dentro do plot eu coloquei uma outra curva &lt;code>GOE analítico&lt;/code>, essa curva é aquela que melhor descreve a distribuição de espaçamentos quando suas matrizes aleatórias são simétricas. Isso é uma lição importante aqui e resalta o que eu já disse anteriormente. Não temos apenas &lt;em>&amp;ldquo;um limite central para matrizes aleatórias&lt;/em>&amp;rdquo;, mas todo um &lt;strong>zoológico que mudará dependendo do tipo do seu problema.&lt;/strong>.&lt;/p>
&lt;h1 id="3-usando-rmt-para-encontrar-e-filtrar-ruídos-em-matrizes">3-Usando &lt;em>RMT&lt;/em> para encontrar e filtrar ruídos em matrizes&lt;/h1>
&lt;p>Na seção 1 relembramos o resultado do teorema central do limite. Na seção 2 foi mostrado que devemos ter em mente as simetrias e restrições do nosso problema para analisar qual regra de universalidade é respeitada. Isto é: a depender da simetria e restrições das nossas matrizes temos um outro &amp;ldquo;&lt;em>timbre de universalidade&lt;/em>&amp;rdquo;.&lt;/p>
&lt;p>Um exemplo de outro timbre surge no espectro de matrizes de correlação; matrizes que são comumente utilizadas para análise de carteiras de investimento. Tais matrizes tem &lt;strong>pelo menos a seguinte estrutura&lt;/strong>:&lt;/p>
&lt;p>$$
\mathbf C = \mathbf X \mathbf X^T
$$
onde $\mathbf X$ é uma matriz real $N\times M$ e $M&amp;gt;N$.&lt;/p>
&lt;p>O código abaixo permite explorar em um exemplo o espectro de matrizes aleatórias $N\neq M$ com entradas dadas pela distribuição normal.&lt;/p>
&lt;pre>&lt;code class="language-python">def get_marchenko_bounds(Q, sigma=1):
&amp;quot;&amp;quot;&amp;quot;Computes the Marchenko bounds for a given Q and sigma.
Args:
Q : (float) The Q-value.
sigma : (float) The std value.
Returns:
(float, float): The lower and upper bounds for the eigenvalues.
&amp;quot;&amp;quot;&amp;quot;
QiSqrt = np.sqrt(1/Q)
lp = np.power(sigma*(1 + QiSqrt),2)
lm = np.power(sigma*(1 - QiSqrt),2)
return lp, lm
def marchenko_pastur(l, Q, sigma=1):
&amp;quot;&amp;quot;&amp;quot;Return the probability of a Marchenko-Pastur distribution for
a given Q , sigma and eigenvalue.
Args:
l : (float) The eigenvalue.
Q : (float) The Q-value.
sigma : (float) The std value.
Returns:
(float): The probability
&amp;quot;&amp;quot;&amp;quot;
lp, lm = get_marchenko_bounds(Q, sigma)
# outside the interval [lm, lp]
if l &amp;gt; lp or l &amp;lt; lm:
return 0
return (Q/(2*np.pi*sigma*sigma*l))*np.sqrt((lp-l)*(l-lm))
def plot_marchenko_pastur(ax, eigen_values, Q, sigma=1, bins=100, just_the_bulk=False):
&amp;quot;&amp;quot;&amp;quot;Plots the Marchenko-Pastur distribution for a given Q and sigma
Args:
ax : (matplotlib.axes) The axes to plot on.
eigen_values : (np.array) The eigenvalues.
Q : (float) : The Q-value.
sigma : (float) std
bins : (int) The number of bins to use.
just_the_bulk : (bool) If True, only the eigenvalues inside of
the Marchenko-Pastur bounds are plotted.
&amp;quot;&amp;quot;&amp;quot;
l_max, l_min = get_marchenko_bounds(Q, sigma)
eigenvalues_points = np.linspace(l_min, l_max, 100)
pdf = np.vectorize(lambda x : marchenko_pastur(x, Q, sigma))(eigenvalues_points)
if just_the_bulk:
eigen_values = eigen_values[ (eigen_values &amp;lt; l_max)]
ax.plot(eigenvalues_points, pdf, color = 'r', label='Marchenko-Pastur')
ax.hist(eigen_values, label='sample', bins=bins , density=True)
ax.set_xlabel(r&amp;quot;$\lambda$&amp;quot;)
ax.set_ylabel(r&amp;quot;$\rho$&amp;quot;)
ax.legend()
N = 1000
T = 4000
Q = T/N
X = np.random.normal(0,1,size=(N,T))
cor = np.corrcoef(X)
vals = np.linalg.eigh(cor)[0]
fig, ax = plt.subplots(1,1)
plot_marchenko_pastur(ax, vals, Q, sigma=1, bins=100)
plt.legend()
plt.savefig('Marchenko_Pastur.png', facecolor='w')
plt.close()
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="Marchenko_Pastur.png" alt="">&lt;/p>
&lt;p>A função em vermelho na figura acima é a &lt;strong>universalidade&lt;/strong> que aparece em matrizes com a restrição $N\times M$ e entradas $i.i.d.$ e média $0$. Tal &lt;strong>universalidade&lt;/strong> tem como formato a distribuição de Marchenko-Pastur que é dada por&lt;/p>
&lt;p>$$
\rho (\lambda) = \frac{Q}{2\pi \sigma^2}\frac{\sqrt{(\lambda_{\max} - \lambda)(\lambda - \lambda_{\min})}}{\lambda}
$$
onde
$$
\lambda_{\max,\min} = \sigma^2(1 \pm \sqrt{\frac{1}{Q}})^2.
$$&lt;/p>
&lt;p>Note os parâmetros como $Q$ e $\sigma$. Tais parâmetros precisam ser ajustados para obter um melhor fit com dados reais.&lt;/p>
&lt;p>Agora iremos para um caso real. Vamos usar dados obtidos via Yahoo Finance com a biblioteca &lt;code>yfinance&lt;/code> para consturir uma matriz de correlação com dados de ativos financeiros&lt;/p>
&lt;pre>&lt;code class="language-python"># você precisa desse pacote para baixar os dados
!pip install yfinance
&lt;/code>&lt;/pre>
&lt;p>Isso aqui é um post bem informal, então peguei peguei uma lista aleatória com alguns tickers que encontrei na internet&lt;/p>
&lt;pre>&lt;code class="language-python">
!wget https://raw.githubusercontent.com/shilewenuw/get_all_tickers/master/get_all_tickers/tickers.csv
&lt;/code>&lt;/pre>
&lt;p>selecionei apenas 500 para evitar que o processo de download seja muito demorado&lt;/p>
&lt;pre>&lt;code class="language-python">tickers = np.loadtxt('tickers.csv', dtype=str, delimiter=',').tolist()
tickers = np.random.choice(tickers, size=500, replace=False).tolist()
&lt;/code>&lt;/pre>
&lt;p>vamos baixar agora os dados em um periódo específico&lt;/p>
&lt;pre>&lt;code class="language-python">
import yfinance as yf
df = yf.download (tickers,
start=&amp;quot;2017-01-01&amp;quot;, end=&amp;quot;2019-10-01&amp;quot;,
interval = &amp;quot;1d&amp;quot;,
group_by = 'ticker',
progress = True)
&lt;/code>&lt;/pre>
&lt;p>o &lt;code>yfinance&lt;/code> vai gerar um dataframe com multiindex, então precisamos separar da
forma que queremos&lt;/p>
&lt;pre>&lt;code class="language-python">
tickers_available = list(set([ ticket for ticket, _ in df.columns.T.to_numpy()]))
prices = pd.DataFrame()
for ticker in tickers_available:
try:
prices[ticker] = df[(ticker, 'Adj Close')]
except KeyError:
pass
&lt;/code>&lt;/pre>
&lt;p>Agora iremos calcular o retorno. Aqui entra um ponto delicado. Você poderá achar alguns posts na internet ou mesmo artigos argumentando que é necessário calcular o retorno como
$\log (r+1)$ pois assim as entradas da sua matriz seguirá uma distribuição normal o que permitirá a aplicação de RMT. Já vimos no presente texto que não precisamos que as entradas da matrizes venham de uma distribuição normal para que a &lt;strong>universalidade&lt;/strong> apareça. A escolha ou não de usar $\log$ nos retornos merece mais atenção, inclusive com críticas em relação ao uso[6, 7, 8]. Mas esse post não pretende te vender nada, por isso vou ficar com o mais simples.&lt;/p>
&lt;pre>&lt;code class="language-python"># calculamos os retornos
returns_all = prices.pct_change()
# a primeira linha não faz sentido, não existe retorno no primeiro dia
returns_all = returns_all.iloc[1:, :]
# vamos limpar todas as linhas se mnegociação e dropar qualquer coluna com muitos NaN
returns_all.dropna(axis = 1, thresh=len(returns_all.index)/2, inplace=True)
returns_all.dropna(axis = 0, inplace=True)
# seleciona apenas 150 colunas
returns_all = returns_all[np.random.choice(returns_all.columns, size=120, replace=False)]
#returns_all = returns_all.iloc[150:]
&lt;/code>&lt;/pre>
&lt;p>Com o &lt;code>df&lt;/code> pronto calcularemos a matriz de correlação e seus autovalores&lt;/p>
&lt;pre>&lt;code class="language-python">correlation_matrix = returns_all.interpolate().corr()
vals = np.linalg.eigh(correlation_matrix.values)[0]
&lt;/code>&lt;/pre>
&lt;p>Vamos usar os parâmetros padrões para $Q$ e $\sigma$ e torcer para que funcione&lt;/p>
&lt;pre>&lt;code class="language-python">
T, N = returns_all.shape
Q=T/N
sigma= 1
fig, ax = plt.subplots(1,1)
plot_marchenko_pastur(ax, vals, Q, sigma=1, bins=200, just_the_bulk=False)
plt.legend()
plt.savefig('Marchenko_Pastur_all.png', facecolor='w')
plt.close()
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="Marchenko_Pastur_all.png" alt="">&lt;/p>
&lt;p>Usando todo o intervalo de tempo do nosso &lt;code>df&lt;/code> obtivemos o que parece um ajuste razoável. É claro que você poderia (deveria) rodar algum teste estatistico para verificar tal ajuste.
Existem alguns trabalhos que fizeram essa análise de forma rigorosa, comparando mercados e periódos específicos em relação a distribuição de Marchenko-Pastur[9].&lt;/p>
&lt;p>Se você for uma pessoa atenta notará que na imagem acima existem alguns autovalores fora do suporte da Marchenko-Pastur. A ideia de filtragem via RMT é como dito em [9] testar seus dados em relação a &amp;ldquo;&lt;em>hipótese nula&lt;/em>&amp;rdquo; da RMT. No caso se seus autovalores estão dentro do &lt;em>bulk&lt;/em> da distribuição que descreve um modelo de entradas &lt;em>i.i.d.&lt;/em>.&lt;/p>
&lt;p>Como isso foi aplicado em alguns trabalhos? Vamos ver na prática.&lt;/p>
&lt;p>Usaremos $70$% da série histórica para calcular uma nova matriz de correlação. Com a matriz de correlação em mãos vamos computar os autovalores e autovetores.&lt;/p>
&lt;pre>&lt;code class="language-python"># iremos usar 70% da serie para realizar a filtragem
returns_all.shape[0]*0.70
n_days = returns_all.shape[0]
n_days_in = int(n_days*(1-0.70))
returns = returns_all.copy()
sample = returns.iloc[:(returns.shape[0]-n_days_in), :].copy()
correlation_matrix = sample.interpolate().corr()
vals, vecs = np.linalg.eigh(correlation_matrix.values)
&lt;/code>&lt;/pre>
&lt;p>Os autovalores e autovetores podem ser compreendidos como a decomposição de uma dada matriz.
Portanto, o seguinte teste precisa passar&lt;/p>
&lt;pre>&lt;code class="language-python"> assert np.abs(
np.dot(vecs, np.dot(np.diag(vals), np.transpose(vecs))).flatten()
- correlation_matrix.values.flatten()
).max() &amp;lt; 1e-10
&lt;/code>&lt;/pre>
&lt;p>A distribuição de Marchenko-Pastur serve como um indicativo para nossa filtragem. O que faremos é jogar fora todos os autovalores
que estão dentro da distribuição de Marchenko-Pastur, posteriormente reconstruiremos a matriz de correlação.&lt;/p>
&lt;pre>&lt;code class="language-python">T, N = returns.shape
Q=T/N
sigma = 1
lp, lm = get_marchenko_bounds(Q, sigma)
# Filter the eigenvalues out
vals[vals &amp;lt;= lp ] = 0
# Reconstruct the matrix
filtered_matrix = np.dot(vecs, np.dot(np.diag(vals), np.transpose(vecs)))
np.fill_diagonal(filtered_matrix, 1)
&lt;/code>&lt;/pre>
&lt;p>Com a matriz de correlação filtrada você pode fazer o que bem entender com ela - existem outras maneiras de se realizar uma filtragem - uma das possíveis aplicações que precisa ser utilizada com cuidado é usar tal matriz filtrada como input para algoritmos de otimização de carteira. Talvez faça um outro post descrevendo essa otimização de forma mais clara, mas esse não é meu enfoque nesse post e nem minha especialidade. Portanto, se você quiser dar uma lida recomendo os seguintes posts: [17, 18]&lt;/p>
&lt;p>O que você precisa saber é que uma matriz de covariância, $\mathbf C_\sigma$, adimite uma decomposição em relação a matriz de correlação atráves da seguinte forma&lt;/p>
&lt;p>$$
\mathbf C_\sigma = \mathbf D^{-1/2} \mathbf C \mathbf D^{-1/2}
$$
onde $\mathbf D^{-1/2}$ é uma matriz diagonal com as entradas sendo os desvios padrão para cada serie de dados, isto é&lt;br>
$$
\begin{bmatrix}
\sigma_{1} &amp;amp;0 &amp;amp;\cdots &amp;amp;0 \\
0 &amp;amp;\sigma_{2} &amp;amp;\cdots &amp;amp;0 \\
\vdots &amp;amp;\vdots &amp;amp;\ddots &amp;amp;\vdots \\
0 &amp;amp;0 &amp;amp;\cdots &amp;amp;\sigma_{M} \end{bmatrix}
$$&lt;/p>
&lt;p>Discutimos uma maneira de obter uma matriz de correlação filtrada, $\mathbf{\tilde C}$, através de RMT,
a ideia é plugar essa nova matriz na equação anterior e obter uma nova matriz de covariância onde as informações menos relevantes foram eliminadas.&lt;/p>
&lt;p>$$
\mathbf{\tilde C_\sigma} = \mathbf D^{-1/2} \mathbf{\tilde C} \mathbf D^{-1/2}.
$$&lt;/p>
&lt;p>Tendo essa nova matriz de covâriancia filtrada agora basta você ingerir ela em algum método preferido para otimização e comparar com o resultado obtido usando a matriz original. Aqui usaremos o clássico Markowitz&lt;/p>
&lt;pre>&lt;code class="language-python"># Reconstruct the filtered covariance matrix
covariance_matrix = sample.cov()
inv_cov_mat = np.linalg.pinv(covariance_matrix)
# Construct minimum variance weights
ones = np.ones(len(inv_cov_mat))
inv_dot_ones = np.dot(inv_cov_mat, ones)
min_var_weights = inv_dot_ones/ np.dot( inv_dot_ones , ones)
variances = np.diag(sample.cov().values)
standard_deviations = np.sqrt(variances)
D = np.diag(standard_deviations)
filtered_cov = np.dot(D ,np.dot(filtered_matrix,D))
filtered_cov = filtered_matrix
filtered_cov = (np.dot(np.diag(standard_deviations),
np.dot(filtered_matrix,np.diag(standard_deviations))))
filt_inv_cov = np.linalg.pinv(filtered_cov)
# Construct minimum variance weights
ones = np.ones(len(filt_inv_cov))
inv_dot_ones = np.dot(filt_inv_cov, ones)
filt_min_var_weights = inv_dot_ones/ np.dot( inv_dot_ones , ones)
def get_cumulative_returns_over_time(sample, weights):
weights[weights &amp;lt;= 0 ] = 0
weights = weights / weights.sum()
return (((1+sample).cumprod(axis=0))-1).dot(weights)
cumulative_returns = get_cumulative_returns_over_time(returns, min_var_weights).values
cumulative_returns_filt = get_cumulative_returns_over_time(returns, filt_min_var_weights).values
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">
in_sample_ind = np.arange(0, (returns.shape[0]-n_days_in+1))
out_sample_ind = np.arange((returns.shape[0]-n_days_in), returns.shape[0])
f = plt.figure()
ax = plt.subplot(111)
points = np.arange(0, len(cumulative_returns))[out_sample_ind]
ax.plot(points, cumulative_returns[out_sample_ind], 'orange', linestyle='--', label='original')
ax.plot(points, cumulative_returns_filt[out_sample_ind], 'b', linestyle='-.', label='filtrado')
ymax = max(cumulative_returns[out_sample_ind].max(), cumulative_returns_filt[out_sample_ind].max())
ymin = min(cumulative_returns[out_sample_ind].min(), cumulative_returns_filt[out_sample_ind].min())
plt.legend()
plt.savefig('comp.png', facecolor='w')
plt.close()
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="comp.png" alt="">&lt;/p>
&lt;p>Obtivemos uma melhora, mas novamente ressaltamos que uma analise mais criteriosa deveria ter sido feita. Vamos listar alguns pontos&lt;/p>
&lt;ol>
&lt;li>Em relação a questão da escolha do intervalo de tempo. Isto é, se o tamanho foi pequeno de mais para capturar a correlação ou se foi grande de mais tal que as correlações entre ativos não são estacionárias.&lt;/li>
&lt;li>O (não) uso do $\log$-retorno e seu impacto&lt;/li>
&lt;li>Uma escolha não aleatória do que seria analisado&lt;/li>
&lt;li>Métodos de unfolding dos autovalores (tema para outro post)&lt;/li>
&lt;/ol>
&lt;h1 id="5---vantagens-críticas-e-sugestões">5 - Vantagens, críticas e sugestões&lt;/h1>
&lt;p>Você poderá encontrar alguns trabalhos e posts descrevendo o uso de matrizes aleatórias para filtragem de matrizes de correlação sem uma boa crítica ou explicitação das limitações vou linkar aqui alguns pontos positivos e negativos e limitações&lt;/p>
&lt;h2 id="onde-realmente-rmt-se-mostrou-útil">Onde realmente RMT se mostrou útil&lt;/h2>
&lt;ul>
&lt;li>Obviamente a RMT é indiscutivelmente bem sucedida na matemática e física permitindo compreender sistemas apenas analisando a estatística dos &lt;em>gases matriciais&lt;/em>.&lt;/li>
&lt;li>Em machine learning a RMT também está provando ser uma ferramenta útil para compreender e melhorar o processo de aprendizado [15].&lt;/li>
&lt;li>Entender comportamentos de sistemas sociais, biológicos e econômicos. Aqui com entender o comportamento digo apenas saber se um dado segue uma característica dada por alguma lei específica como a lei de semicírculo. Isto é, não existe discussão em você pegar um dado sistema que é representado por uma matriz, estudar o comportamento do seu espectro de autovalores e autovetores e verificar que seguem algumas lei de universalidade. &lt;strong>Isso é bem diferente de dizer que se você filtrar uma matriz de correlação via RMT você irá obter sempre resultados melhores.&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h2 id="limitações">Limitações&lt;/h2>
&lt;ul>
&lt;li>Note que não realizamos nenhum tipo de teste para decidir se realmente a distribuição de autovalores era a distribuição desejada. Baseamos isso só no olhometro, obviamente não é uma boa ideia.&lt;/li>
&lt;li>A filtragem apenas removendo os autovalores apesar de simples é limitada e pode ser contra produtiva, outros métodos de filtragem podem ser inclusive melhores[14]. Inclusive não é uma das únicas aplicações de RMT para tratamento desse tipo de dado [16]&lt;/li>
&lt;/ul>
&lt;h2 id="para-conhecer-mais">Para conhecer mais&lt;/h2>
&lt;h3 id="ciêntistas">Ciêntistas&lt;/h3>
&lt;ul>
&lt;li>Alguns grandes nomes de RMT: Madan Lal Mehta, Freeman Dyson e Terrence Tao&lt;/li>
&lt;li>Alguns brasileiros: Marcel Novaes autor do livro
&lt;a href="https://link.springer.com/book/10.1007/978-3-319-70885-0" target="_blank" rel="noopener">Introduction to Random Matrices - Theory and Practice&lt;/a>-
&lt;a href="https://arxiv.org/abs/1712.07903" target="_blank" rel="noopener">arxiv&lt;/a>; Fernando Lucas Metz trabalhou com o Nobel Giorgio Parisi.&lt;/li>
&lt;/ul>
&lt;h3 id="encontrou-um-erro-ou-quer-melhorar-esse-texto">Encontrou um erro ou quer melhorar esse texto?&lt;/h3>
&lt;ul>
&lt;li>Faça sua contribuição criando uma
&lt;a href="https://github.com/devmessias/devmessias.github.io/issues/new" target="_blank" rel="noopener">issue&lt;/a> ou um PR editando esse arquivo aqui
&lt;a href="https://github.com/devmessias/devmessias.github.io/blob/master/content/post/random_matrix_theory/index.md" target="_blank" rel="noopener">random_matrix_theory/index.md&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h1 id="6-referências">6-Referências&lt;/h1>
&lt;ul>
&lt;li>
&lt;p>[1] M. Kac, “Can One Hear the Shape of a Drum?,” The American Mathematical Monthly, vol. 73, no. 4, p. 1, Apr. 1966, doi: 10.2307/2313748.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>[2] Wigner, E.P., 1957. Statistical properties of real symmetric matrices with many dimensions (pp. 174-184). Princeton University.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>[4] “From Prime Numbers to Nuclear Physics and Beyond,” Institute for Advanced Study.
&lt;a href="https://www.ias.edu/ideas/2013/primes-random-matrices" target="_blank" rel="noopener">https://www.ias.edu/ideas/2013/primes-random-matrices&lt;/a> (accessed Sep. 30, 2020).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>[5] “GUE hypothesis,” What’s new.
&lt;a href="https://terrytao.wordpress.com/tag/gue-hypothesis/" target="_blank" rel="noopener">https://terrytao.wordpress.com/tag/gue-hypothesis/&lt;/a> (accessed Nov. 22, 2021).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>[6] R. Hudson and A. Gregoriou, “Calculating and Comparing Security Returns is Harder than you Think: A Comparison between Logarithmic and Simple Returns,” Social Science Research Network, Rochester, NY, SSRN Scholarly Paper ID 1549328, Feb. 2010. doi: 10.2139/ssrn.1549328.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>[7] A. Meucci, “Quant Nugget 2: Linear vs. Compounded Returns – Common Pitfalls in Portfolio Management,” Social Science Research Network, Rochester, NY, SSRN Scholarly Paper ID 1586656, May 2010. Accessed: Dec. 01, 2021. [Online]. Available:
&lt;a href="https://papers.ssrn.com/abstract=1586656" target="_blank" rel="noopener">https://papers.ssrn.com/abstract=1586656&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>[8] Lidian, “Analysis on Stocks: Log(1+return) or Simple Return?,” Medium, Sep. 18, 2020.
&lt;a href="https://medium.com/@huangchingchiu/analysis-on-stocks-log-1-return-or-simple-return-371c3f60fab2" target="_blank" rel="noopener">https://medium.com/@huangchingchiu/analysis-on-stocks-log-1-return-or-simple-return-371c3f60fab2&lt;/a> (accessed Nov. 25, 2021).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>[9] N. A. Eterovic and D. S. Eterovic, “Separating the Wheat from the Chaff: Understanding Portfolio Returns in an Emerging Market,” Social Science Research Network, Rochester, NY, SSRN Scholarly Paper ID 2161646, Oct. 2012. doi: 10.2139/ssrn.2161646.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>[10] E. P. Wigner, “Characteristic Vectors of Bordered Matrices With Infinite Dimensions,” Annals of Mathematics, vol. 62, no. 3, pp. 548–564, 1955, doi: 10.2307/1970079.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>[11] E. P. Wigner, “On the statistical distribution of the widths and spacings of nuclear resonance levels,” Mathematical Proceedings of the Cambridge Philosophical Society, vol. 47, no. 4, pp. 790–798, Oct. 1951, doi: 10.1017/S0305004100027237.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>[13] F. W. K. Firk and S. J. Miller, “Nuclei, Primes and the Random Matrix Connection,” Symmetry, vol. 1, no. 1, pp. 64–105, Sep. 2009, doi: 10.3390/sym1010064.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>[14] L. Sandoval, A. B. Bortoluzzo, and M. K. Venezuela, “Not all that glitters is RMT in the forecasting of risk of portfolios in the Brazilian stock market,” Physica A: Statistical Mechanics and its Applications, vol. 410, pp. 94–109, Sep. 2014, doi: 10.1016/j.physa.2014.05.006.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>[15] M. E. A. Seddik, C. Louart, M. Tamaazousti, and R. Couillet, “Random Matrix Theory Proves that Deep Learning Representations of GAN-data Behave as Gaussian Mixtures,” arXiv:2001.08370 [cs, stat], Jan. 2020, Accessed: Dec. 05, 2021. [Online]. Available:
&lt;a href="http://arxiv.org/abs/2001.08370" target="_blank" rel="noopener">http://arxiv.org/abs/2001.08370&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>[16] D. B. Aires, “Análise de crises financeiras brasileiras usando teoria das matrizes aleatórias,” Universidade Estadual Paulista (Unesp), 2021. Accessed: Dec. 05, 2021. [Online]. Available:
&lt;a href="https://repositorio.unesp.br/handle/11449/204550" target="_blank" rel="noopener">https://repositorio.unesp.br/handle/11449/204550&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>[17] S. Rome, “Eigen-vesting II. Optimize Your Portfolio With Optimization,” Scott Rome, Mar. 22, 2016.
&lt;a href="http://srome.github.io//Eigenvesting-II-Optimize-Your-Portfolio-With-Optimization/" target="_blank" rel="noopener">http://srome.github.io//Eigenvesting-II-Optimize-Your-Portfolio-With-Optimization/&lt;/a> (accessed Dec. 05, 2021).&lt;/p>
&lt;/li>
&lt;li>
&lt;p>[18] “11.1 Portfolio Optimization — MOSEK Fusion API for Python 9.3.10.”
&lt;a href="https://docs.mosek.com/latest/pythonfusion/case-studies-portfolio.html" target="_blank" rel="noopener">https://docs.mosek.com/latest/pythonfusion/case-studies-portfolio.html&lt;/a> (accessed Dec. 05, 2021).&lt;/p>
&lt;/li>
&lt;/ul></description></item></channel></rss>