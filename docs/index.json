[{"authors":["admin"],"categories":null,"content":"I’m passionate about how we can use computers and math to solve real and abstract problems. I have experience in the development of open-software projects, community management, and Research \u0026amp; Development.\nToday I’m pursuing my Ph.D. in the topics of graphs aiming to solve problems such as graph characterization and survey analysis. Also, I\u0026rsquo;ve started my studies in Machine Learning Operations.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/bruno-messias/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/bruno-messias/","section":"authors","summary":"I’m passionate about how we can use computers and math to solve real and abstract problems. I have experience in the development of open-software projects, community management, and Research \u0026amp; Development.","tags":null,"title":"Bruno Messias","type":"authors"},{"authors":null,"categories":["Data analysis"],"content":" Esse post é bem informal é foi feito para o grupo de estudos de MlOps. O conteúdo pode mudar significativamente com o passar do tempo.   Quando olhamos uma imagem temos a tendência de procurar padrões o que reduz o esforço e tempo necessário para identificar do que se trata. Em análise de dados aplicamos filtros com a mesma motivação, seja para remover dados espúrios ou focar nossa análise em um conjunto específico.\nEnquanto o processo de filtragem em um conjunto de pontos é apresentado em cursos acadêmicos e tutoriais na internet o mesmo não pode ser dito em relação a grafos. Portanto, criei esse post para discutir o conceito de filtragem e padrões em grafos e as diferentes maneiras de se obter tal filtragem. Tentei ser didático o suficiente para que uma pessoa fora da computação ou exatas (que esteja iniciando em dados) consiga comprender o texto. Sinta-se a vontade para pular qualquer seção do post :)\n Grafos, redes e redes complexas são praticamente o mesmo conceito. Portanto, você pode encontrar termos como filtering edges on complex networks.   Os exemplos desse post usam python e as seguintes bibliotecas:\n$ python3 -m pip install numpy matplotlib networkx  O que é um grafo? Um grafo é uma estrutura de dados que você constantemente está em contato. Alguns exemplos: sua rede de seguidores e seguidos no twitter, as transações financeiras associadas a sua chave PIX, as relações de repositório e contribuições no github, etc.\nUm grafo armazena objetos que tem relações pares a pares entre si. Sendo que é possível associar a cada objeto ou relação um outro tipo de dado genérico tais como um número real, um vetor, imagem ou mesmo outro grafo.\nA imagem abaixo representa um grafo dirigido formado por 4 vértices.\ngraph TD; A--\u0026gt;B; B--\u0026gt;A; A--\u0026gt;C; B--\u0026gt;D; C--\u0026gt;D;  Vamos usar a letra $G$ para representar um grafo, $V$ para o conjunto de vértices (objetos) e $E$ para o conjunto de arestas (relações). Na imagem acima nosso grafo seria dado então pelo conjunto $V=\\{A,B,C,D\\}$ e $E=\\{(A,B), (B,A), (A,C), (B,D), (C,D)\\}$.\nComo disse no início dessa seção é possível associar coisas tanto as arestas quanto aos vértices. Por exemplo, o grafo abaixo poderia representar transações financeiras entre 3 pessoas e o valor que cada uma tem em sua conta corrente\ngraph TD; A[A R\\$100,00]--|R\\$1|B; B[B R\\$3,00]--|R\\$2|A; C[C R\\$0]--|R\\$0,50|A;  Tais grafos de transações financeiras são usados por exemplo para detectar crimes de lavagem de dinheiro, formação de quadrilhas e fraudes quando o comportamento de um dado cliente é anómalo.\nO que é filtragem? Por que filtrar um dado? Filtro tem origem na palavra feltro. O feltro era o material feito de lã e algodão usado antigamente para separar um liquido de suas impurezas. Um filtro em análise de dados é a mesma coisa: uma ferramenta que separa um conjunto de dados de uma sujeira, ruído. Portanto, assim como para filtrar uma bebida temos que decidir antes algumas coisas:\n O que queremos que seja removido? O quão eficiente é nosso fitro? Qual é o resultado esperado?  Filtrar para remover ruídos Talvez a primeira coisa que vem a sua cabeça quando ouve a palavra filtro é Instagram. Alguns filtros de fotos feitos para embelezar nada mais são que um filtro para remoção de ruídos.\n  Imagem original e imagem com contaminação de um ruído.   O que considerámos ruído depende das respostas das perguntas que levantei anteriormente. Um ruído em uma imagem pode ser uma contribuição espúria devido ao sensor de uma câmera ser ruim o que acaba alterando os valores de cores de cada pixel. Um ruído pode ser também algo intrísseco, por exemplo os poros e rugas na sua pele.\nFiltrar para ressaltar características e os princípios de Gestalt  Os princípios de Gestalt é uma suposição de certas leis sobre como a mente humana processa imagens através do reconhecimento de padrões. Em resumo, tal princípio estabelece que a percepção não é baseada em elementos indivíduais mas em padrões em que os elementos são arranjados ou tem contrastes entre-si. Você não compreende uma imagem analisando cada pixel individualmente, mas como os pixels se organizam e diferem entre-si!\n  Os principios da Gestalt são apresentados nessa figura. Para mais informações recomendo esse post [].   Como se relaciona com grafos? Um dos porquês para realizar a filtragem de um grafo consiste em remover relações (arestas) mesmo que não espúrias para ressaltar um dado padrão que queremos analisar. Comumente, esse padrão são estruturas de comunidades e agrupamentos geradas por métodos de visualização.\n  Os princípios da Gestalt são usados para desenvolver métodos de processamento de imagens. Imagem retirada de []   Na imagem acima é mostrado o resultado de um método baseado na Gestalt para simplificar uma imagem. Em grafos temos o conceito de backbones que são uma espécie de espinha dorsal, esqueleto, que representa as relações entres os vértices.\nReduzindo custo computacional por filtragem Embora a filtragem possa ser usada para remover uma contaminação em um dado e/ou facilitar termos insights conseguimos também reduzir o custo computacional de algoritmos que atuam nesses dados. Um exemplo simples é mostrado no código abaixo:\nimport numpy as np import io X, Y = np.meshgrid( np.linspace(-5, 5, 100), np.linspace(-5, 5, 100)) z = np.exp(-0.1*(X**2 + Y**2)) z_noise = z + np.random.normal(0, 0.1, z.shape) z = (z / z.max()*255).astype(np.uint8) z_noise = (z_noise / z_noise.max()*255).astype(np.uint8) data_noisy = io.BytesIO() data = io.BytesIO() np.savez_compressed(data_noisy, z_noise) np.savez_compressed(data, z) print(f\u0026quot;Noisy {data_noisy.getbuffer().nbytes/10**6:.1f} MB\u0026quot;) print(f\u0026quot;Original {data.getbuffer().nbytes/10**6:.1f} MB\u0026quot;)  Noisy 3.6 MB Original 0.2 MB  O output indica que o resultado de contaminação por ruído aumenta o custo de armazenamento de um mesmo padrão de dados.\nEm grafos, filtrar para reduzir custo computacional costuma ser essencial. Por exemplo, muitos algorítimos dependem escalam com o número de arestas. Portanto, um grafo em que cada par de vértices tem uma aresta teria custo computacional $O(numero\\ \\ de\\ \\ vértices^2)$ o que é impraticável para apenas algumas dezenas de milhares de vértices. Portanto, tornando a ánalise de dados inútil.\nO que é um processo de filtragem em um grafo? Vamos entrar agora no tema central do post. Mas antes é bom você ler com calma a seguinte desambiguação para que não fique perdido na literatura.\n Desambiguação.\nA área de processamento de sinais em grafos e a área de analise de redes sociais costumam usar termos distintos para a mesma coisa.\n  Graph coarsening\n  em ciência da computação: o processo de obter uma representação mais grosseira de um grafo removendo arestas e/ou vértices.\n  outras disciplinas: termo não muito utilizado.\n    Edge filtering\n em ciência da computação: o processo de aplicar um filtro (processamento de sinais) em valores definidos nas arestas. Uma filtragem nos valores associados as arestas!   outras disciplinas: o processo de remover arestas que não se adequam a um dado padrão.    Graph sparsification\n esse termo também costuma ser usado, mais por matemáticos e engenheiros. E pode representar tanto a remoção de vértices quanto arestas. Por exemplo: “spectral edge sparsification”. Contudo, é mais utilizado quando você parte de um grafo vazio (sem relações) e vai adicionando tentando preservar as propriedades espectrais do grafo original.    Você pode encontrar trabalhos com o termo spectral filtering ou spectral coarsening , ambos significando a mesma coisa. Contudo, spectral filters costuma ser usado mais em trabalhos de processamento de sinal em grafos.\n  Quando você aplica um filtro em uma foto para te deixar mais bonito você obviamente objetiva que as pessoas ainda te reconhençam. Isto é, as formas e aspectos mais importantes do seu rosto devem ser preservadas ou pouco alteradas. Vamos representar essas considerações por: $$ \\begin{eqnarray} \\mathcal P_{forma}(foto\\ \\ orignal) \\sim \\mathcal P_{forma}(foto\\ \\ filtrada)\\newline \\mathcal P_{cor}(foto\\ \\ orignal) \\sim \\mathcal P_{cor}(foto\\ \\ filtrada)\\newline \u0026hellip;etc \\end{eqnarray} $$ Também espera-se que o ruído da câmera, rugas e imperfeições sejam reduzidas $\\mathcal P_{rugas}(foto\\ \\ orignal) \\neq \\mathcal P_{rugas}(foto\\ \\ filtrada)$ e $|rugas\\ \\ foto \\ \\ original| \\ll |rugas\\ \\ foto \\ \\ filtrada|$. O simbolo $|.|$ significa que estamos contando o número de rugas da foto, do conjunto de rugas, e $\\ll$ significa muito menor.\nDa mesma maneira que no caso de fotos, se temos um grafo, $G$, queremos que sua versão filtrada, $\\tilde G$, tenha uma ou mais propriedades (definido de antemão) preservadas após efetuar a filtragem, isto é $$ \\mathcal P_{algo} (G) \\sim \\mathcal P_{algo} (\\tilde G) $$\nSendo que o objetivo principal costumar ser uma redução drástica no número de relações (arestas), $|E| \\le |\\tilde E|$. OK, então antes de entrar nos métodos de filtragem precisamos discorrer sobre quais seriam essas propriedades que queremos preservar.\n Diferente de uma imagem em que filtros só ocorrem nos valores definidos na posição dos pixels em um grafo podemos filtrar tanto os valores definidos nos vértices quando a própria estrutura do grafo em sí.\n filtrar a estrutura de um grafo $\\neq$ filtrar valores definidos na estrutura de um grafo    Algumas propriedades de grafos Componentes e Comunidades Componentes Uma propriedade importante de um grafo é o número de componentes. Um grafo é fortemente conectado quando é possível sair de qualquer vértice e chegar em qualquer outro. Um grafo fortemente conectado tem apenas uma componente.\nPor exemplo, abaixo é apresentado um grafo fortemente conectado\ngraph LR; A---B; D---A; B---C A---C; D---E;  Ao remover a aresta $(D , A)$ obtemos o seguinte grafo\ngraph LR; A---B; B---C A---C; D---E;  Como é impossível sair de $D$ ou $E$ e chegar em $A$, $B$ ou $C$ após a disconexão o grafo não é mais fortemente conectado e tem duas componentes. Qual a relação disso com filtragem?\n Esperamos que métodos de filtragem sejam bons em preservar o número de componentes. Pois isso afeta em muito dinâmicas ocorrendo no grafo. Assim como algorítimos de análise de dados.\nImagina se ao realizar uma filtragem você remova uma aresta que impede a contaminação por um vírus entre duas cidades no seu modelo?\n  Comunidades Dentro de cada componente de um grafo temos o conceito de comunidade. Intuitivamente, quando pensamos em comunidade no ambito das relações pessoais imaginamos um grupo de pessoas que tem fortes relações entre si, muito mais fortes que as relações com outras pessoas fora do grupo. Por exemplo, família, colegas de trabalho etc. Nesse contexto qual é a tarefa de detecção de comunidades? Como efetuar tal tarefa?\n Em certos casos queremos que a filtragem não altere a identificação das estruturas de comunidade no nosso grafo.   Suponha que você queira modelar o grupo de pessoas pertecentes a dois partidos políticos, opostos na ideologia. Você pode representar as relações entre as pessoas usando grafos. Colocando uma aresta entre uma pessoa e outra se elas concordaram majoritariamente em relação a um conjunto de questões. O que seria um algoritmo de detecção de comunidade em tal caso? Se temos o ground thruth, isto é, o partido que cada pessoa se identifica, o algoritmo é uma caixa, $f$, que recebendo apenas as relações , $E$, gospe um indíce que associa cada pessoa um partido $f: (Pesssoa, E) \\mapsto \\{Esquerda,Direita\\}$. Mas como construir essa $f$? Na minha opinião existem três caminhos principais:\n Não existe uma única definição formal para comunidade. Esse conceito muda dependendo da abordagem que você escolheu para encontrar as comunidades dentro de cada componente.   Caminho 1: Inferir  Pegue por exemplo a distribuição normal. Quando trabalhamos com dados que acreditamos que podem ser modelados por tal distribuição realizamos um processo de ajuste de parâmetros, tentando estimar a média e o desvio padrão da população. A ideia aqui é similar. Propõe-se um modelo generativo definido por um conjunto de parâmetros.. O objetivo é otimizar tais parâmetros tal que o modelo generativo seja um bom candidato para gerador do grafo.  O modelo generativo mais famoso é conhecido como Stocahastic Block Model (SBM). Em português, Modelo de Bloco Estocástico. Usando o networkx você pode gerar uma amostra de um grafo através desse modelo usando o seguinte código\nimport networkx as nx import matplotlib.pyplot as plt # esses são os parâmetros que definiram o número de indíviduos # dentro de cada comunidade n1, n2, n3 = 30, 40, 60 # esses são os parâmetros que definem a probabilidade # de conexão entre indivíduos da mesma comunidade p11, p22, p33 = 0.4, 0.3, 0.7 # esses são os parâmetros que definem a probabilidade # de conexão entre indíviduos de comunidades distintas p12 = .01 p13 = .1 p23 = .01 sizes = [n1, n2, n3] probs = [[p11, p12, p13], [p12, p22, p23], [p13, p23, p33]] g_sbm = nx.stochastic_block_model(sizes, probs, seed=0) W = nx.adjacency_matrix(g_sbm).todense() plt.imshow(W) plt.show()    A matriz de pesos (adjacência pois todos os pesos são 1) do grafo gerado por nosso modelo.   A ideia da inferência com o método mais simples de SBM é a seguinte:\n Extraia a matriz de adjacência de um grafo qualquer: uma rede social, uma rede de transações financeiras, etc. Pegue um SBM, com a matriz de peso tente estimar o número de partições, probabilidade de conexões intra e entre grupos e em qual bloco cada vértice pertence tal que os grafos gerados pelo SBM melhor represente o seu grafo original.  O SBM é poderoso e ao contrário dos outros métodos te fornece uma maneira de checar a qualidade das comunidades encontradas. Isto é, se fazem sentido ou só são frutos de algo aleatório. Contudo, por ser uma técnica mais recente com uma implementação difícil não são todas as bibliotecas que fornecem esse recurso. A biblioteca mais famosa para SBM é o graph tool que consegue estimar comunidades para grafos com centenas de milhares de vértices. Não poderei discorrer mais ou mostrar como usar o SBM pois é um tema bem complexo. Mas o importante agora é você ter conseguido absorver pelo menos a ideia.\nCaminho 2: Quantificar/Descrever  Você parte de um função $f$ qualquer. Exemplo, $f$ é uma função que identifica todo mundo como esquerda ou direita, um sorteio aleatório, etc. Com tal identificação você estipula uma grandeza que vai mensurar o quão forte é a coesão entre as pessoas de cada grupo e quão fraca é entre os grupos. Um exemplo de grandeza que mensura isso é a modularidade. Você ira alterar a sua $f$ tentando maximizar tal grandeza.  O networkx por exemplo possui um método de maximização de modularidade usando um algoritmo guloso. Vamos usar o o grafo gerado pelo sbm para testar esse método usando o seguinte script:\nfrom networkx.algorithms import community def find_where(n, p): return [i for i in range(len(p)) if n in p[i]][0] def plot(g, community_index, p): labels = [chr(ord('A') + i) for i in range(len(p))] plt.scatter(range(len(g.nodes)), community_index) plt.ylabel('Community') plt.xlabel('Vertex Id') plt.yticks(range(len(p)), labels) plt.show() p = community.greedy_modularity_communities(g_sbm) g_sbm_community_index = [find_where(n, p) for n in g_sbm.nodes] print(f\u0026quot;Found {len(set(g_sbm_community_index))} communities\u0026quot;) plot(g_sbm, g_sbm_community_index, p)    Resultado da identificação de comunidades usando o algoritmo guloso. Parece Ok   Temos um resultado muito bom. Mas será que podemos empregar isso em qualquer caso? Vejamos o que acontece quando aplicamos o mesmo algoritmo para um grafo aleatório.\n# erdos_reyni é um modelo de grafo aleatório g = nx.erdos_renyi_graph(150, 0.1, seed=0) p = community.greedy_modularity_communities(g) g_community_index = [find_where(n, p) for n in g.nodes] plot(g, g_community_index, p)    Resultado da identificação de comunidades usando o algoritmo guloso para o modelo ER.   O algoritmo guloso encontrou 4 comunidades e o ponto ruim é que não temos como saber o quão confiável é essa resposta. Mas podemos dizer que provavelmente ela não deveria ser usada pois partimos de um modelo de grafo aleatório.\nCaminho 3: Visualizar  Você utiliza um método que mapeia cada vértice do seu grafo em um espaço vetorial. Por exemplo t-sne, UMAP, force-directed, etc. Com sua visualização você realiza uma inspeção (totalmente subjetiva) para identificar as comunidades (agrupamentos). Em alguns casos é aceitável realizar um k-means nesse espaço para encontrar os clusters.  O script abaixo gera uma visualização dos dois grafos usados no exemplos anteriores: Um obtido do SBM e outro do Erdos-Reyni.\nimport numpy as np pos_sbm = np.array([ v for v in nx.layout.spring_layout(g_sbm, iterations=1000).values()]) pos = np.array([ v for v in nx.layout.spring_layout(g, iterations=1000).values()]) fig, (a1, a2) = plt.subplots(1, 2) a1.scatter(pos_sbm[:, 0], pos_sbm[:, 1], c=g_sbm_community_index, cmap='tab20') a2.scatter(pos[:, 0], pos[:, 1], c=g_community_index, cmap='tab20') for ax in (a1, a2): ax.set_yticklabels([]) ax.set_xticklabels([]) a1.set_title('SBM') a2.set_title('ER') plt.show()    Visualização via force-directed para uma amostra de um SBM e outra Erdos-Reyni. Cores representam as comunidades identificadas pelo método guloso de maximização de modularidade   Note que o método de visualização mostrou um agrupamento de vértices para o SBM. Contudo, no caso do grafo aleatório (ER) só parece uma grande confusão. As cores representam as comunidades obtidas via maximização da modularidade. O que poderemos tirar desse exemplo? Que você deve tomar cuidado quando falar que encontrou uma comunidade ou que existe uma “bolha” na rede social que você encontrou. É recomendado que você use métodos diferentes para checar esses achados de comunidades.\n No caso de visualizações de grafos, especialmente de force-directed talvez seja melhor você utilizar algum sistema de visualização iterativo e 3D. Visualizações em 2D obtidas pelo force-directed podem não ser de grande ajuda e ainda ficarem presas em alguma configuração não ótima.    Devemos tomar muito cuidado com métodos de visualização e detecção por maximização de modularidade. Recomendo ver alguns trabalhos sobre modelos de bloco estocástico especialmente os feitos pelo Tiago Peixoto.\nNew blog post! This time, on something tame and uncontroversial:\n\u0026quot;Modularity maximization considered harmful\u0026quot;\nIt\u0026#39;s the most popular method used for community detection. It is also one of the most problematic. 1/11\n(Based on https://t.co/iCxFjKOIT1)https://t.co/IRdCFwttQL\n\u0026mdash; Tiago Peixoto (@tiagopeixoto) December 6, 2021   Métodos de detecção de comunidade usando modularidade (Gelphi) são úteis. Contudo, podemos identificar comunidades mesmo no caso de um grafo totalmente aleatório!   O tema de comunidades merece alguns posts separados para cada caminho, pois é um assunto denso e com muitos métodos diferentes.\nMatrizes associadas a um grafo e seu espectro Sinto muito, mas o estudo de propriedades espectrais de um grafo é um tópico extremamente extenso.\nPegue o seguinte grafo\ngraph LR; A---|1|B; B---|1/2|C; C---|2|A;  podemos associar com esse grafo uma matriz $3\\times 3$ onde as entradas da matriz representam os valores associados as arestas. Essa matriz é conhecida como matriz de pesos,\n$$ W=\\begin{pmatrix} - \u0026amp; A \u0026amp; B \u0026amp; C\\\\ A \u0026amp; 0 \u0026amp; 1 \u0026amp; 2\\\\ B \u0026amp; 1 \u0026amp; 0 \u0026amp; 1/2\\\\ C \u0026amp; 2 \u0026amp; 1/2 \u0026amp; 0 \\end{pmatrix} $$\n$$ v=\\begin{pmatrix} x\\\\ y\\\\ z \\end{pmatrix} $$\ngraph LR; A[y+2z]---|1|B; B[x+1/2z]---|0.5|C; C[2x+0.5y]---|2|A;   A matriz pesos de um grafo pode ser pensada como uma generalização para combinar valores númericos.    Variações do teorema do limite central para matrizes aleatórias\n1-Threshold Esse com certeza é o método mais simples e mais rápido, embora o mais controverso. É aplicável somente se cada relação (aresta) possuir um número real associado. O método de threshold consiste em descartar qualquer aresta cuja a função peso ultrapasse um dado valor.\nO método de threshold é muito utilizado em neurociência (com críticas) e para análise de dados em geral quando as arestas representam uma medida de correlação (Pearson) entre dois elementos. Como as medidas de correlações podem ser negativas é comum que o threshold seja aplicado no absoluto dos valores associados as arestas.\nTome o seguinte grafo como exemplo:\ngraph LR; A--\u0026gt;|-0.5|B; B--\u0026gt;|0.4|C C--\u0026gt;|2|A; D--\u0026gt;|-1|C;  ao realizar um threshold de $0.5$ iremos remover a relação $(B, C)$ e $(A,B )$. O grafo não é mais fortemente conectado.\ngraph LR; C--\u0026gt;|2|A; D--\u0026gt;|-1|C; B;  É comum que após o threshold todas as arests que sobraram sejam truncadas em $1$. Ficariamos com algo assim no final:\ngraph LR; C--\u0026gt;|1|A; D--\u0026gt;|1|C; B;  Pontos positivos  custo computacional baixo $O(n)$  apenas iterar e comparar os valores.   paralelizável trivial de implementar apenas um parâmetro  Pontos negativos  tendência de produzir muitas componentes desconectadas, parâmetro arbitrário,  chery-picking.   remoção de uma aresta só depende do valor atribuído a ela.  2-Estatístico: disparidade Anteriormente falei um pouco sobre os princípios da Gestaltt e citei o que é chamado de backbones em grafos/redes complexas. Quando usamos o método de threshold estamos fazendo uma operação local nas arestas descartando ou matendo dependendo do seu valor. Em Gestalt estamos preocupados com padrões, com coisas não locais. Em grafos algumas vezes se faz necessário pensar assim também.\nPara deixar isso mais claro veja o exemplo de grafo com pesos a seguir:\ngraph LR; *---|0.4|1; 1---|0.8|2; 3---|0.4|2; 1---|0.6|3; 1---|0.6|4; 4---|0.3|3; 4---|...|...; 1---|...|...; *---|0.4|a; a---|1|b; a---|0.8|c; a---|0.8|d; c---|0.7|e; b---|0.7|f; d---|0.8|g; g---|...|?_1; f---|...|?_2; e---|...|?_3; b---|0.3|c; c---|0.3|d;  Se aplicassemos um threshold em $0.5$ teriamos algo do tipo\ngraph LR; *; 1---2; 1---3; 1---4; 4---|...|...; 1---|...|...; a---b; a---c; a---d; c---e; b---f; d---g; g---|...|?_1; f---|...|?_2; e---|...|?_3;  Produzindo 3 comonentes no nosso grafo. Outro ponto é que o método de threshold é muito sensível a pequenas mudanças no valor que será feito a remoção das arestas, se tivessemos escolhido 0.6 teriamos mais uma componente isolada formada por (1, 3).\nSe fossemos aplicar uma algoritmo de detecção de comunidades teríamos que fazer isso para cada componente. Em uma rede social isso pode ser problemático porque já estariámos analisando “bolhas” isoladas. Então como proceder?\nA ideia é encontrar as arestas que são a sustentação para o grafo. A espinha dorsal do grafo, backbone. O filtro de disparidade funciona dessa maneira: primeiro ele associa com cada aresta um novo valor, $\\alpha$, que leva em conta os pesos das arestas vizinhas, depois realizamos um threshold em cima desses novos valores, $\\alpha$‘s. Como esses novos valores são definidos? $$ p_{uv} = \\frac{Peso\\ da\\ aresta\\ (u,v)}{Soma\\ dos\\ pesos\\ de\\ todas\\ as\\ arestas\\ de\\ u}$$\n$$ = \\frac{w_{uv}}{\\sum\\limits_v w_{uv}} $$\nPontos positivos  é estabelecido dentro de uma formalização matemática robusta tenta evitar que o grafo se desconecte custo computacional baixo  Pontos negativos  podemos argumentar que o teste de hipótese é arbitrário parâmetro $\\alpha$ precisa ser escolhido, embora mais robusto do que apenas o parâmetro de threshold  3-Espectral $$ (1-\\epsilon)v^TLv \\le v^TLv \\le (1+\\epsilon)v^T Lv $$\nPontos positivos  é estabelecido dentro de uma formalização matemática robusta dada as restrições garante preservar as propriedades estabelecidas muito utilizado para processamento de sinais em grafos  Pontos negativos  custo computacional geralmente elevado  alguns métodos espectrais tem custo $O(n^2)$ para cada iteração   muitas maneiras distintas de fazer para cada tipo de grafo e objetivo.  Se o grafo for direcionado ou não, se é livre de escala ou não, se tem um certo padrão específico de conexões, etc.     Conclusão ","date":1644883200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1644883200,"objectID":"7738f89e9d7dbceeb26ec4def7b13cfd","permalink":"/post/edge_graph_filtering/","publishdate":"2022-02-15T00:00:00Z","relpermalink":"/post/edge_graph_filtering/","section":"post","summary":"Assim como filtramos imagens para melhorar a visualização ou ressaltar características também podem filtrar relações (arestas) em um grafo com o mesmo objetivo. Veremos aqui alguns tipos de filtragens.","tags":["graphs","statistics","ml","data analysis","python","Complex Networks","matrix"],"title":"Removendo arestas em um grafo: estatístico, threshold e espectral. Parte I","type":"post"},{"authors":null,"categories":["Data analysis"],"content":" Disponível em https://opencodecom.net/\n No célebre trabalho “Can One Hear the Shape of a Drum?”[1] Kack questiona se conhecendo o espectro (som) de um certo operador que define as oscilações de uma membrana (tambor) seria possível identificar o formato de tal membrana de maneira unívoca. Discutiremos aqui como é possível ouvir matrizes de correlação usando seu espectro e como podemos remover o ruído desse som usando resultados da teoria de matrizes aleatórias. Veremos como essa filtragem pode aprimorar algoritmos de construção de carteiras de investimentos.\n Minhas motivações para escrever esse texto foram o movimento Learn In Public-Sibelius Seraphini e o Nobel de Física de 2021. Um dos temas de Giorgio Parisi é o estudo de matrizes aleatórias www.nobelprize.org 2021.\n ..\n Jupyter notebook disponível aqui\n 1-Introdução: teorema central do limite O teorema central do limite está no coração da análise estatística. Em poucas palavras o mesmo estabelece o seguinte.\n Suponha uma amostra $A = (x_1, x_2, \\dots, x_n)$ de uma variável aleatória com média $\\mu$ e variância $\\sigma^2$ finita. Se a amostragem é $i.i.d.$ o teorema central do limite estabelece que a distribuição de probababilidade da média amostral converge para uma distribuição normal com variância $\\sigma^2/n$ e média $\\mu$ a medida que $n$ aumenta.\n Note que eu não disse nada a respeito de como tal amostra foi gerada; em nenhum momento citei distribuição de Bernoulli, Gauss, Poisson, etc. Desta maneira podemos dizer que tal convergência é uma propriedade universal de amostras aleatórias $i.i.d.$. Essa universalidade é poderosa, pois garante que é possível estimar a média e variância de uma população através de um conjunto de amostragens.\nNão é difícil fazer um experimento computacional onde a implicação desse teorema apareça\nimport numpy as np import matplotlib.pyplot as plt import pandas as pd import seaborn as sns import warnings from matplotlib import style warnings.filterwarnings('ignore') style.use('seaborn-white') np.random.seed(22)  Usaremos uma amostragem de uma distribuição exponencial com média $\\mu = 4$. Tal distribuição tem uma variância dada por $1/\\mu^2$. Faremos $10000$ experimentos com amostras de tamanho $500$. Posteriormente calcularemos a media de cada experimento, mean_by_exp\nrate = 0.25 mu = 1/rate sample_size=500 exponential_sample = np.random.exponential(mu, size=(sample_size, 30000)) mean_by_exp = exponential_sample.mean(axis=0)  Agora basta plotar o histograma em comparação com a distribuição normal dada pelo teorema central do limite\nsns.distplot(mean_by_exp, norm_hist=True, label='sample') x = np.linspace(2.5, 5.5, 100) var = mu**2/(sample_size) y = np.exp(-(x-mu)**2/(2*var))/np.sqrt(2*np.pi*var) plt.plot(x, y, label=r'$N(\\mu, \\sigma)$', c='tomato') plt.legend() plt.xlim(3., 5) plt.savefig('exponential_distribution.png', facecolor='w') plt.close()  Note na figura acima que o plot para a função $\\frac{e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}}{\\sqrt(2\\pi\\sigma^2)}$ e o histograma coincidem. Você pode testar essa coincidência com outras distribuições, o mesmo comportamento se repetira. É isso que quero dizer com universalidade.\nUm questionamento válido é que estamos tratando apenas de uma variável aleatória e sua amostragem. Mas no mundo real existem outras estruturas mais intricadas. Por exemplo pegue um conjunto de variáveis aleatórias $\\mathcal C=(X_{1 1}, X_{1 2}, \\cdots, X_{N N})$, suponha que exista uma certa simetria nesse conjunto, uma possibilidade é $X_{i j} = X_{j i}$. Não é difícil imaginar situações onde tal conjunto apareça.\nPodemos armazenar uma realização de $\\mathcal C$ em uma matriz que nada mais é que um grafo completo com pesos. Ao estudar essas matrizes oriundas desse tipo de amostragem entramos em um novo campo da matemática, o campo das matrizes aleatórias. Nesse campo de estudos uma amostragem não retorna um número, mas sim uma matriz.\nA função normalRMT apresentada abaixo é um gerador de matrizes aleatórias conhecidas como Gaussianas ortogonais.\ndef normalRMT(n=100): \u0026quot;\u0026quot;\u0026quot;Generate a random matrix with normal distribution entries Args: n : (int) number of rows and columns Returns: m : (numpy.ndarray) random matrix \u0026quot;\u0026quot;\u0026quot; std = 1/np.sqrt(2) m = np.random.normal(size=(n,n), scale=std) m = (m+m.T) m /= np.sqrt(n) return m np.set_printoptions(precision=3) print(f'{normalRMT(3)},\\n\\n{normalRMT(3)}')  [[-1.441e+00 -2.585e-01 -1.349e-01] [-2.585e-01 -2.304e-01 1.166e-03] [-1.349e-01 1.166e-03 -1.272e+00]], [[-0.742 0.607 -0.34 ] [ 0.607 0.678 0.277] [-0.34 0.277 -0.127]]  Sabemos que quando estamos trantando de variáveis aleatórias o teorema central do limite é importantíssimo. O que você pode se perguntar agora é: Existe um análogo para o teorema central do limite para matrizes aleatórias?\n2-Núcleos atômicos, gás de números primos e universalidade Para o bem e para o mal o conhecimento da física atômica foi um dos temas mais importantes desenvolvidos pela humanidade. Portanto, não é de se estranhar que após o ano de 1930 iniciou-se uma grande corrida para compreender núcleos atômicos pesados e a física de nêutrons [13].\nPara compreender essa nova física de nêutrons era necessário conhecer a organização do espectro de ressonância dos núcleos pesados (esse espectro nada mais é que os autovalores de um operador muito especial). Uma maneira de se fazer isso é do jeito que muitas das coisas são estudadas na física: pegando se uma coisa e jogando na direção da coisa a ser estudada. Essa metodologia experimental torna possível amostrar alguns valores possíveis para o espectro. Contudo, acredito que não preciso argumentar que fazer isso naquela época era extremamente difícil e caro. Poucos centros conseguiam realizar alguns experimentos e ainda com uma resolução muito baixa para obter resultados suficientes para uma compreensão adequada dos núcleos. Era preciso uma saída mais barata e ela foi encontrada. Tal saída dependeu apenas de física-matemática e maços de papel.\nDentre os pioneiros que decidiram atacar o problema de núcleos pesados usando matemática temos Eugene Paul Wigner (Nobel de 1963). A grande sacada de Wigner foi perceber que o fato das interações nucleares serem tão complicadas e a infinitude de graus de liberdade seria possível tentar compreender essas interações como uma amostragem sujeita a certas condições de simetria.[10 , 11]\nAqui com simetria queremos dizer que as matrizes envolvidas possuem certas restrições tais como\nnp.assert_equal(A, A.T)  Na próxima seção veremos qual o impacto dessas restrições na distribuição de autovalores das matrizes envolvidas.\n2-a) Universalidade e lei do semicírculo A função normalRMT gera uma matriz simétrica onde as entradas são extraídas de uma distribuição normal. A função laplaceRMT gera também uma matriz simétrica, contudo as entradas são amostras de uma distribuição de Laplace.\ndef laplaceRMT(n=100): \u0026quot;\u0026quot;\u0026quot;Generate a random matrix with Laplace distribution Args: n : (int) size of the matrix Returns: m : (numpy.ndarray) random matrix with Laplace distribution \u0026quot;\u0026quot;\u0026quot; # we know that the variance of the laplace distribution is 2*scale**2 scale = 1/np.sqrt(2) m = np.zeros((n,n)) values = np.random.laplace(size=n*(n-1)//2, scale=scale) m[np.triu_indices_from(m, k=1)] = values # copy the upper diagonal to the lower diagonal m[np.tril_indices_from(m, k=-1)] = values np.fill_diagonal(m, np.random.laplace(size=n, scale=scale)) m = m/np.sqrt(n) return m  As propriedades universais que iremos explorar aqui estão ligadas aos autovalores das matrizes que foram amostradas. Como nossas matrizes são simétricas esses autovalores são todos reais.\nComo cada matriz é diferente os autovalores também serão, eles também são variáveis aleatórias.\nvals_laplace = np.array([ np.linalg.eigh(laplaceRMT(n=100))[0] for i in range(100) ]) vals_normal = np.array([ np.linalg.eigh(normalRMT(n=100))[0] for i in range(100) ])  Na decáda de 50 não havia poder computacional suficiente para realizar investigações númericas, mas você pode facilmente investigar como os autovalores se distribuem usando seu computador e gerando os histogramas\nt = 1 x = np.linspace(-2*t, 2*t, 100) y = np.zeros_like(x) x0 = x[4*t-x*2\u0026gt;0] y[4*t-x*2\u0026gt;0] = np.sqrt(4*t-x0**2)/(2*np.pi*t) plt.figure(facecolor='white') plt.hist(vals_laplace.flatten(), bins=50, hatch ='|', density=True, label='laplace', alpha=.2) plt.hist(vals_normal.flatten(), bins=50, hatch ='o', density=True, label='normal', alpha=.2) #sns.distplot(vals_laplace, norm_hist=True, label='Laplace') #sns.distplot(vals_normal, norm_hist=True, label='Normal') #sns.distplot(vals2, norm_hist=True, label='sample2') plt.plot(x, y, label='analytical') plt.xlabel(r'$\\lambda$') plt.ylabel(r'$\\rho(\\lambda)$') plt.legend() plt.savefig('RMT_distribution.png', facecolor='w') plt.close()  Veja na figura acima que a distribuição de autovalores de matrizes simétricas relacionadas com a distribuição normal e de Laplace coincidem. O que estamos vendo aqui é uma propriedade universal! Espero que você acredite em mim, mas dado que você tenha uma matriz aleatória simétrica, quadrada e se as entradas são $i.i.d.$ a distribuição de autovalores seguem o que é conhecido como lei de semicírculo de Wigner. Se a média e variância das entradas da matriz são $0$ e $1$ respectivamente, então tal lei tem a seguinte expressão para a distribuição de probabilidade dos autovalores $$ \\rho(\\lambda) = \\begin{cases} \\frac{\\sqrt{4-\\lambda^2}}{(2\\pi)} \\textrm{ se } 4-\\lambda^2 \\leq 0\\newline 0 \\textrm{ caso contrário.} \\end{cases} $$\nSe trocarmos as simetrias, restrições ou formato (array.shape[0]!=array.shape[1]) das matrizes podemos encontrar variações da distribuição apresentada acima. Exemplo se a matriz é complexa mas Hermitiana, ou se é \u0026ldquo;retangular\u0026rdquo; e real tal como algums matrizes que são usadas para otimizar carteiras de investimento. A próxima seção mostrará um caso com outro formato para universalidade.\n2-b) Repulsão entre números primos Inciamos nosso texto falando sobre como a teoria de matrizes aleatórias floreceu com os estudos estatísticos de núcleos atômicos pesados, especificamente nos trabalhos de Wigner. Embora tenha essa origem, muitas vezes ferramentas matemáticas desenvolvidas apenas por motivações práticas alcançam outros ramos da matemática. Brevemente discutirei aqui alguns pontos e relações com uma das conjecturas mais famosas da matemática: a hipótese de Riemann.\nQualquer pessoa com alguma curiosidade sobre matemática já ouviu falar sobre a hipótese de Riemann. Essa hipótese estabele uma relação entre os zeros da função zeta de Riemann e a distribuição de números primos. Dada sua importância os maiores ciêntistas do século XX se debruçaram sobre ela almejando a imortalidade. Um desses ciêntistas foi Hugh Montgomery[4].\nPor volta de 1970 Montgomery notou que os zeros da função zeta tinham uma certa propriedade cuirosa, pareciam repelir uns aos outros. Uma expressão foi obtidada, que é a seguinte\n$$ 1 - \\left( \\frac{\\sin (\\pi u)}{\\pi u}\\right)^2 + \\delta(u) $$\nNão se preocupe em entender a expressão acima, ela está aqui apenas for motivos estéticos. O que importa é que ela é simples, tão simples que quando Freeman Dyson - um dos gigantes da física-matemática - colocou os olhos sobre tal equação ele notou imediatamente que tal equação era idêntica a obtida no contexto de matrizes aleatórias Hermitianas (uma matriz é hermitiana se ela é igual a sua transporta conjugada) utilizadas para compreender o comportamento de núcleos de átomos pesados, tais como urânio. A imagem abaixo é uma carta escrita por Dyson.\nAs conexão entre um ferramental desenvolvido para estudar núcleos atômicos e números primos era realmente inesperada e talvez seja um dos caminhos para a prova da hipotese de Riemann[5, 2]. Contudo deixemos a história de lado, e voltemos ao ponto principal que é te dar outro exemplo de universalidade.\nLembra que Montgomery disse que parecia haver uma repulsão entre os zeros da função Zeta? O que seria esse conceito de repulsão em matrizes aleatórias? Vamos checar numericamente\nVoltaremos a usar nossas matrizes aleatórias geradas por distribuições Gaussianas e Laplacianas. Usando o mesmo conjunto de autovalores que obtivemos anteriormente iremos calular o espaçamento entre cada par de autovalores para cada realização de uma matriz aleatória. É bem fácil, basta chamar a função diff do numpy\ndiff_laplace = np.diff(vals_laplace, axis=1) diff_normal = np.diff(vals_normal, axis=1)  Agora o que faremos é estimar a densidade de probabilidade usnado KDE. Mas antes disso aqui vai uma dica:\n Evite o KDE do sklearn no seu dia a dia, a implementação é lenta e não flexivél. Difícilmente você conseguirá bons resultados com milhões de pontos. Aqui vou usar uma implementação de KDE mais eficiente você pode instalar ela execuntando o comando abaixo\n !pip install KDEpy  from KDEpy import FFTKDE estimator_normal = FFTKDE( bw='silverman').fit(diff_normal.flatten()) x_normal, probs_normal = estimator_normal.evaluate(100) mu_normal = np.mean(diff_normal, axis=1).mean() estimator_laplace = FFTKDE( bw='silverman').fit(diff_laplace.flatten()) x_laplace, probs_laplace = estimator_laplace.evaluate(100) mu_laplace = np.mean(diff_laplace, axis=1).mean()  goe_law = lambda x: np.pi*x*np.exp(-np.pi*x**2/4)/2 spacings = np.linspace(0, 4, 100) p_s = goe_law(spacings) plt.plot(spacings, p_s, label=r'GOE analítico', c='orange', linestyle='--') plt.plot( x_normal/mu_normal, probs_normal*mu_normal, linestyle=':', linewidth=2, zorder=1, label='normal', c='black') plt.plot(x_laplace/mu_laplace, probs_laplace*mu_laplace, zorder=2, linestyle='--', label='laplace', c='tomato') plt.legend() plt.savefig('RMT_diff_distribution.png', facecolor='w') plt.close()  O que as distribuições acima dizem é que dado sua matriz ser $i.i.d.$ quadrada e simétrica então a probabilidade que você encontre dois autovalores iguais é $0$ (zero). Além do mais, existe um ponto de máximo global em relação a distribuição de espaçamentos. Esse comportamento que balanceia repulsão e atração dos autovalores lembra o comportamento de partículas em um fluído. Não é de espantar que o método matemático desenvolvido por Wigner para compreender tais matrizes foi denominado Gás de Coloumb[2].\nAgora que você tem pelo menos uma ideia do que seria essa repulsão para o caso que já abordamos (matrizes simétricas quadradas) voltemos ao problema dos números primos.\nO comando a seguir baixa os primeiros 100k zeros da função zeta\n!wget http://www.dtc.umn.edu/~odlyzko/zeta_tables/zeros1  Um pequeno preprocessamento dos dados:\nzeros = [] with open('zeros1', 'r') as f: for line in f.readlines(): # remove all spaces in the line and convert it to a float zeros.append(float(line.replace(' ', ''))) zeta_zeros = np.array(zeros)  Iremos calcular os espaçamentos entre os zeros, a média de tais espaçamento e executar um KDE\nfrom KDEpy import FFTKDE diff_zeta = np.diff(zeta_zeros[10000:]) m = np.mean(diff_zeta) estimator = FFTKDE( bw='silverman').fit(diff_zeta)  x, probs = estimator.evaluate(100) p = np.pi goe_law = lambda x: p*x*np.exp(-p*x**2/4)/2 def gue(xs): arg = -4/np.pi*np.power(xs,2) vals = 32/np.pi**2*xs**2*np.exp(arg) return vals spacings = np.linspace(0, 4, 100) p_s = gue(spacings) p_s2 = goe_law(spacings) plt.plot(x/m, probs*m, label='zeros zeta', linestyle='--') plt.plot(spacings, p_s, label=r'GUE analítico', c='blue', linestyle='-.') plt.plot(spacings, p_s2, label=r'GOE analitico', c='orange', linestyle='-.') plt.xlim(-0.1, 4) plt.legend() plt.savefig('zeta.png', facecolor='w') plt.close()  Veja que a propriedade de repulsão apareceu novamente. Note que dentro do plot eu coloquei uma outra curva GOE analítico, essa curva é aquela que melhor descreve a distribuição de espaçamentos quando suas matrizes aleatórias são simétricas. Isso é uma lição importante aqui e resalta o que eu já disse anteriormente. Não temos apenas \u0026ldquo;um limite central para matrizes aleatórias\u0026rdquo;, mas todo um zoológico que mudará dependendo do tipo do seu problema..\n3-Usando RMT para encontrar e filtrar ruídos em matrizes Na seção 1 relembramos o resultado do teorema central do limite. Na seção 2 foi mostrado que devemos ter em mente as simetrias e restrições do nosso problema para analisar qual regra de universalidade é respeitada. Isto é: a depender da simetria e restrições das nossas matrizes temos um outro \u0026ldquo;timbre de universalidade\u0026rdquo;.\nUm exemplo de outro timbre surge no espectro de matrizes de correlação; matrizes que são comumente utilizadas para análise de carteiras de investimento. Tais matrizes tem pelo menos a seguinte estrutura:\n$$ \\mathbf C = \\mathbf X \\mathbf X^T $$ onde $\\mathbf X$ é uma matriz real $N\\times M$ e $M\u0026gt;N$.\nO código abaixo permite explorar em um exemplo o espectro de matrizes aleatórias $N\\neq M$ com entradas dadas pela distribuição normal.\ndef get_marchenko_bounds(Q, sigma=1): \u0026quot;\u0026quot;\u0026quot;Computes the Marchenko bounds for a given Q and sigma. Args: Q : (float) The Q-value. sigma : (float) The std value. Returns: (float, float): The lower and upper bounds for the eigenvalues. \u0026quot;\u0026quot;\u0026quot; QiSqrt = np.sqrt(1/Q) lp = np.power(sigma*(1 + QiSqrt),2) lm = np.power(sigma*(1 - QiSqrt),2) return lp, lm def marchenko_pastur(l, Q, sigma=1): \u0026quot;\u0026quot;\u0026quot;Return the probability of a Marchenko-Pastur distribution for a given Q , sigma and eigenvalue. Args: l : (float) The eigenvalue. Q : (float) The Q-value. sigma : (float) The std value. Returns: (float): The probability \u0026quot;\u0026quot;\u0026quot; lp, lm = get_marchenko_bounds(Q, sigma) # outside the interval [lm, lp] if l \u0026gt; lp or l \u0026lt; lm: return 0 return (Q/(2*np.pi*sigma*sigma*l))*np.sqrt((lp-l)*(l-lm)) def plot_marchenko_pastur(ax, eigen_values, Q, sigma=1, bins=100, just_the_bulk=False): \u0026quot;\u0026quot;\u0026quot;Plots the Marchenko-Pastur distribution for a given Q and sigma Args: ax : (matplotlib.axes) The axes to plot on. eigen_values : (np.array) The eigenvalues. Q : (float) : The Q-value. sigma : (float) std bins : (int) The number of bins to use. just_the_bulk : (bool) If True, only the eigenvalues inside of the Marchenko-Pastur bounds are plotted. \u0026quot;\u0026quot;\u0026quot; l_max, l_min = get_marchenko_bounds(Q, sigma) eigenvalues_points = np.linspace(l_min, l_max, 100) pdf = np.vectorize(lambda x : marchenko_pastur(x, Q, sigma))(eigenvalues_points) if just_the_bulk: eigen_values = eigen_values[ (eigen_values \u0026lt; l_max)] ax.plot(eigenvalues_points, pdf, color = 'r', label='Marchenko-Pastur') ax.hist(eigen_values, label='sample', bins=bins , density=True) ax.set_xlabel(r\u0026quot;$\\lambda$\u0026quot;) ax.set_ylabel(r\u0026quot;$\\rho$\u0026quot;) ax.legend() N = 1000 T = 4000 Q = T/N X = np.random.normal(0,1,size=(N,T)) cor = np.corrcoef(X) vals = np.linalg.eigh(cor)[0] fig, ax = plt.subplots(1,1) plot_marchenko_pastur(ax, vals, Q, sigma=1, bins=100) plt.legend() plt.savefig('Marchenko_Pastur.png', facecolor='w') plt.close()  A função em vermelho na figura acima é a universalidade que aparece em matrizes com a restrição $N\\times M$ e entradas $i.i.d.$ e média $0$. Tal universalidade tem como formato a distribuição de Marchenko-Pastur que é dada por\n$$ \\rho (\\lambda) = \\frac{Q}{2\\pi \\sigma^2}\\frac{\\sqrt{(\\lambda_{\\max} - \\lambda)(\\lambda - \\lambda_{\\min})}}{\\lambda} $$ onde $$ \\lambda_{\\max,\\min} = \\sigma^2(1 \\pm \\sqrt{\\frac{1}{Q}})^2. $$\nNote os parâmetros como $Q$ e $\\sigma$. Tais parâmetros precisam ser ajustados para obter um melhor fit com dados reais.\nAgora iremos para um caso real. Vamos usar dados obtidos via Yahoo Finance com a biblioteca yfinance para consturir uma matriz de correlação com dados de ativos financeiros\n# você precisa desse pacote para baixar os dados !pip install yfinance  Isso aqui é um post bem informal, então peguei peguei uma lista aleatória com alguns tickers que encontrei na internet\n!wget https://raw.githubusercontent.com/shilewenuw/get_all_tickers/master/get_all_tickers/tickers.csv  selecionei apenas 500 para evitar que o processo de download seja muito demorado\ntickers = np.loadtxt('tickers.csv', dtype=str, delimiter=',').tolist() tickers = np.random.choice(tickers, size=500, replace=False).tolist()  vamos baixar agora os dados em um periódo específico\nimport yfinance as yf df = yf.download (tickers, start=\u0026quot;2017-01-01\u0026quot;, end=\u0026quot;2019-10-01\u0026quot;, interval = \u0026quot;1d\u0026quot;, group_by = 'ticker', progress = True)  o yfinance vai gerar um dataframe com multiindex, então precisamos separar da forma que queremos\ntickers_available = list(set([ ticket for ticket, _ in df.columns.T.to_numpy()])) prices = pd.DataFrame() for ticker in tickers_available: try: prices[ticker] = df[(ticker, 'Adj Close')] except KeyError: pass  Agora iremos calcular o retorno. Aqui entra um ponto delicado. Você poderá achar alguns posts na internet ou mesmo artigos argumentando que é necessário calcular o retorno como $\\log (r+1)$ pois assim as entradas da sua matriz seguirá uma distribuição normal o que permitirá a aplicação de RMT. Já vimos no presente texto que não precisamos que as entradas da matrizes venham de uma distribuição normal para que a universalidade apareça. A escolha ou não de usar $\\log$ nos retornos merece mais atenção, inclusive com críticas em relação ao uso[6, 7, 8]. Mas esse post não pretende te vender nada, por isso vou ficar com o mais simples.\n# calculamos os retornos returns_all = prices.pct_change() # a primeira linha não faz sentido, não existe retorno no primeiro dia returns_all = returns_all.iloc[1:, :] # vamos limpar todas as linhas se mnegociação e dropar qualquer coluna com muitos NaN returns_all.dropna(axis = 1, thresh=len(returns_all.index)/2, inplace=True) returns_all.dropna(axis = 0, inplace=True) # seleciona apenas 150 colunas returns_all = returns_all[np.random.choice(returns_all.columns, size=120, replace=False)] #returns_all = returns_all.iloc[150:]  Com o df pronto calcularemos a matriz de correlação e seus autovalores\ncorrelation_matrix = returns_all.interpolate().corr() vals = np.linalg.eigh(correlation_matrix.values)[0]  Vamos usar os parâmetros padrões para $Q$ e $\\sigma$ e torcer para que funcione\nT, N = returns_all.shape Q=T/N sigma= 1 fig, ax = plt.subplots(1,1) plot_marchenko_pastur(ax, vals, Q, sigma=1, bins=200, just_the_bulk=False) plt.legend() plt.savefig('Marchenko_Pastur_all.png', facecolor='w') plt.close()  Usando todo o intervalo de tempo do nosso df obtivemos o que parece um ajuste razoável. É claro que você poderia (deveria) rodar algum teste estatistico para verificar tal ajuste. Existem alguns trabalhos que fizeram essa análise de forma rigorosa, comparando mercados e periódos específicos em relação a distribuição de Marchenko-Pastur[9].\nSe você for uma pessoa atenta notará que na imagem acima existem alguns autovalores fora do suporte da Marchenko-Pastur. A ideia de filtragem via RMT é como dito em [9] testar seus dados em relação a \u0026ldquo;hipótese nula\u0026rdquo; da RMT. No caso se seus autovalores estão dentro do bulk da distribuição que descreve um modelo de entradas i.i.d..\nComo isso foi aplicado em alguns trabalhos? Vamos ver na prática.\nUsaremos $70$% da série histórica para calcular uma nova matriz de correlação. Com a matriz de correlação em mãos vamos computar os autovalores e autovetores.\n# iremos usar 70% da serie para realizar a filtragem returns_all.shape[0]*0.70 n_days = returns_all.shape[0] n_days_in = int(n_days*(1-0.70)) returns = returns_all.copy() sample = returns.iloc[:(returns.shape[0]-n_days_in), :].copy() correlation_matrix = sample.interpolate().corr() vals, vecs = np.linalg.eigh(correlation_matrix.values)  Os autovalores e autovetores podem ser compreendidos como a decomposição de uma dada matriz. Portanto, o seguinte teste precisa passar\nassert np.abs( np.dot(vecs, np.dot(np.diag(vals), np.transpose(vecs))).flatten() - correlation_matrix.values.flatten() ).max() \u0026lt; 1e-10  A distribuição de Marchenko-Pastur serve como um indicativo para nossa filtragem. O que faremos é jogar fora todos os autovalores que estão dentro da distribuição de Marchenko-Pastur, posteriormente reconstruiremos a matriz de correlação.\nT, N = returns.shape Q=T/N sigma = 1 lp, lm = get_marchenko_bounds(Q, sigma) # Filter the eigenvalues out vals[vals \u0026lt;= lp ] = 0 # Reconstruct the matrix filtered_matrix = np.dot(vecs, np.dot(np.diag(vals), np.transpose(vecs))) np.fill_diagonal(filtered_matrix, 1)  Com a matriz de correlação filtrada você pode fazer o que bem entender com ela - existem outras maneiras de se realizar uma filtragem - uma das possíveis aplicações que precisa ser utilizada com cuidado é usar tal matriz filtrada como input para algoritmos de otimização de carteira. Talvez faça um outro post descrevendo essa otimização de forma mais clara, mas esse não é meu enfoque nesse post e nem minha especialidade. Portanto, se você quiser dar uma lida recomendo os seguintes posts: [17, 18]\nO que você precisa saber é que uma matriz de covariância, $\\mathbf C_\\sigma$, adimite uma decomposição em relação a matriz de correlação atráves da seguinte forma\n$$ \\mathbf C_\\sigma = \\mathbf D^{-1/2} \\mathbf C \\mathbf D^{-1/2} $$ onde $\\mathbf D^{-1/2}$ é uma matriz diagonal com as entradas sendo os desvios padrão para cada serie de dados, isto é\n$$ \\begin{bmatrix} \\sigma_{1} \u0026amp;0 \u0026amp;\\cdots \u0026amp;0 \\\\ 0 \u0026amp;\\sigma_{2} \u0026amp;\\cdots \u0026amp;0 \\\\ \\vdots \u0026amp;\\vdots \u0026amp;\\ddots \u0026amp;\\vdots \\\\ 0 \u0026amp;0 \u0026amp;\\cdots \u0026amp;\\sigma_{M} \\end{bmatrix} $$\nDiscutimos uma maneira de obter uma matriz de correlação filtrada, $\\mathbf{\\tilde C}$, através de RMT, a ideia é plugar essa nova matriz na equação anterior e obter uma nova matriz de covariância onde as informações menos relevantes foram eliminadas.\n$$ \\mathbf{\\tilde C_\\sigma} = \\mathbf D^{-1/2} \\mathbf{\\tilde C} \\mathbf D^{-1/2}. $$\nTendo essa nova matriz de covâriancia filtrada agora basta você ingerir ela em algum método preferido para otimização e comparar com o resultado obtido usando a matriz original. Aqui usaremos o clássico Markowitz\n# Reconstruct the filtered covariance matrix covariance_matrix = sample.cov() inv_cov_mat = np.linalg.pinv(covariance_matrix) # Construct minimum variance weights ones = np.ones(len(inv_cov_mat)) inv_dot_ones = np.dot(inv_cov_mat, ones) min_var_weights = inv_dot_ones/ np.dot( inv_dot_ones , ones) variances = np.diag(sample.cov().values) standard_deviations = np.sqrt(variances) D = np.diag(standard_deviations) filtered_cov = np.dot(D ,np.dot(filtered_matrix,D)) filtered_cov = filtered_matrix filtered_cov = (np.dot(np.diag(standard_deviations), np.dot(filtered_matrix,np.diag(standard_deviations)))) filt_inv_cov = np.linalg.pinv(filtered_cov) # Construct minimum variance weights ones = np.ones(len(filt_inv_cov)) inv_dot_ones = np.dot(filt_inv_cov, ones) filt_min_var_weights = inv_dot_ones/ np.dot( inv_dot_ones , ones) def get_cumulative_returns_over_time(sample, weights): weights[weights \u0026lt;= 0 ] = 0 weights = weights / weights.sum() return (((1+sample).cumprod(axis=0))-1).dot(weights) cumulative_returns = get_cumulative_returns_over_time(returns, min_var_weights).values cumulative_returns_filt = get_cumulative_returns_over_time(returns, filt_min_var_weights).values  in_sample_ind = np.arange(0, (returns.shape[0]-n_days_in+1)) out_sample_ind = np.arange((returns.shape[0]-n_days_in), returns.shape[0]) f = plt.figure() ax = plt.subplot(111) points = np.arange(0, len(cumulative_returns))[out_sample_ind] ax.plot(points, cumulative_returns[out_sample_ind], 'orange', linestyle='--', label='original') ax.plot(points, cumulative_returns_filt[out_sample_ind], 'b', linestyle='-.', label='filtrado') ymax = max(cumulative_returns[out_sample_ind].max(), cumulative_returns_filt[out_sample_ind].max()) ymin = min(cumulative_returns[out_sample_ind].min(), cumulative_returns_filt[out_sample_ind].min()) plt.legend() plt.savefig('comp.png', facecolor='w') plt.close()  Obtivemos uma melhora, mas novamente ressaltamos que uma analise mais criteriosa deveria ter sido feita. Vamos listar alguns pontos\n Em relação a questão da escolha do intervalo de tempo. Isto é, se o tamanho foi pequeno de mais para capturar a correlação ou se foi grande de mais tal que as correlações entre ativos não são estacionárias. O (não) uso do $\\log$-retorno e seu impacto Uma escolha não aleatória do que seria analisado Métodos de unfolding dos autovalores (tema para outro post)  5 - Vantagens, críticas e sugestões Você poderá encontrar alguns trabalhos e posts descrevendo o uso de matrizes aleatórias para filtragem de matrizes de correlação sem uma boa crítica ou explicitação das limitações vou linkar aqui alguns pontos positivos e negativos e limitações\nOnde realmente RMT se mostrou útil  Obviamente a RMT é indiscutivelmente bem sucedida na matemática e física permitindo compreender sistemas apenas analisando a estatística dos gases matriciais. Em machine learning a RMT também está provando ser uma ferramenta útil para compreender e melhorar o processo de aprendizado [15]. Entender comportamentos de sistemas sociais, biológicos e econômicos. Aqui com entender o comportamento digo apenas saber se um dado segue uma característica dada por alguma lei específica como a lei de semicírculo. Isto é, não existe discussão em você pegar um dado sistema que é representado por uma matriz, estudar o comportamento do seu espectro de autovalores e autovetores e verificar que seguem algumas lei de universalidade. Isso é bem diferente de dizer que se você filtrar uma matriz de correlação via RMT você irá obter sempre resultados melhores.  Limitações  Note que não realizamos nenhum tipo de teste para decidir se realmente a distribuição de autovalores era a distribuição desejada. Baseamos isso só no olhometro, obviamente não é uma boa ideia. A filtragem apenas removendo os autovalores apesar de simples é limitada e pode ser contra produtiva, outros métodos de filtragem podem ser inclusive melhores[14]. Inclusive não é uma das únicas aplicações de RMT para tratamento desse tipo de dado [16]  Para conhecer mais Ciêntistas  Alguns grandes nomes de RMT: Madan Lal Mehta, Freeman Dyson e Terrence Tao Alguns brasileiros: Marcel Novaes autor do livro Introduction to Random Matrices - Theory and Practice- arxiv; Fernando Lucas Metz trabalhou com o Nobel Giorgio Parisi.  Encontrou um erro ou quer melhorar esse texto?  Faça sua contribuição criando uma issue ou um PR editando esse arquivo aqui random_matrix_theory/index.md.  6-Referências   [1] M. Kac, “Can One Hear the Shape of a Drum?,” The American Mathematical Monthly, vol. 73, no. 4, p. 1, Apr. 1966, doi: 10.2307/2313748.\n  [2] Wigner, E.P., 1957. Statistical properties of real symmetric matrices with many dimensions (pp. 174-184). Princeton University.\n  [4] “From Prime Numbers to Nuclear Physics and Beyond,” Institute for Advanced Study. https://www.ias.edu/ideas/2013/primes-random-matrices (accessed Sep. 30, 2020).\n  [5] “GUE hypothesis,” What’s new. https://terrytao.wordpress.com/tag/gue-hypothesis/ (accessed Nov. 22, 2021).\n  [6] R. Hudson and A. Gregoriou, “Calculating and Comparing Security Returns is Harder than you Think: A Comparison between Logarithmic and Simple Returns,” Social Science Research Network, Rochester, NY, SSRN Scholarly Paper ID 1549328, Feb. 2010. doi: 10.2139/ssrn.1549328.\n  [7] A. Meucci, “Quant Nugget 2: Linear vs. Compounded Returns – Common Pitfalls in Portfolio Management,” Social Science Research Network, Rochester, NY, SSRN Scholarly Paper ID 1586656, May 2010. Accessed: Dec. 01, 2021. [Online]. Available: https://papers.ssrn.com/abstract=1586656\n  [8] Lidian, “Analysis on Stocks: Log(1+return) or Simple Return?,” Medium, Sep. 18, 2020. https://medium.com/@huangchingchiu/analysis-on-stocks-log-1-return-or-simple-return-371c3f60fab2 (accessed Nov. 25, 2021).\n  [9] N. A. Eterovic and D. S. Eterovic, “Separating the Wheat from the Chaff: Understanding Portfolio Returns in an Emerging Market,” Social Science Research Network, Rochester, NY, SSRN Scholarly Paper ID 2161646, Oct. 2012. doi: 10.2139/ssrn.2161646.\n  [10] E. P. Wigner, “Characteristic Vectors of Bordered Matrices With Infinite Dimensions,” Annals of Mathematics, vol. 62, no. 3, pp. 548–564, 1955, doi: 10.2307/1970079.\n  [11] E. P. Wigner, “On the statistical distribution of the widths and spacings of nuclear resonance levels,” Mathematical Proceedings of the Cambridge Philosophical Society, vol. 47, no. 4, pp. 790–798, Oct. 1951, doi: 10.1017/S0305004100027237.\n  [13] F. W. K. Firk and S. J. Miller, “Nuclei, Primes and the Random Matrix Connection,” Symmetry, vol. 1, no. 1, pp. 64–105, Sep. 2009, doi: 10.3390/sym1010064.\n  [14] L. Sandoval, A. B. Bortoluzzo, and M. K. Venezuela, “Not all that glitters is RMT in the forecasting of risk of portfolios in the Brazilian stock market,” Physica A: Statistical Mechanics and its Applications, vol. 410, pp. 94–109, Sep. 2014, doi: 10.1016/j.physa.2014.05.006.\n  [15] M. E. A. Seddik, C. Louart, M. Tamaazousti, and R. Couillet, “Random Matrix Theory Proves that Deep Learning Representations of GAN-data Behave as Gaussian Mixtures,” arXiv:2001.08370 [cs, stat], Jan. 2020, Accessed: Dec. 05, 2021. [Online]. Available: http://arxiv.org/abs/2001.08370\n  [16] D. B. Aires, “Análise de crises financeiras brasileiras usando teoria das matrizes aleatórias,” Universidade Estadual Paulista (Unesp), 2021. Accessed: Dec. 05, 2021. [Online]. Available: https://repositorio.unesp.br/handle/11449/204550\n  [17] S. Rome, “Eigen-vesting II. Optimize Your Portfolio With Optimization,” Scott Rome, Mar. 22, 2016. http://srome.github.io//Eigenvesting-II-Optimize-Your-Portfolio-With-Optimization/ (accessed Dec. 05, 2021).\n  [18] “11.1 Portfolio Optimization — MOSEK Fusion API for Python 9.3.10.” https://docs.mosek.com/latest/pythonfusion/case-studies-portfolio.html (accessed Dec. 05, 2021).\n  ","date":1638748800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638748800,"objectID":"b3eeddcf82fa78f2c7c70e4b563fa573","permalink":"/post/random_matrix_portfolio/","publishdate":"2021-12-06T00:00:00Z","relpermalink":"/post/random_matrix_portfolio/","section":"post","summary":"Como é possível ouvir matrizes de correlação usando seu espectro?Como podemos analisar esse \"*barulho*\" usando resultados da teoria de matrizes aleatórias para aprimorar algoritmos de construção de carterias de investimento?","tags":["portfolio","matrizes aleatórias","random matrix","matrix","spectral analysis","physics","statistics","eigenvalues","python","otmização","autovalores","história da ciência"],"title":"Variações do teorema central do limite para matrizes aleatórias.","type":"post"},{"authors":null,"categories":null,"content":"Helios repo  Helios is a Python library aiming to provide an easy way to visualize huge networks dynamically. Helios also provides visualizations through an interactive Stadia-like streaming system in which users can be collaboratively access (and share) visualizations created in a server or through Jupyter Notebook/Lab environments. It incorporates state-of-the-art layout algorithms and optimized rendering techniques (powered by FURY).\n General Information • Key Features • Installation • Usage • History • Credits General Information  Website and Documentation: https://heliosnetwork.io/ Examples: https://heliosnetwork.io/examples_gallery/index.html Blog: https://heliosnetwork.io/blog.html Free software: MIT license Community: Come to chat on Discord  Key Features  Force-directed layout using octrees Minimum-distortion embeddings ForceAtlas2 using cugraph Interactive local and Remote rendering in Jupyter Notebooks WebRTC or MJPEG interactive streaming system  Installation Use pip install pointed to this repository:\npip git+https://github.com/fury-gl/helios.git  As an alternative, Helios can be installed from the source code through the following steps:\n  Step 1. Get the latest source by cloning this repo:\ngit clone https://github.com/fury-gl/helios.git    Step 2. Install requirements:\npip install -r requirements.txt    Step 3. Install Helios\nAs a local project installation using:\n pip install .  Or as an \u0026ldquo;editable\u0026rdquo; installation using:\n pip install -e .    Step 4: Enjoy!\n  For more information, see also installation page on heliosnetwork.io\nDependencies Helios requires Python 3.7+ and the following mandatory dependencies:\n numpy \u0026gt;= 1.7.1 vtk \u0026gt;= 8.1.0 fury  To enable WebRTC streaming and enable optimizations to the streaming system, install the following optional packages:\n  Required for WebRTC streaming\n aiohttp aiortc    Optional packages that may improve performance\n cython opencv    Testing After installation, you can install test suite requirements:\npip install -r requirements_dev.txt  And to launch test suite:\npytest -svv helios  Usage There are many ways to start using Helios:\n Go to Getting Started Explore our Examples or API.  Example usage:\nfrom helios import NetworkDraw from helios.layouts import HeliosFr import numpy as np vertex_count = 8 edges = np.array([ [0,1], [0,2], [1,2], [2,3], [3,4], [3,5], [4,5], [5,6], [6,7], [7,0] ]); centers = np.random.normal(size=(vertex_count, 3)) network_draw = NetworkDraw( positions=centers, edges=edges, colors=(0.25,0.25,0.25), scales=1, node_edge_width=0, marker='s', edge_line_color=(0.5,0.5,0.5), window_size=(600, 600) ) layout = HeliosFr(edges, network_draw) layout.start() network_draw.showm.initialize() network_draw.showm.start()  History Helios project started as a replacement to the desktop version of the Networks 3D tools. The project evolved quickly along the summer of 2021 due to the GSoC’21 under the responsibility of the Python Software Foundation and the FURY team. The majority of the initial work has been done by @devmessias mentored by @filipinascimento and @skoudoro. The GSoC’21 project associated with Helios is “A system for collaborative visualization of large network layouts using FURY”. Check out the final report for more information.\n","date":1631555002,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631555002,"objectID":"d68fc333af60a0056d5f4a42577c030e","permalink":"/project/helios/","publishdate":"2021-09-13T17:43:22Z","relpermalink":"/project/helios/","section":"project","summary":"Helios is a Python library that provides an easy way to visualize huge networks dynamically. Helios also provides visualizations through an interactive Stadia-like streaming using WebRTC ","tags":["graphs","python","WebRTC","data viz","data science","Complex Networks","open-software"],"title":"Helios: graph layout viz and streaming","type":"project"},{"authors":null,"categories":null,"content":" Detailed weekly tasks, progress and work done can be found here\n Abstract We have changed some points of my project in the first meeting. Specifically, we focused the efforts into developing a streaming system using the WebRTC protocol that could be used in more generic scenarios than just the network visualization. In addition to that, we have opted to develop the network visualization for fury as a separated repository and package available here. The name Helios was selected for this new network visualization system based on the Fury rendering pipeline.\nProposed Objectives  Create a streaming system (stadia-like) for FURY  Should work in a low-bandwidth scenario Should allow user interactions and collaboration across the Internet using a web-browser   Helios Network System objectives:  Implement the Force-Directed Algorithm with examples Implement the ForceAtlas2 algorithm using cugraph with examples Implement Minimum-Distortion Embeddings algorithm (PyMDE) and examples Non-blocking network algorithms computation avoiding the GIL using the Shared Memory approach Create the documentation and the actions for the CI   Stretch Goals:  Create an actor in FURY to draw text efficiently using shaders Add support to draw millions of nodes using FURY Add support to control the opengl state on FURY    Objectives Completed   Create a streaming system (stadia-like) for FURY\nThere are several reasons to have a streaming system for data visualization. Because I am doing my Ph.D. in developing country, I always need to think of the less expensive solutions to use the computational resources available. For example, with the GPU\u0026rsquo;s prices increasing, it is necessary to share the a single machine with GPU with other users at different locations.\nTo construct the streaming system for my project we have opted to follow three main properties and behaviors:\n avoid blocking the code execution in the main thread (where the vtk/fury instance resides) work inside of a low bandwidth environment make it easy and cheap to share the rendering result. For example, using the free version of ngrok  To achieve the first property we need to circumvent the GIL and allow python code to execute in parallel. Using the threading module alone is not good enough to reach real parallelism as Python calls in the same process can not execute concurrently. In addition to that, to achieve better organization it is desirable to define the server system as an uncoupled module from the rendering pipeline. Therefore, I have chosen to employ the multiprocessing approach for that. The second and third property can be only achieved choosing a suitable protocol for transfering the rendered results to the client. We have opted to implement two streaming protocols: the MJPEG and the WebRTC. The latter is more suitable for low-bandwidth scenarios [1].\nThe image below shows a simple representation of the streaming system.\n\u0026lt;center\u0026gt;  \u0026lt;/center\u0026gt;  The video below shows how our streaming system works smothly and can be easily integrated inside of a Jupyter notebook.\n   Video: WebRTC Streaming + Ngrok\n Video: WebRTC Streaming + Jupyter\nPull Requests: * https://github.com/fury-gl/fury/pull/480\n  2D and 3D marker actor\nThis feature gave FURY the ability to efficiently draw millions of markers and impostor 3D spheres. This feature was essential for the development of Helios. This feature work with signed distance fields (SDFs) you can get more information about how SDFs works here [4] .\nThe image below shows 1 million of markers rendered using an Intel HD graphics 3000.\n    Fine-Tunning the OpenGl State\nSometimes users may need to have finer control on how OpenGL will render the actors. This can be useful when they need to create specialized visualization effects or to improve the performance.\nIn this PR I have worked in a feature that allows FURY to control the OpenGL context created by VTK\nPull Request:\n  https://github.com/fury-gl/fury/pull/432    Helios Network Visualization Lib: Network Layout Algorithms\nCase 1: Suppose that you need to monitor a hashtag and build a social graph. You want to interact with the graph and at the same time get insights about the structure of the user interactions. To get those insights you can perform a node embedding using any kind of network layout algorithm, such as force-directed or minimum distortion embeddings.\nCase 2: Suppose that you are modelling a network dynamic such as an epidemic spreading or a Kuramoto model. In some of those network dynamics a node can change the state and the edges related to the node must be deleted. For example, in an epidemic model a node can represent a person who died due to a disease. Consequently, the layout of the network must be recomputed to give better insights.\nIn the described cases, if we want a better (UX) and at the same time a more practical and insightful application of Helios, the employed layout algorithms should not block any kind of computation in the main thread.\nIn Helios we already have a lib written in C (with a python wrapper) which performs the force-directed layout algorithm using separated threads avoiding the GIL problem and consequently avoiding blocking the main thread. But what about the other open-source network layout libs available on the internet? Unfortunately, most of those libs have not been implemented like Helios force-directed methods and consequently, if we want to update the network layout the Python interpreter will block the computation and user interaction in your network visualization.\nMy solution for having PyMDE and CuGraph-ForceAtlas not blocking the main thread was to break the network layout method into two different types of processes: A and B and communicate both process using the Shared Memory approach. You can more information about this PR through my following posts [2], [3].\n  The image below show an example that I made and is available at https://github.com/fury-gl/helios/blob/main/docs/examples/viz_mde.py\nPull Requests:\n MDE Layout: https://github.com/fury-gl/helios/pull/6 CuGraph ForceAtlas2 https://github.com/fury-gl/helios/pull/13 Force-Directed and MDE improvements https://github.com/fury-gl/helios/pull/14 Helios Network Visualization Lib: Visual Aspects  I\u0026rsquo;ve made several stuffs to give Helios a better visual aspects. One of them was to give a smooth real-time network layout animations. Because the layout computations happens into a different process that the process responsible to render the network was necessary to record the positions and communicate the state of layout between both process.\nThe GIF below shows how the network layout through IPC behaved before these modification\n\u0026lt;center\u0026gt;  \u0026lt;/center\u0026gt;  below, you can see how after those modifications the visual aspect is better.\n\u0026lt;center\u0026gt;  \u0026lt;/center\u0026gt;  Pull Requests:\n OpenGL SuperActors: https://github.com/fury-gl/helios/pull/1 Fixed the flickering effect https://github.com/fury-gl/helios/pull/10 Improvements in the network node visual aspects https://github.com/fury-gl/helios/pull/15 Smooth animations when using IPC layouts https://github.com/fury-gl/helios/pull/17 Helios Network Visualization Lib: CI and Documentation  Because Helios was an project that begins in my GSoC project It was necessary to create the documentation, hosting and more. Now we have a online documentation available at https://heliosnetwork.io/ altough the documentation still need some improvements.\nBelow is presented the Helios Logo which was developed by my mentor Filipi Nascimento.\n\u0026lt;center\u0026gt;  \u0026lt;/center\u0026gt;  Pull Requests:\n  CI and pytests: https://github.com/fury-gl/helios/pull/5, https://github.com/fury-gl/helios/pull/20\n  Helios Logo, Sphinx Gallery and API documentation https://github.com/fury-gl/helios/pull/18\n  Documentation improvements: https://github.com/fury-gl/helios/pull/8\n  Objectives in Progress\n  Draw texts on FURY and Helios\nThis two PRs allows FURY and Helios to draw millions of characters in VTK windows instance with low computational resources consumptions. I still working on that, finishing the SDF font rendering which the theory behinds was developed here [5].\nPull Requests:\n   https://github.com/fury-gl/helios/pull/24\n   https://github.com/fury-gl/fury/pull/489\n\u0026lt;center\u0026gt;  \u0026lt;/center\u0026gt;      GSoC weekly Blogs\nWeekly blogs were added to the FURY Website.\nPull Requests:\n First Evaluation: https://github.com/fury-gl/fury/pull/476 Second Evaluation: TBD    References [1] ( Python GSoC - Post #1 - A Stadia-like system for data visualization - demvessias s Blog, n.d.; https://blogs.python-gsoc.org/en/demvessiass-blog/post-1-a-stadia-like-system-for-data-visualization/\n[2] Python GSoC - Post #2: SOLID, monkey patching a python issue and network layouts through WebRTC - demvessias s Blog, n.d.; https://blogs.python-gsoc.org/en/demvessiass-blog/post-2-solid-monkey-patching-a-python-issue-and-network-layouts-through-webrtc/\n[3] Python GSoC - Post #3: Network layout algorithms using IPC -demvessias s Blog, n.d.) https://blogs.python-gsoc.org/en/demvessiass-blog/post-3-network-layout-algorithms-using-ipc/\n[4] Rougier, N.P., 2018. An open access book on Python, OpenGL and Scientific Visualization [WWW Document]. An open access book on Python, OpenGL and Scientific Visualization. URL https://github.com/rougier/python-opengl (accessed 8.21.21).\n[5] Green, C., 2007. Improved alpha-tested magnification for vector textures and special effects, in: ACM SIGGRAPH 2007 Courses on -SIGGRAPH \u0026lsquo;07. Presented at the ACM SIGGRAPH 2007 courses, ACM Press, San Diego, California, p. 9. https://doi.org/10.1145/1281500.1281665\n","date":1629676800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629676800,"objectID":"49dfccf7db50455ac1b10dbc337c49c2","permalink":"/post/2021-23-08-gsoc-devmessias-final-report/2021-23-08-gsoc-devmessias-final-report/","publishdate":"2021-08-23T00:00:00Z","relpermalink":"/post/2021-23-08-gsoc-devmessias-final-report/2021-23-08-gsoc-devmessias-final-report/","section":"post","summary":"Detailed weekly tasks, progress and work done can be found here\n Abstract We have changed some points of my project in the first meeting. Specifically, we focused the efforts into developing a streaming system using the WebRTC protocol that could be used in more generic scenarios than just the network visualization.","tags":["gsoc","python","open-source","data-viz","webrtc"],"title":"GSoC-  Google Summer of Code 2021 Final Work Product","type":"post"},{"authors":null,"categories":null,"content":"What did I do this week? FURY   PR fury-gl/fury#489:  | I\u0026rsquo;ve created the PR that will allow FURY to draw hundreds thousands of labels using texture maps. By default, this PR give to FURY three pre-built texture maps using different fonts. However, is quite easy to create new fonts to be used in a visualization. | It\u0026rsquo;s was quite hard to develop the shader code and find the correct positions of the texture maps to be used in the shader. Because we used the freetype-py to generate the texture and packing the glyps. However, the lib has some examples with bugs. But fortunelly, now everthing is woking on FURY. I\u0026rsquo;ve also created two different examples to show how this PR works.\n* The first example, viz_huge_amount_of_labels.py, shows that feature has a realy good performance. The user can draw hundreds of thounsands of characters in a regular computer. ![](https://user-images.githubusercontent.com/6979335/129643743-6cb12c06-3415-4a02-ba43-ccc97003b02d.png) * The second example, viz_billboad_labels.py, shows the different behaviors of the label actor. In addition, presents to the user how to create a new texture atlas font to be used across different visualizations.    PR fury-gl/fury#437:   Fix: avoid multiple OpenGl context on windows using asyncio\nThe streaming system must be generic, but opengl and vtk behaves in uniques ways in each Operating System. Thus, can be tricky to have the same behavior acrros different OS. One hard stuff that we founded is that was not possible to use my TimeIntervals objects (implemented with threading module) with vtk. The reason for this impossibility is because we can't use vtk in windows in different threads. But fortunely, moving from the threading (multithreading) to the asyncio approcach (concurrency) have fixed this issue and now the streaming system is ready to be used anywhere.    Flickering\nFinally, I could found the cause of the flickering effect on the streaming system. This flickering was appearing only when the streaming was created using the Widget object. The cause seems to be a bug or a strange behavior from vtk. Calling      iren.MouseWheelForwardEvent() or\niren.MouseWheelBackwardEvent() inside of a thread without invoking the Start method from a vtk instance produces a memory corruption. Fortunately, I could fix this behavior and now the streaming system is working without this glitch effect.\nFURY/Helios    PR fury-gl/helios#24 :  This uses the PRfury-gl/fury#489: to give the network label feature to helios. Is possible to draw node labels, update the colors, change the positions at runtime. In addition, when a network layout algorithm is running this will automatically update the node labels positions to follow the nodes across the screen.\n  PR fury-gl/helios#23: Merged.  This PR granted compatibility between IPC Layouts and Windows. Besides that , now is quite easier to create new network layouts using inter process communication\nDid I get stuck anywhere? I did not get stuck this week.\n","date":1629072e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629072e3,"objectID":"2780ba80ce6607d6e39d3ca426f835fe","permalink":"/post/2021-16-08-gsoc-devmessias-11/2021-16-08-gsoc-devmessias-11/","publishdate":"2021-08-16T00:00:00Z","relpermalink":"/post/2021-16-08-gsoc-devmessias-11/2021-16-08-gsoc-devmessias-11/","section":"post","summary":"What did I do this week? FURY   PR fury-gl/fury#489:  | I\u0026rsquo;ve created the PR that will allow FURY to draw hundreds thousands of labels using texture maps. By default, this PR give to FURY three pre-built texture maps using different fonts.","tags":["gsoc","python","open-source","data-viz","webrtc"],"title":"GSoC-  SDF fonts and OpenGL","type":"post"},{"authors":null,"categories":null,"content":"Hi all. In the past weeks, I\u0026rsquo;ve been focusing on developing Helios; the network visualization library for FURY. I improved the visual aspects of the network rendering as well as implemented the most relevant network layout methods.\nIn this post I will discuss the most challenging task that I faced to implement those new network layout methods and how I solved it.\nThe problem: network layout algorithm implementations with a blocking behavior Case 1: Suppose that you need to monitor a hashtag and build a social graph. You want to interact with the graph and at the same time get insights about the structure of the user interactions. To get those insights you can perform a node embedding using any kind of network layout algorithm, such as force-directed or minimum distortion embeddings.\nCase 2: Suppose that you are modelling a network dynamic such as an epidemic spreading or a Kuramoto model. In some of those network dynamics a node can change the state and the edges related to the node must be deleted. For example, in an epidemic model a node can represent a person who died due to a disease. Consequently, the layout of the network must be recomputed to give better insights.\nIn described cases if we want a better (UX) and at the same time a more practical and insightful application of Helios layouts algorithms shouldn\u0026rsquo;t block any kind of computation in the main thread.\nIn Helios we already have a lib written in C (with a python wrapper) which performs the force-directed layout algorithm using separated threads avoiding the GIL problem and consequently avoiding the blocking. But and the other open-source network layout libs available on the internet? Unfortunately, most of those libs have not been implemented like Helios force-directed methods and consequently, if we want to update the network layout the python interpreter will block the computation and user interaction in your network visualization. How to solve this problem?\nWhy is using the python threading is not a good solution? One solution to remove the blocking behavior of the network layout libs like PyMDE is to use the threading module from python. However, remember the GIL problem: only one thread can execute python code at once. Therefore, this solution will be unfeasible for networks with more than some hundreds of nodes or even less! Ok, then how to solve it well?\nIPC using python As I said in my previous posts I\u0026rsquo;ve created a streaming system for data visualization for FURY using webrtc. The streaming system is already working and an important piece in this system was implemented using the python SharedMemory from multiprocessing. We can get the same ideas from the streaming system to remove the blocking behavior of the network layout libs.\nMy solution to have PyMDE and CuGraph-ForceAtlas without blocking was to break the network layout method into two different types of processes: A and B. The list below describes the most important behaviors and responsibilities for each process\nProcess A:\n Where the visualization (NetworkDraw) will happen Create the shared memory resources: edges, weights, positions, info.. Check if the process B has updated the shared memory resource which stores the positions using the timestamp stored in the info_buffer Update the positions inside of NetworkDraw instance  Process B:\n Read the network information stored in the shared memory resources: edges , weights, positions Execute the network layout algorithm Update the positions values inside of the shared memory resource Update the timestamp inside of the shared memory resource  I used the timestamp information to avoid unnecessary updates in the FURY/VTK window instance, which can consume a lot of computational resources.\nHow have I implemented the code for A and B? Because we need to deal with a lot of different data and share them between different processes I\u0026rsquo;ve created a set of tools to deal with that, take a look for example in the ShmManagerMultiArrays Object , which makes the memory management less painful.\nI'm breaking the layout method into two different processes. Thus I\u0026rsquo;ve created two abstract objects to deal with any kind of network layout algorithm which must be performed using inter-process-communication (IPC). Those objects are: NetworkLayoutIPCServerCalc ; used by processes of type B and NetworkLayoutIPCRender ; which should be used by processes of type A.\nI\u0026rsquo;ll not bore you with the details of the implementation. But let\u0026rsquo;s take a look into some important points. As I\u0026rsquo;ve said saving the timestamp after each step of the network layout algorithm. Take a look into the method _check_and_sync from NetworkLayoutIPCRender here. Notice that the update happens only if the stored timestamp has been changed. Also, look at this line helios/layouts/mde.py#L180, the IPC-PyMDE implementation This line writes a value 1 into the second element of the info_buffer. This value is used to inform the process A that everything worked well. I used that info for example in the tests for the network layout method, see the link helios/tests/test_mde_layouts.py#L43\nResults Until now Helios has three network layout methods implemented: Force Directed , Minimum Distortion Embeddings and Force Atlas 2. Here docs/examples/viz_helios_mde.ipynb you can get a jupyter notebook that I\u0026rsquo;ve a created showing how to use MDE with IPC in Helios.\nIn the animation below we can see the result of the Helios-MDE application into a network with a set of anchored nodes.\nNext steps I\u0026rsquo;ll probably focus on the Helios network visualization system. Improving the documentation and testing the ForceAtlas2 in a computer with cuda installed. See the list of opened issues\nSummary of most important pull-requests:  IPC tools for network layout methods (helios issue #7) fury-gl/helios/pull/6 New network layout methods for fury (helios issue #7) fury-gl/helios/pull/9 fury-gl/helios/pull/14 fury-gl/helios/pull/13 Improved the visual aspects and configurations of the network rendering(helios issue #12) https://github.com/devmessias/helios/tree/fury_network_actors_improvements Tests, examples and documentation for Helios (helios issues #3 and #4) fury-gl/helios/pull/5 Reduced the flickering effect on the FURY/Helios streaming system fury-gl/helios/pull/10 fury-gl/fury/pull/437/commits/a94e22dbc2854ec87b8c934f6cabdf48931dc279  ","date":1626048e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1626048e3,"objectID":"1612bed18870bb064495b41fba00f51a","permalink":"/post/2021-07-12-gsoc-devmessias-6/2021-07-12-gsoc-devmessias-6/","publishdate":"2021-07-12T00:00:00Z","relpermalink":"/post/2021-07-12-gsoc-devmessias-6/2021-07-12-gsoc-devmessias-6/","section":"post","summary":"Hi all. In the past weeks, I\u0026rsquo;ve been focusing on developing Helios; the network visualization library for FURY. I improved the visual aspects of the network rendering as well as implemented the most relevant network layout methods.","tags":["gsoc","python","open-source","data-viz","webrtc"],"title":"GSoC-  Network layout algorithms using IPC","type":"post"},{"authors":null,"categories":null,"content":"What did you do this week?   PR fury-gl/fury#422 (merged): Integrated the 3d impostor spheres with the marker actor.  PR fury-gl/fury#422 (merged): Fixed some issues with my maker PR which now it's merged on fury.  PR fury-gl/fury#432 I've made some improvements in my PR which can be used to fine tune the opengl state on VTK.  PR fury-gl/fury#437 I've made several improvements in my streamer proposal for FURY related to memory management.  PR fury-gl/helios#1 First version of async network layout using force-directed.  Did I get stuck anywhere? A python-core issue I've spent some hours trying to discover this issue. But now it's solved through the commit devmessias/fury/commit/071dab85\nThe SharedMemory from python\u0026gt;=3.8 offers a new a way to share memory resources between unrelated process. One of the advantages of using the SharedMemory instead of the RawArray from multiprocessing is that the SharedMemory allows to share memory blocks without those processes be related with a fork or spawm method. The SharedMemory behavior allowed to achieve our jupyter integration and simplifies the use of the streaming system. However, I saw a issue in the shared memory implementation.\nLet\u0026rsquo;s see the following scenario:\n1-Process A creates a shared memory X 2-Process A creates a subprocess B using popen (shell=False) 3-Process B reads X 4-Process B closes X 5-Process A kills B 4-Process A closes X 5-Process A unlink() the shared memory resource X  The above scenario should work flawless. Calling unlink() in X is the right way as discussed in the python official documentation. However, there is a open issue related the unlink method\n  Issue: https://bugs.python.org/issue38119  PR python/cpython/pull/21516  Fortunately, I could use a monkey-patching solution to fix that meanwhile we wait to the python-core team to fix the resource_tracker (38119) issue.\nWhat is coming up next? I'm planning to work in the fury-gl/fury#432 and fury-gl/helios#1.\n","date":1624233600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624233600,"objectID":"39af57ec20dcc44b81fcda856e408a71","permalink":"/post/2021-06-21-gsoc-devmessias-3/2021-06-21-gsoc-devmessias-3/","publishdate":"2021-06-21T00:00:00Z","relpermalink":"/post/2021-06-21-gsoc-devmessias-3/2021-06-21-gsoc-devmessias-3/","section":"post","summary":"What did you do this week?   PR fury-gl/fury#422 (merged): Integrated the 3d impostor spheres with the marker actor.  PR fury-gl/fury#422 (merged): Fixed some issues with my maker PR which now it's merged on fury.","tags":["gsoc","python","open-source","data-viz","webrtc"],"title":"GSoC-  Bugs!","type":"post"},{"authors":null,"categories":null,"content":"Hi all! In this post I'll talk about the PR #437.\nThere are several reasons to have a streaming system for data visualization. Because I\u0026rsquo;m doing a PhD in a developing country I always need to think of the cheapest way to use the computational resources available. For example, with the GPUs prices increasing, it\u0026rsquo;s necessary to share a machine with a GPU with different users in different locations. Therefore, to convince my Brazilian friends to use FURY I need to code thinking inside of the (a) low-budget scenario.\nTo construct the streaming system for my project I\u0026rsquo;m thinking about the following properties and behaviors:\n I want to avoid blocking the code execution in the main thread (where the vtk/fury instance resides). The streaming should work inside of a low bandwidth environment. I need an easy way to share the rendering result. For example, using the free version of ngrok.  To achieve the property 1. we need to circumvent the GIL problem. Using the threading module alone it\u0026rsquo;s not good enough because we can\u0026rsquo;t use the python-threading for parallel CPU computation. In addition, to achieve a better organization it\u0026rsquo;s better to define the server system as an uncoupled module. Therefore, I believe that multiprocessing-lib in python will fit very well for our proposes.\nFor the streaming system to work smoothly in a low-bandwidth scenario we need to choose the protocol wisely. In the recent years the WebRTC protocol has been used in a myriad of applications like google hangouts and Google Stadia aiming low latency behavior. Therefore, I choose the webrtc as my first protocol to be available in the streaming system proposal.\nTo achieve the third property, we must be economical in adding requirements and dependencies.\nCurrently, the system has some issues, but it's already working. You can see some tutorials about how to use this streaming system here. After running one of these examples you can easily share the results and interact with other users. For example, using the ngrok For example, using the ngrok\n./ngrok http 8000  |\nHow does it works? The image below it's a simple representation of the streaming system.\nAs you can see, the streaming system is made up of different processes that share some memory blocks with each other. One of the hardest part of this PR was to code this sharing between different objects like VTK, numpy and the webserver. I'll discuss next some of technical issues that I had to learn/circumvent.\nSharing data between process We want to avoid any kind of unnecessary duplication of data or expensive copy/write actions. We can achieve this economy of computational resources using the multiprocessing module from python.\nmultiprocessing RawArray | The RawArray from multiprocessing allows to share resources between different processes. However, there are some tricks to get a better performance when we are dealing with RawArray's. For example, take a look at my PR in a older stage. In this older stage my streaming system was working well. However, one of my mentors (Filipi Nascimento) saw a huge latency for high-resolutions examples. My first thought was that latency was caused by the GPU-CPU copy from the opengl context. However, I discovered that I've been using RawArray's wrong in my entire life! | See for example this line of code fury/stream/client.py#L101 The code below shows how I've been updating the raw arrays\nraw_arr_buffer[:] = new_data  This works fine for small and medium sized arrays, but for large ones it takes a large amount of time, more than GPU-CPU copy. The explanation for this bad performance is available here : Demystifying sharedctypes performance. The solution which gives a stupendous performance improvement is quite simple. RawArrays implements the buffer protocol. Therefore, we just need to use the memoryview:\nmemview(arr_buffer)[:] = new_data  The memview is really good, but there it's a litle issue when we are dealing with uint8 RawArrays. The following code will cause an exception:\nmemview(arr_buffer_uint8)[:] = new_data_uint8  There is a solution for uint8 rawarrays using just memview and cast methods. However, numpy comes to rescue and offers a simple and a generic solution. You just need to convert the rawarray to a np representation in the following way:\narr_uint8_repr = np.ctypeslib.as_array(arr_buffer_uint8) arr_uint8_repr[:] = new_data_uint8  You can navigate to my repository in this specific commit position and test the streaming examples to see how this little modification improves the performance.\nMultiprocessing inside of different Operating Systems Serge Koudoro, who is one of my mentors, has pointed out an issue of the streaming system running in MacOs. I don't know many things about MacOs, and as pointed out by Filipi the way that MacOs deals with multiprocessing is very different than the Linux approach. Although we solved the issue discovered by Serge, I need to be more careful to assume that different operating systems will behave in the same way. If you want to know more,I recommend that you read this post Python: Forking vs Spawm. And it's also important to read the official documentation from python. It can save you a lot of time. Take a look what the official python documentation says about the multiprocessing method\nSource: https://docs.python.org/3/library/multiprocessing.html\n","date":1623456e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1623456e3,"objectID":"34e0ced7953795e17dea358df37598ce","permalink":"/post/2021-06-12-gsoc-devmessias-2/2021-06-12-gsoc-devmessias-2/","publishdate":"2021-06-12T00:00:00Z","relpermalink":"/post/2021-06-12-gsoc-devmessias-2/2021-06-12-gsoc-devmessias-2/","section":"post","summary":"Hi all! In this post I'll talk about the PR #437.\nThere are several reasons to have a streaming system for data visualization. Because I\u0026rsquo;m doing a PhD in a developing country I always need to think of the cheapest way to use the computational resources available.","tags":["gsoc","python","open-source","data-viz","webrtc"],"title":"GSoC-  A Stadia-like system for data visualization","type":"post"},{"authors":["Bruno Messias {F. de Resende}","Luciano da {F. Costa}"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"2a78667fb4b30bcef614d8558a8c641a","permalink":"/publication/2020-01-01_chaos2019/","publishdate":"2020-01-01T00:00:00Z","relpermalink":"/publication/2020-01-01_chaos2019/","section":"publication","summary":"","tags":[],"title":"Characterization and comparison of large directed networks through the spectra of the magnetic Laplacian","type":"publication"},{"authors":["Thomas Peron","Bruno {Messias F. de Resende}","Angelica S. Mata","Francisco A. Rodrigues","Yamir Moreno"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"7036b5c54430e8c0c8e9272a27a55716","permalink":"/publication/2019-01-01_pre_2019/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/2019-01-01_pre_2019/","section":"publication","summary":"","tags":[],"title":"Onset of synchronization of Kuramoto oscillators in scale-free networks","type":"publication"},{"authors":null,"categories":null,"content":"Add your privacy policy here and set draft: false to publish it. Otherwise, delete this file if you don\u0026rsquo;t need it.\n","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"18d05a63a1c8d7ed973cc51838494e41","permalink":"/privacy/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/privacy/","section":"","summary":"Add your privacy policy here and set draft: false to publish it. Otherwise, delete this file if you don\u0026rsquo;t need it.","tags":null,"title":"Privacy Policy","type":"page"},{"authors":null,"categories":null,"content":"Add your terms here and set draft: false to publish it. Otherwise, delete this file if you don\u0026rsquo;t need it.\n","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"9b10c1f64082d3869fd4cb1f85809430","permalink":"/terms/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/terms/","section":"","summary":"Add your terms here and set draft: false to publish it. Otherwise, delete this file if you don\u0026rsquo;t need it.","tags":null,"title":"Terms","type":"page"},{"authors":null,"categories":null,"content":"Bibcure helps in boring tasks by keeping your bibfile up to date and normalized...also allows you to easily download all papers inside your bibtex  Requirements/Install $ sudo python /usr/bin/pip install bibcure  scihub2pdf(beta) If you want download articles via a DOI number, article title or a bibtex file, using the database of arxiv, libgen or sci-hub, see\n bibcure/scihub2pdf\nFeatures and how to use bibcure Given a bib file\u0026hellip;\n$ bibcure -i input.bib -o output.bib    check sure the Arxiv items have been published, then update them(requires internet connection)\n  complete all fields(url, journal, etc) of all bib items using DOI number(requires internet connection)\n  find and create DOI number associated with each bib item which has not DOI field(requires internet connection)\n  abbreviate jorunals names\n  arxivcheck Given a arxiv id\u0026hellip;\n$ arxivcheck 1601.02785   check if has been published, and then returns the updated bib (requires internet connection)  Given a title\u0026hellip;\n$ arxivcheck --title An useful paper published on arxiv  search papers related and return a bib the first item. You can easily append a bib into a bibfile, just do\n$ arxivcheck --title An useful paper published on arxiv \u0026gt;\u0026gt; file.bib  You also can interact with results, just pass \u0026ndash;ask parameter\n$ arxivcheck --ask --title An useful paper published on arxiv  scihub2pdf Given a bibtex file\n$ scihub2pdf -i input.bib  Given a DOI number\u0026hellip;\n$ scihub2pdf 10.1038/s41524-017-0032-0  Given arxivId\u0026hellip;\n$ scihub2pdf arxiv:1708.06891  Given a title\u0026hellip;\n$ scihub2bib --title An useful paper  or arxiv\u0026hellip;\n$ scihub2bib --title arxiv:An useful paper  Location folder as argument\n$ scihub2pdf -i input.bib -l somefoler/  Use libgen instead sci-hub\n$ scihub2pdf -i input.bib --uselibgen  doi2bib Given a DOI number\u0026hellip;\n$ doi2bib 10.1038/s41524-017-0032-0   get bib item given a doi(requires internet connection)  You can easily append a bib into a bibfile, just do\n$ doi2bib 10.1038/s41524-017-0032-0 \u0026gt;\u0026gt; file.bib  You also can generate a bibtex from a txt file containing a list of DOIs\n$ doi2bib --input file_with_dois.txt --output refs.bib  title2bib Given a title\u0026hellip;\n$ title2bib An useful paper   search papers related and return a bib for the selected paper(requires internet connection)  You can easily append a bib into a bibfile, just do\n$ title2bib An useful paper --first \u0026gt;\u0026gt; file.bib  You also can generate a bibtex from a txt file containing a list of \u0026ldquo;titles\u0026rdquo;\n$ title2bib --input file_with_titles.txt --output refs.bib --first  Sci-Hub vs LibGen Sci-hub:  Stable Annoying CAPTCHA Fast  Libgen  Unstalbe No CAPTCHA Slow  ","date":1505324602,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1505324602,"objectID":"189b15baed803715b34bef2950255a7a","permalink":"/project/bibcure/","publishdate":"2017-09-13T17:43:22Z","relpermalink":"/project/bibcure/","section":"project","summary":"Bibcure helps in boring tasks by keeping your bibfile up to date and normalized...also allows you to easily download all papers inside your bibtex","tags":["latex","bibtex","scihub","python"],"title":"Bibcure","type":"project"},{"authors":["B. {Messias de Resende}","F. Crasto {de Lima}","R. H. Miwa","E. Vernek","G. J. Ferreira"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"11fe085b7e739b5e1488c84289d180f8","permalink":"/publication/2017-01-01_prb_2017/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/2017-01-01_prb_2017/","section":"publication","summary":"","tags":[],"title":"Confinement and fermion doubling problem in Dirac-like Hamiltonians","type":"publication"},{"authors":[],"categories":[],"content":"","date":1471271380,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1471271380,"objectID":"c23089a112a0e7f75db39152c4696d08","permalink":"/project/python_triangulo/","publishdate":"2016-08-15T11:29:40-03:00","relpermalink":"/project/python_triangulo/","section":"project","summary":"A comunidade python triângulo foi criada em 2016 com o intuito de promover a divulgação da linguagem python e o uso de software-livre em Uberlândia-MG e região","tags":["python","code","software-livre","free-software"],"title":"Python Triângulo","type":"project"},{"authors":[],"categories":[],"content":" O novo sistema de periódicos da Capes (assim como o CAFe) reduz drasticamente a agilidade de busca e acesso a referências bibliográficas, pois é necessário que os usuários acessem o portal de buscas da Capes. O que acaba inutilizando o acesso a publicações via links diretos, links internos dos arquivos PDF e sistemas como o DOI.\nPor isso o Prof. Gerson Ferreira e esse que vos fala desenvolveram uma ferramenta que facilita a vida de nos, já aterefados cientistas. A extensão está disponível tanto para firefox quanto para chrome\n Link para Chrome\n Link para Firefox\nEstamos recebendo um excelente feedback da comunidade ciêntifica, com um total de mais de 5 mil usuários(firefox \u0026amp; chrome)\n Como não fazemos a parte do time de TI da CAPES não possuímos a lista oficial, e ainda não obtivemos resposta do time de TI da CAPES\n    ","date":1468851944,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1468851944,"objectID":"8b5a00fd72e1af6445eac589c656c225","permalink":"/project/capes/","publishdate":"2016-07-18T14:25:44Z","relpermalink":"/project/capes/","section":"project","summary":"Extensão que permite ganhar agilidade ao utilizar o novo sistema de periódicos CAPES","tags":["open-science","free-software"],"title":"Redir Capes - thousands of scientists using in Brazil","type":"project"},{"authors":[],"categories":["inclusão digital","social"],"content":"O Recicla Aqui é um projeto que desenvolvo em parceria com um amigo(Régis Maicon). Tal projeto visa inserir tecnologia no processo da coleta seletiva e aumentar a qualidade de vida dos catadores de material reciclável.\nTemos três eixos norteadores:\nFornecer tecnologias de Gerenciamento e Formação técnica para as Cooperativas Criação de um aplicativo que permita conectar Cidadães e Coletores \u0026ldquo;Revitalização\u0026rdquo; da imagem dos coletores \u0026ldquo;Alfabetização Tecnológica\u0026rdquo; O aplicativo você encontra na Goolge Play (futuramente na Apple Store)\nObjetivamos utilizar as redes sociais para conscientizar a população da importância dos catadores\n","date":1464691556,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464691556,"objectID":"22c723ecb71e3750fef4aa2cf80ff54f","permalink":"/project/reciclaaqui/","publishdate":"2016-05-31T10:45:56Z","relpermalink":"/project/reciclaaqui/","section":"project","summary":"O Recicla Aqui é um projeto que foi desenvolvido em parceria com um amigo(Régis Maicon) e o CIAEM-UFU para alfabetização tecnológica e aprimoramento de processos das cooperativas em Uberlândia-MG","tags":["python","django","social","inclusão digital","react"],"title":"ReciclaAqui","type":"project"},{"authors":["B. M. F. {de Resende}","E. Vernek"],"categories":null,"content":"","date":1325376e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1325376e3,"objectID":"cc237e3dfc5e1d2773926481e84f6de4","permalink":"/publication/2012-01-01_apl_2012/","publishdate":"2012-01-01T00:00:00Z","relpermalink":"/publication/2012-01-01_apl_2012/","section":"publication","summary":"","tags":[],"title":"Parity oscillations of Kondo temperature in a single molecule break junction","type":"publication"},{"authors":null,"categories":null,"content":"eMaTe it is a python package which the main goal is to provide methods capable of estimating the spectral densities and trace functions of large sparse matrices. eMaTe can run in both CPU and GPU and can estimate the spectral density and related trace functions, such as entropy and Estrada index, even in directed or undirected networks with million of nodes.\nInstall pip install emate  If you a have a GPU you should also install cupy.\nKernel Polynomial Method (KPM) The Kernel Polynomial Method can estimate the spectral density of large sparse Hermitan matrices with a computational cost almost linear. This method combines three key ingredients: the Chebyshev expansion + the stochastic trace estimator + kernel smoothing.\nExample import networkx as nx import numpy as np n = 3000 g = nx.erdos_renyi_graph(n , 3/n) W = nx.adjacency_matrix(g) vals = np.linalg.eigvals(W.todense()).real  from emate.hermitian import tfkpm num_moments = 40 num_vecs = 40 extra_points = 10 ek, rho = tfkpm(W, num_moments, num_vecs, extra_points)  import matplotlib.pyplot as plt plt.hist(vals, density=True, bins=100, alpha=.9, color=\u0026quot;steelblue\u0026quot;) plt.scatter(ek, rho, c=\u0026quot;tomato\u0026quot;, zorder=999, alpha=0.9, marker=\u0026quot;d\u0026quot;)  If the CUPY package it is available in your machine, you can also use the cupy implementation. When compared to tf-kpm, the Cupy-kpm is slower for median matrices (100k) and faster for larger matrices (\u0026gt; 10^6). The main reason it\u0026rsquo;s because the tf-kpm was implemented in order to calc all te moments in a single step.\nimport matplotlib.pyplot as plt from emate.hermitian import cupykpm num_moments = 40 num_vecs = 40 extra_points = 10 ek, rho = cupykpm(W.tocsr(), num_moments, num_vecs, extra_points) plt.hist(vals, density=True, bins=100, alpha=.9, color=\u0026quot;steelblue\u0026quot;) plt.scatter(ek.get(), rho.get(), c=\u0026quot;tomato\u0026quot;, zorder=999, alpha=0.9, marker=\u0026quot;d\u0026quot;)  Stochastic Lanczos Quadrature (SLQ)  The problem of estimating the trace of matrix functions appears in applications ranging from machine learning and scientific computing, to computational biology.[2]\n Example Computing the Estrada index from emate.symmetric.slq import pyslq import tensorflow as tf def trace_function(eig_vals): return tf.exp(eig_vals) num_vecs = 100 num_steps = 50 approximated_estrada_index, _ = pyslq(L_sparse, num_vecs, num_steps, trace_function) exact_estrada_index = np.sum(np.exp(vals_laplacian)) approximated_estrada_index, exact_estrada_index  The above code returns\n(3058.012, 3063.16457163222)  Entropy import scipy import scipy.sparse def entropy(eig_vals): s = 0. for val in eig_vals: if val \u0026gt; 0: s += -val*np.log(val) return s L = np.array(G.laplacian(normalized=True), dtype=np.float64) vals_laplacian = np.linalg.eigvalsh(L).real exact_entropy = entropy(vals_laplacian) def trace_function(eig_vals): def entropy(val): return tf.cond(val\u0026gt;0, lambda:-val*tf.log(val), lambda: 0.) return tf.map_fn(entropy, eig_vals) L_sparse = scipy.sparse.coo_matrix(L) num_vecs = 100 num_steps = 50 approximated_entropy, _ = pyslq(L_sparse, num_vecs, num_steps, trace_function) approximated_entropy, exact_entropy  (-509.46283, -512.5283224633046)   [1] Hutchinson, M. F. (1990). A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines. Communications in Statistics-Simulation and Computation, 19(2), 433-450.\n [2] Ubaru, S., Chen, J., \u0026amp; Saad, Y. (2017). Fast Estimation of tr(f(A)) via Stochastic Lanczos Quadrature. SIAM Journal on Matrix Analysis and Applications, 38(4), 1075-1099.\n [3] The Kernel Polynomial Method applied to tight binding systems with time-dependence\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c6edb1da40d4c9307d11a800c073848d","permalink":"/project/emate/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/emate/","section":"project","summary":"eMaTe is a python package which can estimate spectral propreties of very large matrices and networks","tags":null,"title":"eMaTe","type":"project"}]