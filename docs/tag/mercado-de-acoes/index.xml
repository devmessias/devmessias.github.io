<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>mercado-de-ações | Bruno Messias</title><link>/tag/mercado-de-acoes/</link><atom:link href="/tag/mercado-de-acoes/index.xml" rel="self" type="application/rss+xml"/><description>mercado-de-ações</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en</language><copyright>Bruno Messias</copyright><lastBuildDate>Mon, 18 Apr 2022 00:00:00 +0000</lastBuildDate><image><url>/images/icon_hucd6a3d413e7b81060a1d462b35f64cf9_5018_512x512_fill_lanczos_center_3.png</url><title>mercado-de-ações</title><link>/tag/mercado-de-acoes/</link></image><item><title>Correlation matrix analysis with the nested stochastic block model:the graph structure of stock market.</title><link>/post/nsbm_sp500_stock_market_disparity_filter/</link><pubDate>Mon, 18 Apr 2022 00:00:00 +0000</pubDate><guid>/post/nsbm_sp500_stock_market_disparity_filter/</guid><description>&lt;details
class="toc-inpage d-print-none d-none d-sm-block d-md-none " open>
&lt;summary class="font-weight-bold">Table of Contents&lt;/summary>
&lt;nav id="TableOfContents">
&lt;ul>
&lt;li>
&lt;ul>
&lt;li>&lt;a href="#introduction">Introduction&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#graphs">Graphs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#correlation-matrices">Correlation matrices&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#downloading-and-preprocessing-the-data">Downloading and preprocessing the data&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#closing-prices-of-the-sp-500">Closing prices of the S&amp;amp;P 500&lt;/a>&lt;/li>
&lt;li>&lt;a href="#returns-and-correlation-matrix">Returns and correlation matrix&lt;/a>&lt;/li>
&lt;li>&lt;a href="#removing-irrelevant-correlations-with-the-edgeseraser-library">Removing irrelevant correlations with the edgeseraser library&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#nsbm-fiding-the-hierarquical-structure">nSBM: fiding the hierarquical structure&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#converting-from-igraph-to-graph-tool">Converting from igraph to graph-tool&lt;/a>&lt;/li>
&lt;li>&lt;a href="#how-to-use-the-graph-tool-nsbm">How to use the graph-tool nSBM?&lt;/a>&lt;/li>
&lt;li>&lt;a href="#how-to-analyze">How to analyze?&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#conclusion">Conclusion&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/nav>
&lt;/details>
&lt;p>In this post, I’ll show you how the nested stochastic block model (nSBM) can extract insights from correlation matrices using a combination with a filtering edge graph technique called disparity filter. I’ll use as a case study a correlation matrix derived from the stock market data.&lt;/p>
&lt;p>Don’t be afraid, you will reproduce the beautiful plot presented below and understand what it means.&lt;/p>
&lt;figure >
&lt;a data-fancybox="" href="/post/nsbm_sp500_stock_market_disparity_filter/nsbm_final_2018-01-01_2018-06-01_hudcff6362c16284ef400b42c5fe2e1b69_266080_0x500_resize_lanczos_3.png" >
&lt;img data-src="/post/nsbm_sp500_stock_market_disparity_filter/nsbm_final_2018-01-01_2018-06-01_hudcff6362c16284ef400b42c5fe2e1b69_266080_0x500_resize_lanczos_3.png" class="lazyload" alt="" width="100%" height="500px">
&lt;/a>
&lt;/figure>
&lt;p>If you want to reproduce, use a conda environment because of the graph-tool dependency. Besides that, check if you have the following packages installed:&lt;/p>
&lt;pre>&lt;code>
matplotlib, pandas, yfinance
&lt;/code>&lt;/pre>
&lt;p>Probably you will need to install the &lt;code>igraph&lt;/code> package&lt;/p>
&lt;pre>&lt;code>
$ pip install python-igraph
&lt;/code>&lt;/pre>
&lt;p>Now, install the &lt;code>graph-tool&lt;/code> made by
&lt;a href="https://twitter.com/tiagopeixoto" target="_blank" rel="noopener">Tiago Peixoto&lt;/a>.&lt;/p>
&lt;pre>&lt;code>
$ conda install -c conda-forge graph-tool
&lt;/code>&lt;/pre>
&lt;p>No less important, we will need to remove weakling edges from the graph. To do so, we will use my package, &lt;code>edgeseraser&lt;/code>. The repository of the project is available here:
&lt;a href="https://github.com/devmessias/edgeseraser" target="_blank" rel="noopener">devmessias/edgeseraser&lt;/a>. To install the &lt;code>edgeseraser&lt;/code> run the following command:&lt;/p>
&lt;pre>&lt;code>
$ pip install edgeseraser
&lt;/code>&lt;/pre>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Exploratory data analysis allows us to gain insights or improve our choices of the next steps in the data science process. In some cases, we can explore the data assuming that all the entities are independent of each other. What happens when it’s not the case?&lt;/p>
&lt;h3 id="graphs">Graphs&lt;/h3>
&lt;p>A graph is a data structure used to store objects (vertexes) with binary relationships. Those relationships are called edges.&lt;/p>
&lt;p>Each edge can have generic data associated with it. Like a numeric value.&lt;/p>
&lt;p>Maybe you can think that this data structure is rare in the world of data. But&lt;/p>
&lt;p>if you look carefully at the data spread on the internet, graphs are everywhere: social networks, economic transactions, correlation matrices and many more.&lt;/p>
&lt;h3 id="correlation-matrices">Correlation matrices&lt;/h3>
&lt;p>A correlation matrix is a storing mechanism that quantifies relationships between pairs of random variables.&lt;/p>
&lt;p>Thus, a correlation matrix is at least a weighted graph. The main point of this post is to show how we can use tools of network science to analyze the correlation matrix. Ok, but maybe you’re asking: Why do we need to do this?&lt;/p>
&lt;p>Correlation matrices are commonly used to discover and segment the universe of possible random variables. With this segmentation into groups (clustering) we can analyze the dataset in a granular way. Therefore, improving our efficiency in discarding unnecessary information.&lt;/p>
&lt;p>A widely spread way to analyze correlation matrices using graph analytics is through the use of minimum spanning trees. I won’t go into details about this method (check the following posts
&lt;a href="https://hudsonthames.org/networks-with-mlfinlab-minimum-spanning-tree-mst/#:~:text=A%20Minimum%20Spanning%20Tree%20%28MST%29%20is%20a%20graph%20consisting%20of,a%20stock%20crisis%20%5B2%5D." target="_blank" rel="noopener">Networks with MlFinLab: Minimum Spanning &lt;/a> and [Dynamic spanning trees in stock market networks:&lt;/p>
&lt;p>The case of Asia-Pacific](
&lt;a href="https://www.bcb.gov.br/pec/wps/ingl/wps351.pdf" target="_blank" rel="noopener">https://www.bcb.gov.br/pec/wps/ingl/wps351.pdf&lt;/a>)) because I want to show how we can use a different technique that at the same time is more flexible and powerful when we want to understand the hidden group structure of a correlation matrix.&lt;/p>
&lt;p>This technique is called the nested stochastic block model (nSBM) and it’s a probabilistic model that allows us to infer the hidden group structure of a graph. The nSBM is a generalization of the stochastic block model (SBM) that assumes that the graph is composed of a set of communities (groups) that are connected by a set of edges. The SBM is a generative model that allows us to generate graphs with several communities. This generative model also creates those communities using parameters that describe the probability of a connection between vertices living in the same or different communities. Although the SBM is a powerful tool to analyze the structure of a graph, it’s not enough to infer the small communities (small groups of stocks) in our graph. To do so, we need to use the nSBM.&lt;/p>
&lt;p>Ok, let’s go to the actual stuff and code.&lt;/p>
&lt;h2 id="downloading-and-preprocessing-the-data">Downloading and preprocessing the data&lt;/h2>
&lt;h3 id="closing-prices-of-the-sp-500">Closing prices of the S&amp;amp;P 500&lt;/h3>
&lt;p>Let’s import what we need&lt;/p>
&lt;pre>&lt;code class="language-python">
import yfinance as yf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import igraph as ig
from edgeseraser.disparity import filter_ig_graph
&lt;/code>&lt;/pre>
&lt;p>To keep things simple, I choose to use the following data available
&lt;a href="https://raw.githubusercontent.com/datasets/s-and-p-500-companies/master/data/constituents.csv" target="_blank" rel="noopener">here&lt;/a>. This data encompasses the S&amp;amp;P500 from a specific time with symbols and sectors like this:&lt;/p>
&lt;p>| Symbol | Name | Sector |&lt;/p>
&lt;p>| &amp;mdash;&amp;mdash; | &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;- | &amp;mdash;&amp;mdash;&amp;mdash;&amp;ndash; |&lt;/p>
&lt;p>| MMM | 3M | Industrials |&lt;/p>
&lt;p>| AOS | A. O. Smith | Industrials |&lt;/p>
&lt;p>| ABT | Abbott Laboratories | Health Care |&lt;/p>
&lt;p>| ABBV | AbbVie | Health Care |&lt;/p>
&lt;pre>&lt;code class="language-python">
!wget https://raw.githubusercontent.com/datasets/s-and-p-500-companies/master/data/constituents.csv
&lt;/code>&lt;/pre>
&lt;p>After downloading the data we just need to load it into a pandas dataframe.&lt;/p>
&lt;pre>&lt;code class="language-python">
df = pd.read_csv(“constituents.csv”)
all_symbols = df[‘Symbol’].values
all_sectors = df[‘Sector’].values
all_names = df[‘Name’].values
&lt;/code>&lt;/pre>
&lt;p>The following dictionaries will map the symbols to sectors and names later on&lt;/p>
&lt;pre>&lt;code>
symbol2sector = dict(zip(all_symbols, all_sectors))
symbol2name = dict(zip(all_symbols, all_names))
&lt;/code>&lt;/pre>
&lt;p>Time to download the information about each symbol. Let’s request one semester&lt;/p>
&lt;p>of data from yahoo finance API&lt;/p>
&lt;pre>&lt;code class="language-python">
start_date = ‘2018-01-01’
end_date = ‘2018-06-01’
try:
prices = pd.read_csv(
f&amp;quot;sp500_prices_{start_date}_{end_date}.csv&amp;quot;, index_col=&amp;quot;Date&amp;quot;)
tickers_available = prices.columns.values
except FileNotFoundError:
df = yf.download(
list(all_symbols),
start=start_date,
end=end_date,
interval=&amp;quot;1d&amp;quot;,
group_by=’ticker’,
progress=True
)
tickers_available = list(
set([ticket for ticket, _ in df.columns.T.to_numpy()]))
prices = pd.DataFrame.from_dict(
{
ticker: df[ticker][“Adj Close”].to_numpy()
for ticker in tickers_available
}
)
prices.index = df.index
prices = prices.iloc[:-1]
del df
prices.to_csv(
f&amp;quot;sp500_prices_{start_date}_{end_date}.csv&amp;quot;)
&lt;/code>&lt;/pre>
&lt;h3 id="returns-and-correlation-matrix">Returns and correlation matrix&lt;/h3>
&lt;p>We first start by computing the percentage of price changes for each stock, the return of each stock.&lt;/p>
&lt;pre>&lt;code class="language-python">
returns_all = prices.pct_change()
# a primeira linha não faz sentido, não existe retorno no primeiro dia
returns_all = returns_all.iloc[1:, :]
returns_all.dropna(axis=1, thresh=len(returns_all.index)//2., inplace=True)
returns_all.dropna(axis=0, inplace=True)
symbols = returns_all.columns.values
&lt;/code>&lt;/pre>
&lt;p>Now, we compute the correlation matrix using the returns&lt;/p>
&lt;pre>&lt;code class="language-python">
# plot the correlation matrix with ticks at each item
correlation_matrix = returns_all.corr()
plt.title(f&amp;quot;Correlation matrix from {start_date} to {end_date}&amp;quot;)
plt.imshow(correlation_matrix)
plt.colorbar()
plt.savefig(“correlation.png”, dpi=150)
plt.clf()
&lt;/code>&lt;/pre>
&lt;figure id="figure-correlation-matrix-for-sp500-off-the-first-2018-semester-yes-its-a-mess">
&lt;a data-fancybox="" href="/post/nsbm_sp500_stock_market_disparity_filter/correlation_hu6c68fc9d1aa2b5f7307017b57ffa4d90_521823_0x500_resize_lanczos_3.png" data-caption="Correlation matrix for S&amp;amp;amp;P500 off the first 2018 semester. Yes, it’s a mess!">
&lt;img data-src="/post/nsbm_sp500_stock_market_disparity_filter/correlation_hu6c68fc9d1aa2b5f7307017b57ffa4d90_521823_0x500_resize_lanczos_3.png" class="lazyload" alt="" width="100%" height="500px">
&lt;/a>
&lt;figcaption>
Correlation matrix for S&amp;amp;P500 off the first 2018 semester. Yes, it’s a mess!
&lt;/figcaption>
&lt;/figure>
&lt;p>Ok, look into the image above. If you try to find groups of stocks that are less correlated just using visual inspection, you will have a hard time. This works just for a small set of stocks. Obviously, that is another approach like using linked cluster algorithms, but they are still hard to analyze when the number of stocks increases.&lt;/p>
&lt;h3 id="removing-irrelevant-correlations-with-the-edgeseraser-library">Removing irrelevant correlations with the edgeseraser library&lt;/h3>
&lt;p>Our goal here is to understand the structure of the stock market communities,&lt;/p>
&lt;p>so we will keep just the matrix elements with positive values.&lt;/p>
&lt;pre>&lt;code class="language-python">
pos_correlation = correlation_matrix.copy()
# vamos considerar apenas as correlações positivas pois queremos
# apenas as comunidade&amp;gt;
pos_correlation[pos_correlation &amp;lt; 0.] = 0
# diagonal principal é setada a 0 para evitar auto-arestas
np.fill_diagonal(pos_correlation.values, 0)
&lt;/code>&lt;/pre>
&lt;p>Now, we create a weighted graph where each vertex is a stock and each positive correlation is a weight associated to the edge between two stocks.&lt;/p>
&lt;pre>&lt;code class="language-python">
g = ig.Graph.Weighted_Adjacency(pos_correlation.values, mode=’undirected’)
# criamos uma feature symbol para cada vértice
g.vs[“symbol”] = returns_all.columns
# o grafo pode estar desconectado. Portanto, extraímos a componente gigante
cl = g.clusters()
g = cl.giant()
n_edges_before = g.ecount()
&lt;/code>&lt;/pre>
&lt;p>The graph above has too much information, which is problematic for graph analysis. So, we must first identify and keep only the relevant edges. To do so, we use the disparity filter.&lt;/p>
&lt;p>available in my &lt;code>edgeseraser&lt;/code> package.&lt;/p>
&lt;pre>&lt;code class="language-python">
_ = filter_ig_graph(g, .25, cond=&amp;quot;both&amp;quot;, field=&amp;quot;weight&amp;quot;)
cl = g.clusters()
g = cl.giant()
n_edges_after = g.ecount()
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">
print(f&amp;quot;Percentage of edges removed: {(n_edges_before - n_edges_after)/n_edges_before*100:.2f}%&amp;quot;)
print(f&amp;quot;Number of remained stocks: {len(symbols)}&amp;quot;)
...
Percentage of edges removed: 95.76%
Number of remained stocks: 492
&lt;/code>&lt;/pre>
&lt;p>We have deleted most of the edges and only a backbone remained in our graph. This backbone structure can now be used to extract.&lt;/p>
&lt;p>insights from the stock market.&lt;/p>
&lt;h2 id="nsbm-fiding-the-hierarquical-structure">nSBM: fiding the hierarquical structure&lt;/h2>
&lt;h3 id="converting-from-igraph-to-graph-tool">Converting from igraph to graph-tool&lt;/h3>
&lt;p>Here, we will use the graph-tool lib to infer the hierarchical structure of the graph. Thus, we must first convert the graph from &lt;code>igraph&lt;/code> to &lt;code>graph-tool&lt;/code>. We can convert that using the following code&lt;/p>
&lt;pre>&lt;code class="language-python">
import graph_tool.all as gt
gnsbm = gt.Graph(directed=False)
# iremos adicionar os vértices
for v in g.vs:
gnsbm.add_vertex()
# e as arestas
for e in g.es:
gnsbm.add_edge(e.source, e.target)
&lt;/code>&lt;/pre>
&lt;h3 id="how-to-use-the-graph-tool-nsbm">How to use the graph-tool nSBM?&lt;/h3>
&lt;p>With the sparsified correlation graph that encompasses the stock relationships, we can ask ourselves how those relationships ties stocks together. Here, we will use the nSBM to answer this.&lt;/p>
&lt;p>The
&lt;a href="https://dx.doi.org/10.1103/PhysRevX.4.011047" target="_blank" rel="noopener">nSBM&lt;/a>&lt;/p>
&lt;p>is an inference method that optimizes a set of parameters that defines a generative graph model for our data. In our case, to find this model for the sparsified s&amp;amp;p500 correlation graph, we will cal the method &lt;code>minimize_nested_blockmodel_dl&lt;/code>&lt;/p>
&lt;pre>&lt;code class="language-python">
state = gt.minimize_nested_blockmodel_dl(gnsbm)
&lt;/code>&lt;/pre>
&lt;p>We can use the code below to define the colors for our plot.&lt;/p>
&lt;pre>&lt;code class="language-python">
symbols = g.vs[“symbol”]
sectors = [symbol2sector[symbol] for symbol in symbols]
u_sectors = np.sort(np.unique(sectors))
u_colors = [plt.cm.tab10(i/len(u_sectors))
for i in range(len(u_sectors))]
# a primeira cor da lista era muito similar a segunda,
u_colors[0] = [0, 1, 0, 1]
sector2color = {sector: color for sector, color in zip(u_sectors, u_colors)}
rgba = gnsbm.new_vertex_property(“vector&amp;lt;double&amp;gt;“)
gnsbm.vertex_properties[‘rgba’] = rgba
for i, symbol in enumerate(symbols):
c = sector2color[symbol2sector[symbol]]
rgba[i] = [c[0], c[1], c[2], .5]
&lt;/code>&lt;/pre>
&lt;p>Calling the &lt;code>draw&lt;/code> method, we create a plot with the nSBM result. You can play a little, changing the `$\beta \in (0, 1)$ parameter to see how the things change. This parameter controls the strength of the &lt;strong>edge-bundling&lt;/strong> algorithm.&lt;/p>
&lt;pre>&lt;code class="language-python">
options = {
‘output’: f’nsbm_{start_date}_{end_date}.png’,
‘beta’: .9,
‘bg_color’: ‘w’,
#’output_size’: (1500, 1500),
‘vertex_color’: gnsbm.vertex_properties[‘rgba’],
‘vertex_fill_color’: gnsbm.vertex_properties[‘rgba’],
‘hedge_pen_width’: 2,
‘hvertex_fill_color’: np.array([0., 0., 0., .5]),
‘hedge_color’: np.array([0., 0., 0., .5]),
‘hedge_marker_size’: 20,
‘hvertex_size’:20
}
state.draw(**options)
&lt;/code>&lt;/pre>
&lt;p>Finally, we can see the hierarchical structure got from our filtered graph using&lt;/p>
&lt;p>the nSBM algorithm.&lt;/p>
&lt;pre>&lt;code class="language-python">
plt.figure(dpi=150)
plt.title(f”Sectors of the S&amp;amp;P 500 from {start_date} to {end_date}&amp;quot;)
legend = plt.legend(
[plt.Line2D([0], [0], color=c, lw=10)
for c in list(sector2color.values())],
list(sector2color.keys()),
bbox_to_anchor=(1.05, 1),
loc=2,
borderaxespad=0.)
plt.imshow(plt.imread(f’nsbm_{start_date}_{end_date}.png’))
plt.xticks([])
plt.yticks([])
plt.axis(‘off’)
plt.savefig(f’nsbm_final_{start_date}_{end_date}.png’, bbox_inches=’tight’,
dpi=150, bbox_extra_artists=(legend,), facecolor=’w’, edgecolor=’w’)
plt.show()
&lt;/code>&lt;/pre>
&lt;figure id="figure-nsbm-result-for-the-correlation-matrix-of-sp500-returns">
&lt;a data-fancybox="" href="/post/nsbm_sp500_stock_market_disparity_filter/nsbm_final_2018-01-01_2018-06-01_hudcff6362c16284ef400b42c5fe2e1b69_266080_0x500_resize_lanczos_3.png" data-caption="nSBM result for the correlation matrix of S&amp;amp;amp;P500 returns.">
&lt;img data-src="/post/nsbm_sp500_stock_market_disparity_filter/nsbm_final_2018-01-01_2018-06-01_hudcff6362c16284ef400b42c5fe2e1b69_266080_0x500_resize_lanczos_3.png" class="lazyload" alt="" width="100%" height="500px">
&lt;/a>
&lt;figcaption>
nSBM result for the correlation matrix of S&amp;amp;P500 returns.
&lt;/figcaption>
&lt;/figure>
&lt;p>Wow, beautiful, isn’t it? But maybe you can only see a beautiful picture and not be&lt;/p>
&lt;p>able to understand what is happening. So, let’s try to understand what this plot is&lt;/p>
&lt;p>telling us.&lt;/p>
&lt;h3 id="how-to-analyze">How to analyze?&lt;/h3>
&lt;figure >
&lt;a data-fancybox="" href="/post/nsbm_sp500_stock_market_disparity_filter/descripition_nsbm_sp500_eng_hubc8bd3aa9dee5e4b9544c9c74b387fed_1482556_0x400_resize_lanczos_3.png" >
&lt;img data-src="/post/nsbm_sp500_stock_market_disparity_filter/descripition_nsbm_sp500_eng_hubc8bd3aa9dee5e4b9544c9c74b387fed_1482556_0x400_resize_lanczos_3.png" class="lazyload" alt="" width="80%" height="400px">
&lt;/a>
&lt;/figure>
&lt;ul>
&lt;li>
&lt;p>Each circle (leaf) represents a stock. A vertex in the original graph.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Each group of leafs which resembles a broom is a community detected by the nSBM.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Navigating backwards through the arrows, we can see the hierarchical community structure. The image above shows three communities that belong to the same grandfather community.&lt;/p>
&lt;p>We can see interesting stuff in the community structure. For example, most stocks related to &lt;strong>Consumer staples&lt;/strong>, &lt;strong>Real state&lt;/strong>, and &lt;strong>Utilities&lt;/strong> belongs to the same community.&lt;/p>
&lt;p>What is the meaning of the edges?&lt;/p>
&lt;ul>
&lt;li>A larger number of edges between &lt;strong>Financials&lt;/strong>, &lt;strong>Industrials&lt;/strong> and &lt;strong>Information technology&lt;/strong> survive the filtering technique, which shows a strong relationship between these sectors.&lt;/li>
&lt;/ul>
&lt;p>The visual inspection of the edges relationships is meaningful only because of the use of edge bundling algorithm. Let’s see how a weak bundling turns our job into a hard one, setting $\beta = 0.5$&lt;/p>
&lt;figure id="figure-can-you-understand-what-is-happening-me-neither">
&lt;a data-fancybox="" href="/post/nsbm_sp500_stock_market_disparity_filter/nsbm_2018-01-01_2018-06-01_beta_0.5_hu63a031e361e4cd7a15f51f9db9995b63_1589031_0x400_resize_lanczos_3.png" data-caption="Can you understand what is happening? Me neither.">
&lt;img src="/post/nsbm_sp500_stock_market_disparity_filter/nsbm_2018-01-01_2018-06-01_beta_0.5_hu63a031e361e4cd7a15f51f9db9995b63_1589031_0x400_resize_lanczos_3.png" alt="" height="4200px">
&lt;/a>
&lt;figcaption>
Can you understand what is happening? Me neither.
&lt;/figcaption>
&lt;/figure>
&lt;p>A visual inspection is sometimes not enough, and maybe you need to create an automatic analysis, which is easy to do with the graph-tool. For example, to get a summary of the hierarchical community structure got by the nSBM algorithm, we can call the &lt;code>print_summary&lt;/code> method.&lt;/p>
&lt;pre>&lt;code class="language-python">
state.print_summary()
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>l: 0, N: 483, B: 25
l: 1, N: 25, B: 6
l: 2, N: 6, B: 2
l: 3, N: 2, B: 1
l: 4, N: 1, B: 1
&lt;/code>&lt;/pre>
&lt;p>In the first level, we have &lt;strong>21&lt;/strong> communities for all the &lt;strong>11&lt;/strong> sectors.&lt;/p>
&lt;p>If we want to know which communities the &lt;strong>TSLA&lt;/strong> stock belongs to, we go through&lt;/p>
&lt;p>the hierarchy in the reverse order,&lt;/p>
&lt;pre>&lt;code class="language-python">
# esse é o indice da TSLA no nosso grafo original
symbol = “TSLA”
index_tesla = symbols.index(symbol)
print(symbol, symbol2sector[symbol], symbol2name[symbol])
r0 = state.levels[0].get_blocks()[index_tesla]
r1 = state.levels[1].get_blocks()[r0]
r2 = state.levels[2].get_blocks()[r1]
r3 = state.levels[3].get_blocks()[r2]
(r1, r2, r3)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>
(‘TSLA’, ‘Consumer Discretionary’, ‘Tesla’)
(19, 0, 0)
&lt;/code>&lt;/pre>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>The aim of this post was to give you a first impression of a method that combines the use of nSBM and edge filtering techniques to enhance our understanding of the correlation structure of the data, especially with financial data. However, we can also use this method in different scenarios, as&lt;/p>
&lt;p>
&lt;a href="https://www.science.org/doi/10.1126/sciadv.aaq1360" target="_blank" rel="noopener">topic modeling in NLP&lt;/a> and
&lt;a href="https://arxiv.org/abs/2110.01421" target="_blank" rel="noopener">survey analysis&lt;/a>.&lt;/p></description></item></channel></rss>